[
    [
        {
            "user_input": "How can I utilize Device Farm for testing my Android application?",
            "reference_contexts": [
                "This document explains how a simple Android Sample Application operates using the [Detr\\_resnet50\\_dc5](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/a631921e-dc8b-46cb-ac17-d23c5a54db26) model optimized for Exynos hardware. **1\\. Functionality** This Sample Application identifies objects in images either from stored image files or input via the camera. Detected objects are highlighted with bounding boxes, and the label and score of each object are displayed. Additionally, the inference time is shown at the bottom of the application interface. ![](images/sample_application/phone_1.png) **2\\. Prerequisites** * To download [Android Studio](https://developer.android.com/studio), go to the official website and download it. * To run the Sample Application, clone the following repository. ```sh git clone https://github.com/exynos-eco/enn-sdk-samples-9925.git ``` * If there is no device available to run the application, you can use a real device provided in the Device Farm. For guidance on connecting a device in Android Studio, refer to the ADB proxy guide. **3. Starting Sample Application** * Open the cmd window and check if adb is connected. ```sh adb device List of devices attached 000011b58d246013 device ``` * Push a sample image for testing to the following path using adb push. ```sh adb push sample.jpg /storage/emulated/0/Pictures/ ``` * If the sample image is not visible after running the Sample Application, the media scanner may not have been updated, preventing the app from detecting the file. In this case, run the following command in adb shell to trigger media scanning. ```sh adb adb shell am broadcast -a android.intent.action.MEDIA_SCANNER_SCAN_FILE -d file:///storage/emulated/0/Pictures/sample.jpg ``` * After confirming a proper connection with Device Farm, select Tools \\-\\> Device Manager in Android Studio. ![](images/sample_application/device_manager_1.png) * After that check if the physical device is properly connected. ![](images/sample_application/samsung_electronics_1.png) * Run the Object Detection project from the Sample Applications obtained via git clone in Android Studio. ![](images/sample_application/object_detection_1.png) * When you select the object-detection project Android Studio will automatically perform a gradle sync. * The application in the selected project checks the connected device and runs when you click the Run button. ![](images/sample_application/run_1.png) * Select image mode and provide the data to be used for inference. ![](images/sample_application/camera_1.png) **4\\. I/O Specs** | INPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | input | An RGB image | \\[1, 3, 480, 480\\] | float32 | | OUTPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | output | Class score value | \\[1, 100, 92\\] | float32 | | 4363 | Show bounding box | \\[1, 100, 4\\] | Float32 | **5\\."
            ],
            "reference": "To utilize Device Farm for testing your Android application, you can connect a real device provided in the Device Farm if you do not have a device available. Ensure you confirm a proper connection with Device Farm by selecting Tools -> Device Manager in Android Studio and checking if the physical device is properly connected.",
            "synthesizer_name": "single_hop_specifc_query_synthesizer",
            "file": "C:\\Users\\User\\Downloads\\new_doc\\ECO25_GENERAL_Doc\\documentation\\documentation.md"
        },
        {
            "user_input": "What is the purpose of ennExecute in the object detection application workflow?",
            "reference_contexts": [
                "Object Detection Application Workflow** The Object Detection Sample Application follows these steps to produce results. 1. The model execution preparation sequence is as follows. 1-1. Initialize the framework using the ennInitialize function. 1-2. Load the ML model into the framework using the ennOpenModel function. 1-3. Allocate and commit the necessary buffers using the ennAllocateAllBuffers function. 2. The model execution sequence is as follows. 2-1. Set the input data as parameters. 2-2. Call the ennExecute function. 2-3. Receive the execution results. 3. The method for releasing the framework initialization is as follows. 3-1. Release the allocated memory of the buffers using the ennReleaseBuffers function. 3-2. Close the model and release other resources using the ennCloseModel function. 3-3. Deinitialize the framework using the ennDeinitialize function. ※ If you want to learn more about the detailed functions, please refer to the [Framework API](#bookmark=id.3o7alnk) at the bottom of the document. **6\\. Checklist when replacing model** 1\\. Model Filename (MODEL\\_NAME) * Replace the model file (Detr\\_resnet50\\_dc5) by adding it to the assets directory and entering the new model filename in the ModelConstants.kt file. ![](images/sample_application/android_assets_1.png) ![](images/sample_application/modelconstants_1.png) ```kotlin const val MODEL_NAME = \"detr_resnet50_dc5.nnc\" ``` 2. Change the input data type in ModelConstants.kt (INPUT_DATA_TYPE, INPUT_DATA_LAYER) * Other models may accept FLOAT32 format input, so verification is required. Check whether the input format is NHWC, HWC, or CHW before making changes. ```kotlin val INPUT_DATA_TYPE = DataType.FLOAT32 // Change it to the data type required by the model val INPUT_DATA_LAYER = LayerType.CHW // Change it to the layer required by the model ``` 3. Modify the input resolution (INPUT_SIZE_W, INPUT_SIZE_H, INPUT_SIZE_C) in ModelConstants.kt. * You must reflect the input resolution used by the model. ```kotlin const val INPUT_SIZE_W = 320 const val INPUT_SIZE_H = 320 const val INPUT_SIZE_C = 3 // If 3(RGB) ``` 4. Change the normalization method in ModelConstants.kt (INPUT\\_CONVERSION\\_SCALE, INPUT\\_CONVERSION\\_OFFSET) * You need to check and modify the normalization method used by the model. * Input normalization formula normalized_input = (raw_input−INPUT_CONVERSION_OFFSET)/INPUT_CONVERSION_SCALE ```kotlin const val INPUT_CONVERSION_SCALE = 255.0F // Scale the input data by 255 times const val INPUT_CONVERSION_OFFSET = 0.0F // Since the normalization formula is normalized\\_input \\= (raw\\_input−0.0)/255 any original pixel value, such as 0, 128, or 255, will always be normalized within the 0 to 1 range. ``` Deep learning models are designed to accept input data in the range of 0 to 1 or \\-1 to 1\\. 5\\. Output size in ModelConstants.kt (OUTPUT\\_SIZE\\_W, OUTPUT\\_SIZE\\_H) * Check and modify the output size of the model accordingly. ```kotlin const val OUTPUT_SIZE_W = 8400 const val OUTPUT_SIZE_H = 84 ``` 6\\. Class label file in ModelConstants.kt (LABEL_FILE) * If the model uses different class labels, a new .txt file must be provided in the assets directory. ```kotlin const val LABEL_FILE = \"detr_resnet50_dc5.txt\" ``` 7\\. Modify the preProcess() and postProcess() functions in the ModelExecutor.kt file located at the following path ![](images/sample_application/modelExecutor_1.png) * If the model's input or output data differs from the predefined format, modify the preProcess() and postProcess() functions accordingly. ```kotlin Class ModelExecutor -> preProcess(), postProcess() ``` **7\\. Compatible AI Models** * [Detr\\_resnet101\\_dc5](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/9eb3e0fd-4478-49e4-b631-5941ce62d16c) * Face\\_det\\_lite * Foot\\_track\\_net * Yolov5"
            ],
            "reference": "The ennExecute function is called during the model execution sequence to process the input data and receive the execution results.",
            "synthesizer_name": "single_hop_specifc_query_synthesizer",
            "file": "C:\\Users\\User\\Downloads\\new_doc\\ECO25_GENERAL_Doc\\documentation\\documentation.md"
        }
    ],
    [
        {
            "user_input": "What Detr_resnet50_dc5 model do in Android Sample Application?",
            "reference_contexts": [
                "This document explains how a simple Android Sample Application operates using the [Detr\\_resnet50\\_dc5](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/a631921e-dc8b-46cb-ac17-d23c5a54db26) model optimized for Exynos hardware. **1\\. Functionality** This Sample Application identifies objects in images either from stored image files or input via the camera. Detected objects are highlighted with bounding boxes, and the label and score of each object are displayed. Additionally, the inference time is shown at the bottom of the application interface. ![](images/sample_application/phone_1.png) **2\\. Prerequisites** * To download [Android Studio](https://developer.android.com/studio), go to the official website and download it. * To run the Sample Application, clone the following repository. ```sh git clone https://github.com/exynos-eco/enn-sdk-samples-9925.git ``` * If there is no device available to run the application, you can use a real device provided in the Device Farm. For guidance on connecting a device in Android Studio, refer to the ADB proxy guide. **3. Starting Sample Application** * Open the cmd window and check if adb is connected. ```sh adb device List of devices attached 000011b58d246013 device ``` * Push a sample image for testing to the following path using adb push. ```sh adb push sample.jpg /storage/emulated/0/Pictures/ ``` * If the sample image is not visible after running the Sample Application, the media scanner may not have been updated, preventing the app from detecting the file. In this case, run the following command in adb shell to trigger media scanning. ```sh adb adb shell am broadcast -a android.intent.action.MEDIA_SCANNER_SCAN_FILE -d file:///storage/emulated/0/Pictures/sample.jpg ``` * After confirming a proper connection with Device Farm, select Tools \\-\\> Device Manager in Android Studio. ![](images/sample_application/device_manager_1.png) * After that check if the physical device is properly connected. ![](images/sample_application/samsung_electronics_1.png) * Run the Object Detection project from the Sample Applications obtained via git clone in Android Studio. ![](images/sample_application/object_detection_1.png) * When you select the object-detection project Android Studio will automatically perform a gradle sync. * The application in the selected project checks the connected device and runs when you click the Run button. ![](images/sample_application/run_1.png) * Select image mode and provide the data to be used for inference. ![](images/sample_application/camera_1.png) **4\\. I/O Specs** | INPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | input | An RGB image | \\[1, 3, 480, 480\\] | float32 | | OUTPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | output | Class score value | \\[1, 100, 92\\] | float32 | | 4363 | Show bounding box | \\[1, 100, 4\\] | Float32 | **5\\."
            ],
            "reference": "Detr_resnet50_dc5 model in the Android Sample Application identifies objects in images either from stored image files or input via the camera. Detected objects are highlighted with bounding boxes, and the label and score of each object are displayed.",
            "synthesizer_name": "single_hop_specifc_query_synthesizer",
            "file": "C:\\Users\\User\\Downloads\\new_doc\\ECO25_GENERAL_Doc\\documentation\\documentation.md"
        }
    ]
]