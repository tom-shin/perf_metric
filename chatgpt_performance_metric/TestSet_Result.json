[
  {
    "user_input": "How can I download Android Studio for developing applications?",
    "reference_contexts": [
      "This document explains how a simple Android Sample Application operates using the [Detr\\_resnet50\\_dc5](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/a631921e-dc8b-46cb-ac17-d23c5a54db26) model optimized for Exynos hardware. **1\\. Functionality** This Sample Application identifies objects in images either from stored image files or input via the camera. Detected objects are highlighted with bounding boxes, and the label and score of each object are displayed. Additionally, the inference time is shown at the bottom of the application interface. ![](images/sample_application/phone_1.png) **2\\. Prerequisites** * To download [Android Studio](https://developer.android.com/studio), go to the official website and download it. * To run the Sample Application, clone the following repository. ```sh git clone https://github.com/exynos-eco/enn-sdk-samples-9925.git ``` * If there is no device available to run the application, you can use a real device provided in the Device Farm. For guidance on connecting a device in Android Studio, refer to the ADB proxy guide. **3. Starting Sample Application** * Open the cmd window and check if adb is connected. ```sh adb device List of devices attached 000011b58d246013 device ``` * Push a sample image for testing to the following path using adb push. ```sh adb push sample.jpg /storage/emulated/0/Pictures/ ``` * If the sample image is not visible after running the Sample Application, the media scanner may not have been updated, preventing the app from detecting the file. In this case, run the following command in adb shell to trigger media scanning. ```sh adb adb shell am broadcast -a android.intent.action.MEDIA_SCANNER_SCAN_FILE -d file:///storage/emulated/0/Pictures/sample.jpg ``` * After confirming a proper connection with Device Farm, select Tools \\-\\> Device Manager in Android Studio. ![](images/sample_application/device_manager_1.png) * After that check if the physical device is properly connected. ![](images/sample_application/samsung_electronics_1.png) * Run the Object Detection project from the Sample Applications obtained via git clone in Android Studio. ![](images/sample_application/object_detection_1.png) * When you select the object-detection project Android Studio will automatically perform a gradle sync. * The application in the selected project checks the connected device and runs when you click the Run button. ![](images/sample_application/run_1.png) * Select image mode and provide the data to be used for inference. ![](images/sample_application/camera_1.png) **4\\. I/O Specs** | INPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | input | An RGB image | \\[1, 3, 480, 480\\] | float32 | | OUTPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | output | Class score value | \\[1, 100, 92\\] | float32 | | 4363 | Show bounding box | \\[1, 100, 4\\] | Float32 | **5\\."
    ],
    "reference": "To download Android Studio, go to the official website and download it.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation\\documentation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:57:11.112083+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How can I replace the model with detr_resnet50_dc5 in the Object Detection Sample Application?",
    "reference_contexts": [
      "Object Detection Application Workflow** The Object Detection Sample Application follows these steps to produce results. 1. The model execution preparation sequence is as follows. 1-1. Initialize the framework using the ennInitialize function. 1-2. Load the ML model into the framework using the ennOpenModel function. 1-3. Allocate and commit the necessary buffers using the ennAllocateAllBuffers function. 2. The model execution sequence is as follows. 2-1. Set the input data as parameters. 2-2. Call the ennExecute function. 2-3. Receive the execution results. 3. The method for releasing the framework initialization is as follows. 3-1. Release the allocated memory of the buffers using the ennReleaseBuffers function. 3-2. Close the model and release other resources using the ennCloseModel function. 3-3. Deinitialize the framework using the ennDeinitialize function. ※ If you want to learn more about the detailed functions, please refer to the [Framework API](#bookmark=id.3o7alnk) at the bottom of the document. **6\\. Checklist when replacing model** 1\\. Model Filename (MODEL\\_NAME) * Replace the model file (Detr\\_resnet50\\_dc5) by adding it to the assets directory and entering the new model filename in the ModelConstants.kt file. ![](images/sample_application/android_assets_1.png) ![](images/sample_application/modelconstants_1.png) ```kotlin const val MODEL_NAME = \"detr_resnet50_dc5.nnc\" ``` 2. Change the input data type in ModelConstants.kt (INPUT_DATA_TYPE, INPUT_DATA_LAYER) * Other models may accept FLOAT32 format input, so verification is required. Check whether the input format is NHWC, HWC, or CHW before making changes. ```kotlin val INPUT_DATA_TYPE = DataType.FLOAT32 // Change it to the data type required by the model val INPUT_DATA_LAYER = LayerType.CHW // Change it to the layer required by the model ``` 3. Modify the input resolution (INPUT_SIZE_W, INPUT_SIZE_H, INPUT_SIZE_C) in ModelConstants.kt. * You must reflect the input resolution used by the model. ```kotlin const val INPUT_SIZE_W = 320 const val INPUT_SIZE_H = 320 const val INPUT_SIZE_C = 3 // If 3(RGB) ``` 4. Change the normalization method in ModelConstants.kt (INPUT\\_CONVERSION\\_SCALE, INPUT\\_CONVERSION\\_OFFSET) * You need to check and modify the normalization method used by the model. * Input normalization formula normalized_input = (raw_input−INPUT_CONVERSION_OFFSET)/INPUT_CONVERSION_SCALE ```kotlin const val INPUT_CONVERSION_SCALE = 255.0F // Scale the input data by 255 times const val INPUT_CONVERSION_OFFSET = 0.0F // Since the normalization formula is normalized\\_input \\= (raw\\_input−0.0)/255 any original pixel value, such as 0, 128, or 255, will always be normalized within the 0 to 1 range. ``` Deep learning models are designed to accept input data in the range of 0 to 1 or \\-1 to 1\\. 5\\. Output size in ModelConstants.kt (OUTPUT\\_SIZE\\_W, OUTPUT\\_SIZE\\_H) * Check and modify the output size of the model accordingly. ```kotlin const val OUTPUT_SIZE_W = 8400 const val OUTPUT_SIZE_H = 84 ``` 6\\. Class label file in ModelConstants.kt (LABEL_FILE) * If the model uses different class labels, a new .txt file must be provided in the assets directory. ```kotlin const val LABEL_FILE = \"detr_resnet50_dc5.txt\" ``` 7\\. Modify the preProcess() and postProcess() functions in the ModelExecutor.kt file located at the following path ![](images/sample_application/modelExecutor_1.png) * If the model's input or output data differs from the predefined format, modify the preProcess() and postProcess() functions accordingly. ```kotlin Class ModelExecutor -> preProcess(), postProcess() ``` **7\\. Compatible AI Models** * [Detr\\_resnet101\\_dc5](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/9eb3e0fd-4478-49e4-b631-5941ce62d16c) * Face\\_det\\_lite * Foot\\_track\\_net * Yolov5"
    ],
    "reference": "To replace the model with detr_resnet50_dc5, follow these steps: 1. Replace the model file (detr_resnet50_dc5) by adding it to the assets directory and entering the new model filename in the ModelConstants.kt file as follows: const val MODEL_NAME = \"detr_resnet50_dc5.nnc\". 2. Change the input data type in ModelConstants.kt (INPUT_DATA_TYPE, INPUT_DATA_LAYER) to match the model's requirements. For example: val INPUT_DATA_TYPE = DataType.FLOAT32 and val INPUT_DATA_LAYER = LayerType.CHW. 3. Modify the input resolution (INPUT_SIZE_W, INPUT_SIZE_H, INPUT_SIZE_C) in ModelConstants.kt to reflect the model's input resolution: const val INPUT_SIZE_W = 320, const val INPUT_SIZE_H = 320, const val INPUT_SIZE_C = 3. 4. Change the normalization method in ModelConstants.kt (INPUT_CONVERSION_SCALE, INPUT_CONVERSION_OFFSET) to match the model's normalization method. For instance: const val INPUT_CONVERSION_SCALE = 255.0F and const val INPUT_CONVERSION_OFFSET = 0.0F. 5. Adjust the output size in ModelConstants.kt (OUTPUT_SIZE_W, OUTPUT_SIZE_H) accordingly: const val OUTPUT_SIZE_W = 8400, const val OUTPUT_SIZE_H = 84. 6. If the model uses different class labels, provide a new .txt file in the assets directory: const val LABEL_FILE = \"detr_resnet50_dc5.txt\". 7. Finally, modify the preProcess() and postProcess() functions in the ModelExecutor.kt file if the model's input or output data differs from the predefined format.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation\\documentation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:57:11.112083+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How can I use Device Farm to run my Android application?",
    "reference_contexts": [
      "This document explains how a simple Android Sample Application operates using the [Densenet121](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/118f8cc6-f251-43b7-b8c2-ec77a3c50fda) model optimized for Exynos hardware. **1\\. Functionality** This application classifies objects in images either from stored image files or captured via the camera. The classified items, corresponding scores, and inference time are displayed at the bottom of the application interface. ![](images/sample_application/mouse.png) **2\\. Prerequisites** * To download [Android Studio](https://developer.android.com/studio), go to the official website and download it. * To run the Sample Application, clone the following repository. ```sh git clone https://github.com/exynos-eco/enn-sdk-samples-9945.git ``` * If there is no device available to run the application, you can use a real device provided in the Device Farm. For guidance on connecting a device in Android Studio, refer to the ADB proxy guide. **3\\. Starting Sample Application** * Open the cmd window and check if adb is connected. ```sh adb device List of devices attached 000011b58d246013 device ``` * Push a sample image for testing to the following path using adb push. ```sh adb push sample.jpg /storage/emulated/0/Pictures/ ``` * If the sample image is not visible after running the Sample Application, the media scanner may not have been updated, preventing the app from detecting the file. In this case, run the following command in adb shell to trigger media scanning. ```sh adb adb shell am broadcast -a android.intent.action.MEDIA_SCANNER_SCAN_FILE -d file:///storage/emulated/0/Pictures/sample.jpg ``` * After confirming a proper connection with Device Farm, select Tools \\-\\> Device Manager in Android Studio. ![](images/sample_application/device_manager_1.png) * After that check if the physical device is properly connected. ![](images/sample_application/samsung_electronics_1.png) * Run the Image Classification project from the Sample Applications obtained via git clone in Android Studio. ![](images/sample_application/image_classification_1.png) * When you select the image-classification project Android Studio will automatically perform a gradle sync. * The application in the selected project checks the connected device and runs when you click the Run button. ![](images/sample_application/run_1.png) * Select image mode and provide the data to be used for inference. ![](images/sample_application/camera_1.png) **4\\."
    ],
    "reference": "If there is no device available to run the application, you can use a real device provided in the Device Farm. For guidance on connecting a device in Android Studio, refer to the ADB proxy guide.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation\\documentation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:57:11.112083+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is MnasNet05 and how it used in image classification?",
    "reference_contexts": [
      "I/O Specs** | INPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | Image | An RGB image | \\[1, 3, 224, 224\\] | float32 | | OUTPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | Classes | [Dataset Classes](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/assets/labels1001.txt) | \\[1, 1000\\] | float32 | **5\\. Image Classification** **Application Workflow** The Image Classification Sample Application follows these steps to produce results. 1. The model execution preparation sequence is as follows. 1-1. Initialize the framework using the ennInitialize function. 1-2. Load the ML model into the framework using the ennOpenModel function. 1-3. Allocate and commit the necessary buffers using the ennAllocateAllBuffers function. 2. The model execution sequence is as follows. 2-1. Set the input data as parameters. 2-2. Call the ennExecute function. 2-3. Receive the execution results. 3. The method for releasing the framework initialization is as follows. 3-1. Release the allocated memory of the buffers using the ennReleaseBuffers function. 3-2. Close the model and release other resources using the ennCloseModel function. 3-3. Deinitialize the framework using the ennDeinitialize function. ※ If you want to learn more about the detailed functions, please refer to the [Framework API](#bookmark=id.3o7alnk) at the bottom of the document. **6\\. Checklist when replacing model** 1\\. Model Filename (Densenet121) * Replace the model file (densenet121) by adding it to the assets directory and entering the new model filename in the ModelConstants.kt file. ![](images/sample_application/modelconstants_directory.png) ![](images/sample_application/modelconstants_2.png) ```kotlin const val MODEL_NAME = \"densenet121.nnc\" ``` 2\\. Change the input data type in ModelConstants.kt (INPUT_DATA_TYPE, INPUT_DATA_LAYER) * Other models may accept FLOAT32 format input, so verification is required. * Check whether the input format is NHWC, HWC, or CHW before making changes. ```kotlin val INPUT_DATA_TYPE = DataType.FLOAT32 // Change it to the data type required by the model val INPUT_DATA_LAYER = LayerType.CHW // Change it to the layer required by the model ``` 3\\. Modify the input resolution (INPUT_SIZE_W, INPUT_SIZE_H, INPUT_SIZE_C) in ModelConstants.kt * You must reflect the input resolution used by the model. ```kotlin const val INPUT_SIZE_W = 224 // If the model uses **224x224**, modify it accordingly const val INPUT_SIZE_H = 224 const val INPUT_SIZE_C = 3 // If 3(RGB) ``` 4\\. Change the normalization method in ModelConstants.kt (INPUT\\_CONVERSION\\_SCALE, INPUT\\_CONVERSION\\_OFFSET) * You need to check and modify the normalization method used by the model. * Input normalization formula normalized\\_input \\= (raw\\_input−INPUT\\_CONVERSION\\_OFFSET)/INPUT\\_CONVERSION\\_SCALE ```kotlin const val INPUT_CONVERSION_SCALE = 1/255F // Scale the input data by 255 times const val INPUT_CONVERSION_OFFSET = 0F // No data is subtracted from the input. Since the normalization formula is normalized\\_input \\= (raw\\_input–0)/(1/255) this is equivalent to raw\\_input×255. Therefore, the original pixel values in the range 0–255 are scaled up by 255 times, effectively keeping the 0–255 values unchanged. ``` 5\\. Modify the output size in ModelConstants.kt (OUTPUT\\_DATA\\_TYPE, OUTPUT\\_CONVERSION\\_SCALE, OUTPUT\\_CONVERSION\\_OFFSET) * You need to check and modify the model is data type and normalization method accordingly. * Output normalization formula final\\_output=(model\\_output×OUTPUT\\_CONVERSION\\_SCALE)+OUTPUT\\_CONVERSION\\_OFFSET ```kotlin val OUTPUT_DATA_TYPE = DataType.FLOAT32 const val OUTPUT_CONVERSION_SCALE = 1F // The output data is kept in its original form without scaling const val OUTPUT_CONVERSION_OFFSET = 0F // Since the formula is (model_output × 1) + 0 the output data remains unchanged and is used as is without any transformation ``` 6\\. Class label file in ModelConstants.kt (LABEL\\_FILE) * If the model uses different class labels a new .txt file must be provided in the assets directory. ```kotlin const val LABEL_FILE = \"densenet121.txt\" ``` 7\\. Modify the preProcess() and postProcess() functions in the ModelExecutor.kt file located at the following path. ![](images/sample_application/image_classification_modelExecutor.png) * If the model's input or output data differs from the predefined format, modify the preProcess() and postProcess() functions accordingly. ```kotlin Class ModelExecutor -> preProcess(), postProcess() ``` **7\\. Compatible AI Models** * [EfficientNet-B4](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/9d310aaa-d2f0-43d8-bdb1-0c31413da46e) * [MobileNet-v2](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/0c031a1e-0eed-442d-9691-421d416a5556) * [ResNet18](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/4c29e543-f74f-4bc3-a373-bc993c7ac7df) * [Resnet34\\_v1\\_7](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/df74a1bf-b048-4648-9396-31231b6fed49) * [ResNet50](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/27b58ffc-c760-4c87-ab60-533aba27ffa6) * [ResNet101](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/311c216e-f50c-4fee-a400-952b1fb96506) * [SqueezeNet1\\_1](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/546abf23-be6c-4a1a-9d65-edb48e94eb3a) * [Densenet121](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/118f8cc6-f251-43b7-b8c2-ec77a3c50fda) * [EfficientNet-B0](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/21ed28ef-d958-4cec-8d29-2d13efaf0468) * [MnasNet05](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/34efd7b3-8f3d-44fa-9440-34365277ff5f)"
    ],
    "reference": "MnasNet05 is a compatible AI model used in image classification applications. It follows a specific workflow for model execution, which includes initializing the framework, loading the ML model, allocating necessary buffers, setting input data, executing the model, and receiving results.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation\\documentation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:57:11.112083+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Wht is the purpose of Android Stdio in developing Android applications?",
    "reference_contexts": [
      "This document explains how a simple Android Sample Application operates using the [DeeplabV3](https://prd.ai-studio-farm.com/kr/solution/ai/models/detail/ca3dc3fa-d687-474c-9679-dcee50da8b52) model optimized for Exynos hardware. **1\\. Functionality** This Application provides segmentation results for images either from stored image files or captured via the camera. Each pixel of the segmented object is overlaid with a color corresponding to its label, offering a visual representation of the classification. ![](images/sample_application/deepable.png) **2\\. Prerequisites** * To download [Android Studio](https://developer.android.com/studio), go to the official website and download it. * To run the Sample Application, clone the following repository. ```sh git clone https://github.com/exynos-eco/enn-sdk-samples-9945.git ``` * If there is no device available to run the application, you can use a real device provided in the Device Farm. For guidance on connecting a device in Android Studio, refer to the ADB proxy guide. **3\\. Starting Sample Application** * Open the cmd window and check if adb is connected. ```sh adb device List of devices attached 000011b58d246013 device ``` * Push a sample image for testing to the following path using adb push. ```sh adb push sample.jpg /storage/emulated/0/Pictures/ ``` * If the sample image is not visible after running the Sample Application, the media scanner may not have been updated, preventing the app from detecting the file. In this case, run the following command in adb shell to trigger media scanning. ```sh adb adb shell am broadcast -a android.intent.action.MEDIA_SCANNER_SCAN_FILE -d file:///storage/emulated/0/Pictures/sample.jpg ``` * After confirming a proper connection with Device Farm, select Tools -> Device Manager in Android Studio. ![](images/sample_application/device_manager_1.png) * After that check if the physical device is properly connected. ![](images/sample_application/samsung_electronics_1.png) * Run the Segmentation project from the Sample Applications obtained via git clone in Android Studio. ![](images/sample_application/segmentation.png) * When you select the segmentation project Android Studio will automatically perform a gradle sync. * The application in the selected project checks the connected device and runs when you click the Run button. ![](images/sample_application/run_1.png) * Select image mode and provide the data to be used for inference. ![](images/sample_application/camera_1.png) **4\\."
    ],
    "reference": "Android Studio is essential for developing Android applications as it provides the necessary tools and environment to build, test, and optimize apps. It allows developers to run sample applications, manage devices, and perform gradle syncs for project management.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation\\documentation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:57:11.112083+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is DataType in the context of model input?",
    "reference_contexts": [
      "I/O Specs** | INPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | Image | An RGB image | \\[1,257,257,3\\] | float32 | | OUTPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | Classes | Dataset Classes | \\[1,257,257,21\\] | float32 | **5\\. Segmentation** **Application Workflow** The Segmentation Sample Application follows these steps to produce results. 1. The model execution preparation sequence is as follows. 1-1. Initialize the framework using the ennInitialize function. 1-2. Load the ML model into the framework using the ennOpenModel function. 1-3. Allocate and commit the necessary buffers using the ennAllocateAllBuffers function. 2. The model execution sequence is as follows. 2-1. Set the input data as parameters. 2-2. Call the ennExecute function. 2-3. Receive the execution results. 3. The method for releasing the framework initialization is as follows. 3-1. Release the allocated memory of the buffers using the ennReleaseBuffers function. 3-2. Close the model and release other resources using the ennCloseModel function. 3-3. Deinitialize the framework using the ennDeinitialize function. ※ If you want to learn more about the detailed functions, please refer to the [Framework API](#bookmark=id.3o7alnk) at the bottom of the document. **6\\. Checklist when replacing model** 1\\. Model Filename (DeepLabV3) * Replace the model file (deeplabv3) by adding it to the assets directory and entering the new model filename in the ModelConstants.kt file. ![](images/sample_application/deepable_directory.png) ![](images/sample_application/deepable_modelconstants.png) ```kotlin const val MODEL_NAME = \"deeplabv3.nnc\" ``` 2\\. Change the input data type in ModelConstants.kt (INPUT\\_DATA\\_TYPE, INPUT\\_DATA\\_LAYER) * Other models may accept FLOAT32 format input, so verification is required. * Check whether the input format is NHWC, HWC, or CHW before making changes. ```kotlin val INPUT_DATA_TYPE = DataType.FLOAT32 // Change it to the data type required by the model val INPUT_DATA_LAYER = LayerType.HWC // 모델이 요구하는 레이어 형식으로 변경 ``` 3\\. Modify the input resolution (INPUT\\_SIZE\\_W, INPUT\\_SIZE\\_H, INPUT\\_SIZE\\_C) in ModelConstants.kt * You must reflect the input resolution used by the model. ```kotlin const val INPUT_SIZE_W = 257 // If the model uses 257x257, modify it accordingly const val INPUT_SIZE_H = 257 const val INPUT_SIZE_C = 3 // If 3(RGB) ``` 4\\. Change the normalization method in ModelConstants.kt (INPUT\\_CONVERSION\\_SCALE, INPUT\\_CONVERSION\\_OFFSET) * You need to check and modify the normalization method used by the model. * Input normalization formula normalized\\_input \\= (raw\\_input−INPUT\\_CONVERSION\\_OFFSET)/INPUT\\_CONVERSION\\_SCALE ```kotlin const val INPUT_CONVERSION_SCALE = 127.5F // Scale the input data by 127.5 times const val INPUT_CONVERSION_OFFSET = 127.5F // If the input pixel values range from 0\\~255, this technique is used to symmetrically normalize the data around the central value 127.5. By subtracting 127.5 from the input data (raw_input), the values are centered around 0, a commonly used technique in deep learning ``` 5\\. Output size in ModelConstants.kt (OUTPUT\\_DATA\\_TYPE, OUTPUT\\_SIZE\\_W, OUTPUT\\_SIZE\\_H, OUTPUT\\_SIZE\\_C) * You need to check and modify the model's data type and output size accordingly. The output size should be the same as the input size. * The DeepLabV3 model performs segmentation with 21 classes. For each pixel 21 probability values are output. ```kotlin val OUTPUT_DATA_TYPE = DataType.FLOAT32 const val OUTPUT_SIZE_H = INPUT_SIZE_H const val OUTPUT_SIZE_W = INPUT_SIZE_W const val OUTPUT_SIZE_C = 21 ``` 6\\. Output normalization in ModelConstants.kt (OUTPUT\\_CONVERSION\\_SCALE, OUTPUT\\_CONVERSION\\_OFFSET) * You need to check and modify the normalization method used by the model. * Output normalization formula final_output=(model_output×OUTPUT_CONVERSION_SCALE)+OUTPUT_CONVERSION_OFFSET ```kotlin const val OUTPUT_CONVERSION_SCALE = 1F // Use the output scale without conversion const val OUTPUT_CONVERSION_OFFSET = 0F // Since the formula is (model_output × 1) + 0, the output remains unchanged and is used as is without any transformation ``` 7\\. Modify the preProcess() and postProcess() functions in the ModelExecutor.kt file located at the following path. ![](images/sample_application/deepable_modelexecutor.png) * If the model is input or output data differs from the predefined format, modify the preProcess() and postProcess() functions accordingly. ```kotlin Class ModelExecutor -> preProcess(), postProcess() ``` **7\\. Compatible AI Models** * [DDRNet23\\_Slim](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/994cb06f-b886-4fb6-b8e9-8b4efdc8baee)"
    ],
    "reference": "DataType refers to the format of the input data required by the model. For example, in the context provided, the input data type is specified as float32.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation\\documentation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:57:11.112083+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does the Poseenet model function in the Android Sample Application?",
    "reference_contexts": [
      "This document explains how a simple Android Sample Application operates using the [Posenet](https://prd.ai-studio-farm.com/kr/solution/ai/models/detail/f40473cb-a6e6-42a3-a50a-daf428273eab) model optimized for Exynos hardware **1\\. Functionality** This Application detects key points in images from stored image files or those captured via the camera and automatically measures joint positions. Additionally, the inference time is displayed at the bottom of the application interface. ![](images/sample_application/pose_estimation.png) **2\\. Prerequisites** * To download [Android Studio](https://developer.android.com/studio), go to the official website and download it. * To run the Sample Application, clone the following repository. ```sh git clone https://github.com/exynos-eco/enn-sdk-samples-9945.git ``` * If there is no device available to run the application, you can use a real device provided in the Device Farm. For guidance on connecting a device in Android Studio, refer to the ADB proxy guide. **3\\. Starting Sample Application** * Open the cmd window and check if adb is connected. ```sh adb device List of devices attached 000011b58d246013 device ``` * Push a sample image for testing to the following path using adb push. ```sh adb push sample.jpg /storage/emulated/0/Pictures/ ``` * If the sample image is not visible after running the Sample Application, the media scanner may not have been updated, preventing the app from detecting the file. In this case, run the following command in adb shell to trigger media scanning. ```sh adb adb shell am broadcast -a android.intent.action.MEDIA_SCANNER_SCAN_FILE -d file:///storage/emulated/0/Pictures/sample.jpg ``` * After confirming a proper connection with Device Farm, select Tools -> Device Manager in Android Studio. ![](images/sample_application/device_manager_1.png) * After that check if the physical device is properly connected. ![](images/sample_application/samsung_electronics_1.png) * Run the Pose Estimation project from the Sample Applications obtained via git clone in Android Studio. ![](images/sample_application/pose_estimation_android.png) * When you select the pose-estimation project Android Studio will automatically perform a gradle sync. * The application in the selected project checks the connected device and runs when you click the Run button. ![](images/sample_application/run_1.png) * Select image mode and provide the data to be used for inference. ![](images/sample_application/camera_1.png) **4\\. I/O Specs** | INPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | Image | An RGB image | \\[1, 3, 513, 257\\] | float32 | | OUTPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | Classes | Human Body Part | \\[1, 17, 33, 17\\] | float32 | **5\\."
    ],
    "reference": "The Poseenet model in the Android Sample Application detects key points in images from stored image files or those captured via the camera and automatically measures joint positions. Additionally, the inference time is displayed at the bottom of the application interface.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation\\documentation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:57:11.112083+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What AI models are compatible with the Facemap_3dmm?",
    "reference_contexts": [
      "Post Estimation Application Workflow** The Pose Estimation Sample Application follows these steps to produce results. 1. The model execution preparation sequence is as follows. 1-1. Initialize the framework using the ennInitialize function. 1-2. Load the ML model into the framework using the ennOpenModel function. 1-3. Allocate and commit the necessary buffers using the ennAllocateAllBuffers function. 2. The model execution sequence is as follows. 2-1. Set the input data as parameters. 2-2. Call the ennExecute function. 2-3. Receive the execution results. 3. The method for releasing the framework initialization is as follows. 3-1. Release the allocated memory of the buffers using the ennReleaseBuffers function. 3-2. Close the model and release other resources using the ennCloseModel function. 3-3. Deinitialize the framework using the ennDeinitialize function. ※ If you want to learn more about the detailed functions, please refer to the [Framework API](#bookmark=id.3o7alnk) at the bottom of the document. **6\\. Checklist when replacing model** 1\\. Model Filename (PoseNet) * Replace the model file (float32\\_pose) by adding it to the assets directory and entering the new model filename in the ModelConstants.kt file. ![](images/sample_application/pose_estimation_directory.png) ![](images/sample_application/pose_estimation_modelconstants.png) ```koltin const val MODEL_NAME = \"float32_pose.nnc\" ``` 2\\. Change the input data type in ModelConstants.kt (INPUT_DATA_TYPE, INPUT_DATA_LAYER) * Other models may accept FLOAT32 format input, so verification is required. * Check whether the input format is NHWC, HWC, or CHW before making changes. ```kotlin val INPUT_DATA_TYPE = DataType.FLOAT32 // Change it to the data type required by the model val INPUT_DATA_LAYER = LayerType.HWC // Change it to the layer required by the model ``` 3\\.Modify the input resolution in ModelConstants.kt (INPUT\\_SIZE\\_W, INPUT\\_SIZE\\_H, INPUT\\_SIZE\\_C) * You must reflect the input resolution used by the model. ```kotlin const val INPUT_SIZE_W = 257 // If the model uses 257x257, modify it accordingly const val INPUT_SIZE_H = 257 const val INPUT_SIZE_C = 3 // If 3(RGB) ``` 4\\. Change the normalization method in ModelConstants.kt (INPUT\\_CONVERSION\\_SCALE, INPUT\\_CONVERSION\\_OFFSET) * You need to check and modify the normalization method used by the model. * Input normalization formula normalized\\_input \\= (raw\\_input−INPUT\\_CONVERSION\\_OFFSET)/INPUT\\_CONVERSION\\_SCALE ```kotlin const val INPUT_CONVERSION_SCALE = 127.5F // Scale the input data by 127.5 times const val INPUT_CONVERSION_OFFSET = 127.5F // This is a technique commonly used in deep learning to symmetrically normalize data based on the central value of 127.5 when the input pixel values range from 0 to 255\\. By subtracting 127.5 from the input data (raw\\_input), the values are adjusted to center around zero ``` 5\\. Modify the Heatmap data type in ModelConstants.kt (HEATMAP\\_DATA\\_TYPE) * You need to check and modify the model's data type and output size accordingly. ```kotlin val HEATMAP_DATA_TYPE = DataType.FLOAT32 ``` 6\\. Modify the Heatmap output size in ModelConstants.kt (HEATMAP\\_SIZE\\_W, HEATMAP\\_SIZE\\_H, HEATMAP\\_SIZE\\_C) * he Heatmap generates an output of size 9×9×17, where the 17 channels represent the probability distribution for 17 body parts, including the nose, shoulders, elbows, wrists, hips, knees, and ankles. ```kotlin const val HEATMAP_SIZE_W = 9 // Width of heatmap (9x9) const val HEATMAP_SIZE_H = 9 // Height of heatmap const val HEATMAP_SIZE_C = 17 // Number of body parts (PoseNet uses 17 key points) ``` 7\\. Offset data type in ModelConstants.kt (OFFSET\\_DATA\\_TYPE) * You need to check and modify the model is output size accordingly. ```kotlin val OFFSET_DATA_TYPE = DataType.FLOAT32 ``` 8\\. Modify the Offset output size in ModelConstants.kt (OFFSET\\_SIZE\\_W, OFFSET\\_SIZE\\_H, OFFSET\\_SIZE\\_C) * The Offset is used to refine the exact position of each body part within the Heatmap grid. ```kotlin const val OFFSET_SIZE_W = 9 // Width of offset const val OFFSET_SIZE_H = 9 // Height of offset const val OFFSET_SIZE_C = 34 // Twice the body part (17 \\* 2 \\= 34\\) ``` 9\\. Modify the preProcess() and postProcess() functions in the ModelExecutor.kt file located at the following path. ![](images/sample_application/pose_estimation_modelexecutor.png) * If the model's input or output data differs from the predefined format, modify the preProcess() and postProcess() functions accordingly. ```kotlin Class ModelExecutor -> preProcess(), postProcess() ``` **7\\. Compatible AI Models** * Facemap\\_3dmm"
    ],
    "reference": "The compatible AI models include Facemap_3dmm.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation\\documentation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:57:11.112083+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the steps involved in the Depth Estimation Application workflow for the Android Sample Application, and how does it utilize the MiDaS v2 model?",
    "reference_contexts": [
      "<1-hop>\n\nThis document explains how a simple Android Sample Application operates using the [MiDaS v2](https://prd.ai-studio-farm.com/kr/solution/ai/models/detail/73bcae23-4a07-4df9-b0c5-80504427c11c) model optimized for Exynos hardware. **1\\. Functionality** This Application receives input from an image file or a camera. A color representing the estimated distance is overlaid on each pixel, providing a visual representation of depth. Additionally, the inference time is displayed at the bottom of the application interface. ![](images/sample_application/depth_estimation.png) **2\\. Prerequisites** * To download [Android Studio](https://developer.android.com/studio) go to the official website and download it. * To run the Sample Application, clone the following repository. ```sh git clone https://github.com/exynos-eco/enn-sdk-samples-9945.git ``` * If there is no device available to run the application, you can use a real device provided in the Device Farm. For guidance on connecting a device in Android Studio, refer to the ADB proxy guide. **3\\. Starting Sample Application** * Open the cmd window and check if adb is connected. ```sh adb device List of devices attached 000011b58d246013 device ``` * Push a sample image for testing to the following path using adb push. ```sh adb push sample.jpg /storage/emulated/0/Pictures/ ``` * If the sample image is not visible after running the Sample Application, the media scanner may not have been updated, preventing the app from detecting the file. In this case, run the following command in adb shell to trigger media scanning. ```sh adb adb shell am broadcast -a android.intent.action.MEDIA_SCANNER_SCAN_FILE -d file:///storage/emulated/0/Pictures/sample.jpg ``` * After confirming a proper connection with Device Farm, select Tools -> Device Manager in Android Studio. ![](images/sample_application/device_manager_1.png) * After that check if the physical device is properly connected. ![](images/sample_application/samsung_electronics_1.png) * Run the Depth Estimation project from the Sample Applications obtained via git clone in Android Studio. ![](images/sample_application/depth_estimation_android.png) * When you select the depth-estimation project Android Studio will automatically perform a gradle sync. * The application in the selected project checks the connected device and runs when you click the Run button. ![](images/sample_application/run_1.png) * Please select the image mode and provide the data to be used for inference. ![](images/sample_application/camera_1.png) **4\\. I/O Specs** | INPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | Image | An RGB image | \\[1, 3, 256, 256\\] | float32 | | OUTPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | Classes | Depth | \\[1, 256, 256\\] | float32 | **5\\.",
      "<2-hop>\n\nDepth Estimation Application Workflow** The Depth Estimation Sample Application follows these steps to produce results. 1. The model execution preparation sequence is as follows. 1-1. Initialize the framework using the ennInitialize function. 1-2. Load the ML model into the framework using the ennOpenModel function. 1-3. Allocate and commit the necessary buffers using the ennAllocateAllBuffers function. 2. The model execution sequence is as follows. 2-1. Set the input data as parameters. 2-2. Call the ennExecute function. 2-3. Receive the execution results. 3. The method for releasing the framework initialization is as follows. 3-1. Release the allocated memory of the buffers using the ennReleaseBuffers function. 3-2. Close the model and release other resources using the ennCloseModel function. 3-3. Deinitialize the framework using the ennDeinitialize function. ※ If you want to learn more about the detailed functions, please refer to the [Framework API](#bookmark=id.3o7alnk) at the bottom of the document. **6\\. Checklist when replacing model** 1\\. Model Filename (MidasV2) * Replace the model file (midas\\_v2) by adding it to the assets directory and entering the new model filename in the ModelConstants.kt file. ![](images/sample_application/depth_estimation_directory.png) ![](images/sample_application/depth_estimation_modelconstants.png) ```kotlin const val MODEL_NAME = \"madas_v2.nnc\" ``` 2\\. Change the input data type in ModelConstants.kt (INPUT\\_DATA\\_TYPE, INPUT\\_DATA\\_LAYER) * Other models may accept FLOAT32 format input, so verification is required. * Check whether the input format is NHWC, HWC, or CHW before making changes. ```kotlin val INPUT_DATA_TYPE = DataType.FLOAT32 // Change it to the data type required by the model val INPUT_DATA_LAYER = LayerType.CHW // Change it to the layer required by the model ``` 3\\. Modify the input resolution (INPUT_SIZE_W, INPUT_SIZE_H, INPUT_SIZE_C) in ModelConstants.kt * You must reflect the input resolution used by the model. ```kotlin const val INPUT_SIZE_W = 256 // If the model uses 257x257, modify it accordingly const val INPUT_SIZE_H = 256 const val INPUT_SIZE_C = 3 // If 3(RGB) ``` 4\\. Normalize input in the ModelConstants.kt file (INPUT_CONVERSION_SCALE, INPUT_CONVERSION_OFFSET) * You need to check and modify the normalization method used by the model. Input normalization formula normalized_input \\= (raw\\_input−INPUT\\_CONVERSION\\_OFFSET)/INPUT\\_CONVERSION\\_SCALE ```kotlin const val INPUT_CONVERSION_SCALE = 127.5F // Scale the input data by 127.5 times const val INPUT_CONVERSION_OFFSET = 127.5F // A commonly used technique in deep learning for symmetrically normalizing input data based on the central value of 127.5 when the input pixel values range from 0 to 255. This method subtracts 127.5 from the raw input (raw_input) to obtain a value centered around 0 ``` 5\\. Set the output data in the ModelConstants.kt file (OUTPUT\\_DATA\\_TYPE, OUTPUT\\_SIZE\\_W, OUTPUT\\_SIZE\\_H) * Check and modify the output size of the model accordingly. ```kotlin val OUTPUT_DATA_TYPE = DataType.FLOAT32 const val OUTPUT_SIZE_W = 256 const val OUTPUT_SIZE_H = 256 ``` 6\\. Output normalization (OUTPUT\\_CONVERSION\\_SCALE, OUTPUT\\_CONVERSION\\_OFFSET) * You need to check the normalization method used by the model before making any changes. * Output normalization formula final\\_output=(model\\_output×OUTPUT\\_CONVERSION\\_SCALE)+OUTPUT\\_CONVERSION\\_OFFSET ```kotlin const val OUTPUT_CONVERSION_SCALE = 1F // The output data is kept in its original form without scaling const val OUTPUT_CONVERSION_OFFSET = 0F // Since the formula is (model\\_output × 1\\) \\+ 0 the output data remains unchanged and is used as is without any transformation ``` 7\\. Modify the preProcess() and postProcess() functions in the ModelExecutor.kt file located at the following path ![](images/sample_application/depth_estimation_modelexecutor.png) * If the model's input or output data differs from the predefined format, modify the preProcess() and postProcess() functions accordingly. ```kotlin Class ModelExecutor -> preProcess(), postProcess() ``` **7\\. Compatible AI Models** * **Reference** * [**Framework API**](https://prd.ai-studio-farm.com/global/development/enn-sdk/document/api-reference/enn-framework-api-functions)",
      "<3-hop>\n\nThis document explains how a simple Android Sample Application operates using the [Densenet121](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/118f8cc6-f251-43b7-b8c2-ec77a3c50fda) model optimized for Exynos hardware. **1\\. Functionality** This application classifies objects in images either from stored image files or captured via the camera. The classified items, corresponding scores, and inference time are displayed at the bottom of the application interface. ![](images/sample_application/mouse.png) **2\\. Prerequisites** * To download [Android Studio](https://developer.android.com/studio), go to the official website and download it. * To run the Sample Application, clone the following repository. ```sh git clone https://github.com/exynos-eco/enn-sdk-samples-9945.git ``` * If there is no device available to run the application, you can use a real device provided in the Device Farm. For guidance on connecting a device in Android Studio, refer to the ADB proxy guide. **3\\. Starting Sample Application** * Open the cmd window and check if adb is connected. ```sh adb device List of devices attached 000011b58d246013 device ``` * Push a sample image for testing to the following path using adb push. ```sh adb push sample.jpg /storage/emulated/0/Pictures/ ``` * If the sample image is not visible after running the Sample Application, the media scanner may not have been updated, preventing the app from detecting the file. In this case, run the following command in adb shell to trigger media scanning. ```sh adb adb shell am broadcast -a android.intent.action.MEDIA_SCANNER_SCAN_FILE -d file:///storage/emulated/0/Pictures/sample.jpg ``` * After confirming a proper connection with Device Farm, select Tools \\-\\> Device Manager in Android Studio. ![](images/sample_application/device_manager_1.png) * After that check if the physical device is properly connected. ![](images/sample_application/samsung_electronics_1.png) * Run the Image Classification project from the Sample Applications obtained via git clone in Android Studio. ![](images/sample_application/image_classification_1.png) * When you select the image-classification project Android Studio will automatically perform a gradle sync. * The application in the selected project checks the connected device and runs when you click the Run button. ![](images/sample_application/run_1.png) * Select image mode and provide the data to be used for inference. ![](images/sample_application/camera_1.png) **4\\."
    ],
    "reference": "The Depth Estimation Application workflow for the Android Sample Application involves several key steps. First, the model execution preparation sequence includes initializing the framework using the ennInitialize function, loading the ML model into the framework with the ennOpenModel function, and allocating necessary buffers using the ennAllocateAllBuffers function. Next, the model execution sequence is carried out by setting the input data as parameters, calling the ennExecute function, and receiving the execution results. Finally, the framework is released by freeing the allocated memory of the buffers with the ennReleaseBuffers function, closing the model with ennCloseModel, and deinitializing the framework using ennDeinitialize. This workflow effectively utilizes the MiDaS v2 model to provide depth estimation from images.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation\\documentation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:57:11.112083+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do you check if adb is connected and what command is used to trigger media scanning in the Android Sample Application?",
    "reference_contexts": [
      "<1-hop>\n\nThis document explains how a simple Android Sample Application operates using the [Densenet121](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/118f8cc6-f251-43b7-b8c2-ec77a3c50fda) model optimized for Exynos hardware. **1\\. Functionality** This application classifies objects in images either from stored image files or captured via the camera. The classified items, corresponding scores, and inference time are displayed at the bottom of the application interface. ![](images/sample_application/mouse.png) **2\\. Prerequisites** * To download [Android Studio](https://developer.android.com/studio), go to the official website and download it. * To run the Sample Application, clone the following repository. ```sh git clone https://github.com/exynos-eco/enn-sdk-samples-9945.git ``` * If there is no device available to run the application, you can use a real device provided in the Device Farm. For guidance on connecting a device in Android Studio, refer to the ADB proxy guide. **3\\. Starting Sample Application** * Open the cmd window and check if adb is connected. ```sh adb device List of devices attached 000011b58d246013 device ``` * Push a sample image for testing to the following path using adb push. ```sh adb push sample.jpg /storage/emulated/0/Pictures/ ``` * If the sample image is not visible after running the Sample Application, the media scanner may not have been updated, preventing the app from detecting the file. In this case, run the following command in adb shell to trigger media scanning. ```sh adb adb shell am broadcast -a android.intent.action.MEDIA_SCANNER_SCAN_FILE -d file:///storage/emulated/0/Pictures/sample.jpg ``` * After confirming a proper connection with Device Farm, select Tools \\-\\> Device Manager in Android Studio. ![](images/sample_application/device_manager_1.png) * After that check if the physical device is properly connected. ![](images/sample_application/samsung_electronics_1.png) * Run the Image Classification project from the Sample Applications obtained via git clone in Android Studio. ![](images/sample_application/image_classification_1.png) * When you select the image-classification project Android Studio will automatically perform a gradle sync. * The application in the selected project checks the connected device and runs when you click the Run button. ![](images/sample_application/run_1.png) * Select image mode and provide the data to be used for inference. ![](images/sample_application/camera_1.png) **4\\.",
      "<2-hop>\n\nThis document explains how a simple Android Sample Application operates using the [MiDaS v2](https://prd.ai-studio-farm.com/kr/solution/ai/models/detail/73bcae23-4a07-4df9-b0c5-80504427c11c) model optimized for Exynos hardware. **1\\. Functionality** This Application receives input from an image file or a camera. A color representing the estimated distance is overlaid on each pixel, providing a visual representation of depth. Additionally, the inference time is displayed at the bottom of the application interface. ![](images/sample_application/depth_estimation.png) **2\\. Prerequisites** * To download [Android Studio](https://developer.android.com/studio) go to the official website and download it. * To run the Sample Application, clone the following repository. ```sh git clone https://github.com/exynos-eco/enn-sdk-samples-9945.git ``` * If there is no device available to run the application, you can use a real device provided in the Device Farm. For guidance on connecting a device in Android Studio, refer to the ADB proxy guide. **3\\. Starting Sample Application** * Open the cmd window and check if adb is connected. ```sh adb device List of devices attached 000011b58d246013 device ``` * Push a sample image for testing to the following path using adb push. ```sh adb push sample.jpg /storage/emulated/0/Pictures/ ``` * If the sample image is not visible after running the Sample Application, the media scanner may not have been updated, preventing the app from detecting the file. In this case, run the following command in adb shell to trigger media scanning. ```sh adb adb shell am broadcast -a android.intent.action.MEDIA_SCANNER_SCAN_FILE -d file:///storage/emulated/0/Pictures/sample.jpg ``` * After confirming a proper connection with Device Farm, select Tools -> Device Manager in Android Studio. ![](images/sample_application/device_manager_1.png) * After that check if the physical device is properly connected. ![](images/sample_application/samsung_electronics_1.png) * Run the Depth Estimation project from the Sample Applications obtained via git clone in Android Studio. ![](images/sample_application/depth_estimation_android.png) * When you select the depth-estimation project Android Studio will automatically perform a gradle sync. * The application in the selected project checks the connected device and runs when you click the Run button. ![](images/sample_application/run_1.png) * Please select the image mode and provide the data to be used for inference. ![](images/sample_application/camera_1.png) **4\\. I/O Specs** | INPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | Image | An RGB image | \\[1, 3, 256, 256\\] | float32 | | OUTPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | Classes | Depth | \\[1, 256, 256\\] | float32 | **5\\."
    ],
    "reference": "To check if adb is connected, open the cmd window and run the command `adb device`. This will list the devices attached, for example, `000011b58d246013 device`. If the sample image is not visible after running the Sample Application, you can trigger media scanning by using the command `adb shell am broadcast -a android.intent.action.MEDIA_SCANNER_SCAN_FILE -d file:///storage/emulated/0/Pictures/sample.jpg`.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation\\documentation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:57:11.112083+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What does EnnInitialize do in the Enn Framework?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnInitialize](#function-enninitialize)**(void )<br>Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair.  |\n| EnnReturn | **[EnnDeinitialize](#function-enndeinitialize)**(void )<br>Deinitialize Enn Framework. Framework degenerates context in a caller's process.  |"
    ],
    "reference": "EnnInitialize initializes the Enn Framework, generating context in a caller's process and counting the initialize/deinitialize pair.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:57:55.582094+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What role does context play in the Enn Framework initialization process?",
    "reference_contexts": [
      "```cpp\nEnnReturn EnnInitialize(\nvoid\n)\n```  \nInitialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair.  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "In the Enn Framework, context is generated in a caller's process during the initialization. It counts the initialize/deinitialize pair, which is essential for managing the framework's state.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:57:55.582094+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What does EnnReturn do?",
    "reference_contexts": [
      "```cpp\nEnnReturn EnnDeinitialize(\nvoid\n)\n```  \nDeinitialize Enn Framework. Framework degenerates context in a caller's process.  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "EnnReturn is the return type of the EnnDeinitialize function, which deinitializes the Enn Framework and returns a result, where 0 indicates success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:57:55.582094+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What does EnnOpenModel do?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnOpenModel](#function-ennopenmodel)**(const char * model_file, EnnModelId * model_id)<br>OpenModel with model file.  |\n| EnnReturn | **[EnnOpenModelFromMemory](#function-ennopenmodelfrommemory)**(const char * va, const uint32_t size, EnnModelId * model_id)<br>OpenModel from memory buffer.  |\n| EnnReturn | **[EnnCloseModel](#function-ennclosemodel)**(const EnnModelId model_id);<br>Close model and free all resources in OpenModel()|"
    ],
    "reference": "EnnOpenModel opens a model with a specified model file.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:57:55.582094+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does the output from graph-gen relate to the EnnOpenModel function?",
    "reference_contexts": [
      "```cpp\nEnnReturn EnnOpenModel(\nconst char * model_file,\nEnnModelId * model_id\n)\n```  \nOpenModel with model file.  \n**Parameters**:  \n* **model_file** [IN] model_file, output from graph-gen. A caller should access the file.\n* **model_id** [OUT] model_id, 64 bit unsigned int  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "The output from graph-gen is used as the model_file parameter in the EnnOpenModel function, which is required to open a model.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:57:55.582094+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What does the parameter 'va' represent in the function EnnOpenModelFromMemory?",
    "reference_contexts": [
      "```cpp\nEnnReturn EnnOpenModelFromMemory(\nconst char * va,\nconst uint32_t size,\nEnnModelId * model_id\n)\n```  \nOpenModel from memory buffer.  \n**Parameters**:  \n* **va** [IN] address which a model loaded from\n* **size** [IN] size of the buffer\n* **model_id** [OUT] model_id, 64 bit unsigned int  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "'va' is the address from which a model is loaded in the function EnnOpenModelFromMemory.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:57:55.582094+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What does EnnReturn signify in the context of EnnCloseModel?",
    "reference_contexts": [
      "```cpp\nEnnReturn EnnCloseModel(\nconst EnnModelId model_id\n);\n```  \nClose model and free all resources in OpenModel().  \n**Parameters**:  \n* **model_id** [IN] model_id from OpenModel()  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "EnnReturn signifies the result of the EnnCloseModel function, where a return value of 0 indicates success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:57:55.582094+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the function of EnnReleaseBuffers in buffer management?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnCreateBuffer](#function-enncreatebuffer)**([EnnBufferPtr](api-reference/enn-framework-data-type-references/#_ennbuffer) * out, const uint32_t req_size, const bool is_cached =true)<br>Create Buffer with request size support ION or dmabufheap which can be used in a device(DSP/CPU/NPU/GPU)  |\n| EnnReturn | **[EnnAllocateAllBuffers](#function-ennallocateallbuffers)**(const EnnModelId model_id, [EnnBufferPtr](api-reference/enn-framework-data-type-references/#_ennbuffer) ** out_buffers, [NumberOfBuffersInfo](api-reference/enn-framework-data-type-references/#_numberofbuffersinfo) * buf_info, const int session_id =0, const bool do_commit =true)<br>Allocate all buffers which a caller should allocate.  |\n| EnnReturn | **[EnnReleaseBuffers](#function-ennreleasebuffers)**([EnnBufferPtr](api-reference/enn-framework-data-type-references/#_ennbuffer) * buffers, const int32_t numOfBuffers)<br>Release buffer array from EnnAllocatedAllBuffers() This API includes releasing all elements in the array.  |\n| EnnReturn | **[EnnReleaseBuffer](#function-ennreleasebuffer)**([EnnBufferPtr](api-reference/enn-framework-data-type-references/#_ennbuffer) buffer)<br>release buffer from [EnnCreateBuffer()](#function-enncreatebuffer) |"
    ],
    "reference": "EnnReleaseBuffers is used to release a buffer array that was allocated by EnnAllocateAllBuffers(). This API includes releasing all elements in the array.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:57:55.582094+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the process for obtaining meta information about a model using the Enn framework, and how does it relate to setting buffer information for that model?",
    "reference_contexts": [
      "<1-hop>\n\n|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnGetBuffersInfo](#function-enngetbuffersinfo)**([NumberOfBuffersInfo](api-reference/enn-framework-data-type-references/#_numberofbuffersinfo) * buffers_info, const EnnModelId model_id)<br>Get buffers information from loaded model.  |\n| EnnReturn | **[EnnGetBufferInfoByIndex](#function-enngetbufferinfobyindex)**([EnnBufferInfo](api-reference/enn-framework-data-type-references/#_ennbufferinfo) * out_buf_info, const EnnModelId model_id, const enn_buf_dir_e direction, const uint32_t index)<br>Get one buffer information from loaded model.  |\n| EnnReturn | **[EnnGetBufferInfoByLabel](#function-enngetbufferinfobylabel)**([EnnBufferInfo](api-reference/enn-framework-data-type-references/#_ennbufferinfo) * out_buf_info, const EnnModelId model_id, const char * label)<br>Get one buffer information from loaded model.  |\n| EnnReturn | **[EnnSetBufferByIndex](#function-ennsetbufferbyindex)**(const EnnModelId model_id, const enn_buf_dir_e direction, const uint32_t index, [EnnBufferPtr](api-reference/enn-framework-data-type-references/#_ennbuffer) buf, const int session_id =0)<br>Set memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core.  |\n| EnnReturn | **[EnnSetBufferByLabel](#function-ennsetbufferbylabel)**(const EnnModelId model_id, const char * label, [EnnBufferPtr](api-reference/enn-framework-data-type-references/#_ennbuffer) buf, const int session_id =0)<br>Set memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core.  |",
      "<2-hop>\n\n```cpp\nEnnReturn EnnGetMetaInfo(\nconst EnnMetaTypeId info_id,\nconst EnnModelId model_id,\nchar output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]\n)\n```  \nGet Meta Information.  \n**Parameters**:  \n* **info_id** info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done.  \n```cpp\nENN_META_VERSION_FRAMEWORK\nENN_META_VERSION_COMMIT\nENN_META_VERSION_MODEL_COMPILER_NNC\nENN_META_VERSION_MODEL_COMPILER_NPU\nENN_META_VERSION_MODEL_COMPILER_DSP\nENN_META_VERSION_MODEL_SCHEMA\nENN_META_VERSION_MODEL_VERSION\nENN_META_VERSION_DD\nENN_META_VERSION_UNIFIED_FW\nENN_META_VERSION_NPU_FW\nENN_META_VERSION_DSP_FW\n```\n* **model_id**\n* **output_str**  \n**Return**: EnnReturn result, 0 is success  \nThis API includes loaded model information as well as framework information",
      "<3-hop>\n\n```cpp\nEnnReturn EnnOpenModelFromMemory(\nconst char * va,\nconst uint32_t size,\nEnnModelId * model_id\n)\n```  \nOpenModel from memory buffer.  \n**Parameters**:  \n* **va** [IN] address which a model loaded from\n* **size** [IN] size of the buffer\n* **model_id** [OUT] model_id, 64 bit unsigned int  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "To obtain meta information about a model using the Enn framework, one would use the function `EnnGetMetaInfo`, which requires parameters such as `info_id`, `model_id`, and an output string to store the information. The `info_id` can specify various types of meta information, including framework version and model compiler details. This function returns an `EnnReturn` result, where a return value of 0 indicates success. Additionally, to set buffer information for the model, functions like `EnnSetBufferByIndex` or `EnnSetBufferByLabel` can be utilized. These functions allow users to allocate memory objects to commit-space, enabling the model to run effectively. Thus, obtaining meta information and setting buffer information are interconnected processes that facilitate the management and execution of models within the Enn framework.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:57:55.582094+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Wht is the result of EnnSetPreferencePerfMode and EnnOpenModel?",
    "reference_contexts": [
      "<1-hop>\n\n```cpp\nEnnReturn EnnSetPreferencePerfMode(\nconst uint32_t val\n)\n```  \nSetting Performance Mode.  \n**Parameters**:  \n* **val** [IN] value to set Performance Mode  \n**Return**: EnnReturn result, 0 is success",
      "<2-hop>\n\n```cpp\nEnnReturn EnnOpenModel(\nconst char * model_file,\nEnnModelId * model_id\n)\n```  \nOpenModel with model file.  \n**Parameters**:  \n* **model_file** [IN] model_file, output from graph-gen. A caller should access the file.\n* **model_id** [OUT] model_id, 64 bit unsigned int  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "The result of EnnSetPreferencePerfMode is an EnnReturn value, where 0 indicates success. Similarly, the result of EnnOpenModel is also an EnnReturn value, with 0 signifying success.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:57:55.582094+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What does EnnExecuteModelWait do?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnExecuteModel](Modules/group__api__execute.md#function-ennexecutemodel)**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model with commited buffers.  |\n| EnnReturn | **[EnnExecuteModelAsync](Modules/group__api__execute.md#function-ennexecutemodelasync)**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model in background asynchronously.  |\n| EnnReturn | **[EnnExecuteModelWait](Modules/group__api__execute.md#function-ennexecutemodelwait)**(const EnnModelId model_id, const int session_id =0)<br>Wait result of calling [EnnExecuteModelAsync()](Modules/group__api__execute.md#function-ennexecutemodelasync) If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished.  |"
    ],
    "reference": "EnnExecuteModelWait waits for the result of calling EnnExecuteModelAsync. If execution is finished, this function returns immediately; if not, it blocks until the execution is finished.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:58:26.489575+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What does EnnReturn do in the context of model execution?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnExecuteModel](Modules/group__api__execute.md#function-ennexecutemodel)**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model with commited buffers.  |\n| EnnReturn | **[EnnExecuteModelAsync](Modules/group__api__execute.md#function-ennexecutemodelasync)**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model in background asynchronously.  |\n| EnnReturn | **[EnnExecuteModelWait](Modules/group__api__execute.md#function-ennexecutemodelwait)**(const EnnModelId model_id, const int session_id =0)<br>Wait result of calling [EnnExecuteModelAsync()](Modules/group__api__execute.md#function-ennexecutemodelasync) If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished.  |"
    ],
    "reference": "EnnReturn is associated with several functions for executing models, including EnnExecuteModel, which requests the service core to execute a model with committed buffers, EnnExecuteModelAsync, which requests the service core to execute a model in the background asynchronously, and EnnExecuteModelWait, which waits for the result of calling EnnExecuteModelAsync, blocking until execution is finished if it is not already complete.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:58:26.489575+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What EnnReturn do when execute model?",
    "reference_contexts": [
      "```\nEnnReturn EnnExecuteModel(\nconst EnnModelId model_id,\nconst int session_id =0\n)\n```  \nRequest to service core to execute model with commited buffers.  \n**Parameters**:  \n* **model_id** [IN] model ID from load_model\n* **session_id** [IN] session ID  \n**Return**: EnnReturn result, 0 is success  \n**Note**: this function runs in block mode"
    ],
    "reference": "EnnReturn is used to request the service core to execute a model with committed buffers, and it returns a result where 0 indicates success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:58:26.489575+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the purpose of EnnModelId in the EnnExecuteModel function?",
    "reference_contexts": [
      "```\nEnnReturn EnnExecuteModel(\nconst EnnModelId model_id,\nconst int session_id =0\n)\n```  \nRequest to service core to execute model with commited buffers.  \n**Parameters**:  \n* **model_id** [IN] model ID from load_model\n* **session_id** [IN] session ID  \n**Return**: EnnReturn result, 0 is success  \n**Note**: this function runs in block mode"
    ],
    "reference": "EnnModelId is used as a parameter in the EnnExecuteModel function to specify the model ID from load_model that is to be executed.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:58:26.489575+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the role of session_id in the EnnExecuteModelAsync function?",
    "reference_contexts": [
      "```\nEnnReturn EnnExecuteModelAsync(\nconst EnnModelId model_id,\nconst int session_id =0\n)\n```  \nRequest to service core to execute model in background asynchronously.  \n**Parameters**:  \n* **model_id** [IN] model ID from load_model\n* **session_id** [IN] session ID  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "In the EnnExecuteModelAsync function, the session_id is an input parameter that represents the session ID used when executing the model in the background asynchronously.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:58:26.489575+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What does model_id represent in the EnnExecuteModelAsync function?",
    "reference_contexts": [
      "```\nEnnReturn EnnExecuteModelAsync(\nconst EnnModelId model_id,\nconst int session_id =0\n)\n```  \nRequest to service core to execute model in background asynchronously.  \n**Parameters**:  \n* **model_id** [IN] model ID from load_model\n* **session_id** [IN] session ID  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "In the EnnExecuteModelAsync function, model_id represents the model ID that is obtained from the load_model function.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:58:26.489575+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What updates were made to the EnnExecuteModelWait function on 2023-08-11?",
    "reference_contexts": [
      "```\nEnnReturn EnnExecuteModelWait(\nconst EnnModelId model_id,\nconst int session_id =0\n)\n```  \nWait result of calling [EnnExecuteModelAsync()](Modules/group__api__execute.md#function-ennexecutemodelasync) If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished.  \n**Parameters**:  \n* **model_id** [IN] model ID from load_model\n* **session_id** [IN] session ID  \n**Return**: EnnReturn result, 0 is success  \n-------------------------------  \nUpdated on 2023-08-11 at 16:24:05 +0900"
    ],
    "reference": "The EnnExecuteModelWait function was updated on 2023-08-11 at 16:24:05 +0900.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:58:26.489575+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the significance of the date 2023-08-11 in the context of the provided information?",
    "reference_contexts": [
      "```\nEnnReturn EnnExecuteModelWait(\nconst EnnModelId model_id,\nconst int session_id =0\n)\n```  \nWait result of calling [EnnExecuteModelAsync()](Modules/group__api__execute.md#function-ennexecutemodelasync) If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished.  \n**Parameters**:  \n* **model_id** [IN] model ID from load_model\n* **session_id** [IN] session ID  \n**Return**: EnnReturn result, 0 is success  \n-------------------------------  \nUpdated on 2023-08-11 at 16:24:05 +0900"
    ],
    "reference": "The date 2023-08-11 is significant as it marks the update time for the information provided, specifically noting that the content was updated on this date at 16:24:05 +0900.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:58:26.489575+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the purpose of the EnnExecuteModelWait function in relation to the session ID?",
    "reference_contexts": [
      "<1-hop>\n\n|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnExecuteModel](Modules/group__api__execute.md#function-ennexecutemodel)**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model with commited buffers.  |\n| EnnReturn | **[EnnExecuteModelAsync](Modules/group__api__execute.md#function-ennexecutemodelasync)**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model in background asynchronously.  |\n| EnnReturn | **[EnnExecuteModelWait](Modules/group__api__execute.md#function-ennexecutemodelwait)**(const EnnModelId model_id, const int session_id =0)<br>Wait result of calling [EnnExecuteModelAsync()](Modules/group__api__execute.md#function-ennexecutemodelasync) If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished.  |",
      "<2-hop>\n\n```\nEnnReturn EnnExecuteModelWait(\nconst EnnModelId model_id,\nconst int session_id =0\n)\n```  \nWait result of calling [EnnExecuteModelAsync()](Modules/group__api__execute.md#function-ennexecutemodelasync) If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished.  \n**Parameters**:  \n* **model_id** [IN] model ID from load_model\n* **session_id** [IN] session ID  \n**Return**: EnnReturn result, 0 is success  \n-------------------------------  \nUpdated on 2023-08-11 at 16:24:05 +0900"
    ],
    "reference": "The EnnExecuteModelWait function is designed to wait for the result of calling EnnExecuteModelAsync. It checks if the execution is finished; if so, it returns immediately. If not, it blocks until the execution is completed. The function takes a session ID as a parameter, which is used to identify the specific session for the model execution.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:58:26.489575+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the difference between EnnExecuteModelWait and EnnExecuteModelAsync in executing models?",
    "reference_contexts": [
      "<1-hop>\n\n```\nEnnReturn EnnExecuteModelAsync(\nconst EnnModelId model_id,\nconst int session_id =0\n)\n```  \nRequest to service core to execute model in background asynchronously.  \n**Parameters**:  \n* **model_id** [IN] model ID from load_model\n* **session_id** [IN] session ID  \n**Return**: EnnReturn result, 0 is success",
      "<2-hop>\n\n|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnExecuteModel](Modules/group__api__execute.md#function-ennexecutemodel)**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model with commited buffers.  |\n| EnnReturn | **[EnnExecuteModelAsync](Modules/group__api__execute.md#function-ennexecutemodelasync)**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model in background asynchronously.  |\n| EnnReturn | **[EnnExecuteModelWait](Modules/group__api__execute.md#function-ennexecutemodelwait)**(const EnnModelId model_id, const int session_id =0)<br>Wait result of calling [EnnExecuteModelAsync()](Modules/group__api__execute.md#function-ennexecutemodelasync) If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished.  |"
    ],
    "reference": "EnnExecuteModelAsync requests the service core to execute a model in the background asynchronously, allowing other processes to continue without waiting for the model execution to finish. In contrast, EnnExecuteModelWait waits for the result of the EnnExecuteModelAsync call; if the execution is finished, it returns immediately, but if not, it blocks until the execution is completed.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:58:26.489575+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain what EnnBufferPtr is and how it is used in memory management functions like EnnCreateBuffer and EnnReleaseBuffer?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnCreateBuffer](Modules/group__api__memory.md#function-enncreatebuffer)**([EnnBufferPtr](Classes/struct__enn_buffer.md) * out, const uint32_t req_size, const bool is_cached =true)<br>Create Buffer with request size support ION or dmabufheap which can be used in a device(DSP/CPU/NPU/GPU)  |\n| EnnReturn | **[EnnAllocateAllBuffers](Modules/group__api__memory.md#function-ennallocateallbuffers)**(const EnnModelId model_id, [EnnBufferPtr](Classes/struct__enn_buffer.md) ** out_buffers, [NumberOfBuffersInfo](Classes/struct___number_of_buffers_info.md) * buf_info, const int session_id =0, const bool do_commit =true)<br>Allocate all buffers which a caller should allocate.  |\n| EnnReturn | **[EnnReleaseBuffers](Modules/group__api__memory.md#function-ennreleasebuffers)**([EnnBufferPtr](Classes/struct__enn_buffer.md) * buffers, const int32_t numOfBuffers)<br>Release buffer array from EnnAllocatedAllBuffers() This API includes releasing all elements in the array.  |\n| EnnReturn | **[EnnReleaseBuffer](Modules/group__api__memory.md#function-ennreleasebuffer)**([EnnBufferPtr](Classes/struct__enn_buffer.md) buffer)<br>release buffer from [EnnCreateBuffer()](Modules/group__api__memory.md#function-enncreatebuffer) |"
    ],
    "reference": "EnnBufferPtr is a pointer type used in various memory management functions. For instance, in the function EnnCreateBuffer, it is used to create a buffer with a requested size that supports ION or dmabufheap, which can be utilized in devices such as DSP, CPU, NPU, or GPU. Additionally, EnnReleaseBuffer uses EnnBufferPtr to release a buffer that was created by the EnnCreateBuffer function.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__memory.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:58:44.679786+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the functions associated with EnnReturn in memory management?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnCreateBuffer](Modules/group__api__memory.md#function-enncreatebuffer)**([EnnBufferPtr](Classes/struct__enn_buffer.md) * out, const uint32_t req_size, const bool is_cached =true)<br>Create Buffer with request size support ION or dmabufheap which can be used in a device(DSP/CPU/NPU/GPU)  |\n| EnnReturn | **[EnnAllocateAllBuffers](Modules/group__api__memory.md#function-ennallocateallbuffers)**(const EnnModelId model_id, [EnnBufferPtr](Classes/struct__enn_buffer.md) ** out_buffers, [NumberOfBuffersInfo](Classes/struct___number_of_buffers_info.md) * buf_info, const int session_id =0, const bool do_commit =true)<br>Allocate all buffers which a caller should allocate.  |\n| EnnReturn | **[EnnReleaseBuffers](Modules/group__api__memory.md#function-ennreleasebuffers)**([EnnBufferPtr](Classes/struct__enn_buffer.md) * buffers, const int32_t numOfBuffers)<br>Release buffer array from EnnAllocatedAllBuffers() This API includes releasing all elements in the array.  |\n| EnnReturn | **[EnnReleaseBuffer](Modules/group__api__memory.md#function-ennreleasebuffer)**([EnnBufferPtr](Classes/struct__enn_buffer.md) buffer)<br>release buffer from [EnnCreateBuffer()](Modules/group__api__memory.md#function-enncreatebuffer) |"
    ],
    "reference": "EnnReturn is associated with several functions in memory management, including EnnCreateBuffer, which creates a buffer with a requested size; EnnAllocateAllBuffers, which allocates all buffers for a given model; EnnReleaseBuffers, which releases an array of buffers; and EnnReleaseBuffer, which releases a single buffer.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__memory.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:58:44.679786+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What EnnBufferPtr do in EnnCreateBuffer?",
    "reference_contexts": [
      "```\nEnnReturn EnnCreateBuffer(\nEnnBufferPtr * out,\nconst uint32_t req_size,\nconst bool is_cached =true\n)\n```  \nCreate Buffer with request size support ION or dmabufheap which can be used in a device(DSP/CPU/NPU/GPU)  \n**Parameters**:  \n* **out** [OUT] output buffer pointer. User can get va, size, offset through *out\n* **req_size** [IN] request size\n* **is_cached** [IN] flag, the buffer uses cache or not  \n**Return**: EnnReturn"
    ],
    "reference": "EnnBufferPtr is an output buffer pointer in the EnnCreateBuffer function, allowing the user to get the virtual address, size, and offset through it.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__memory.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:58:44.679786+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the parameters required for the EnnCreateBuffer function?",
    "reference_contexts": [
      "```\nEnnReturn EnnCreateBuffer(\nEnnBufferPtr * out,\nconst uint32_t req_size,\nconst bool is_cached =true\n)\n```  \nCreate Buffer with request size support ION or dmabufheap which can be used in a device(DSP/CPU/NPU/GPU)  \n**Parameters**:  \n* **out** [OUT] output buffer pointer. User can get va, size, offset through *out\n* **req_size** [IN] request size\n* **is_cached** [IN] flag, the buffer uses cache or not  \n**Return**: EnnReturn"
    ],
    "reference": "The EnnCreateBuffer function requires three parameters: 'out', which is an output buffer pointer that allows the user to get the virtual address, size, and offset; 'req_size', which is the requested size; and 'is_cached', which is a flag indicating whether the buffer uses cache or not.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__memory.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:58:44.679786+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the EnnModelId used for in the EnnAllocateAllBuffers function?",
    "reference_contexts": [
      "```\nEnnReturn EnnAllocateAllBuffers(\nconst EnnModelId model_id,\nEnnBufferPtr ** out_buffers,\nNumberOfBuffersInfo * buf_info,\nconst int session_id =0,\nconst bool do_commit =true\n)\n```  \nAllocate all buffers which a caller should allocate.  \n**Parameters**:  \n* **model_id** model_id from OpenModel()\n* **out_buffers** [OUT] pointer of EnnBuffer array\n* **buf_info** [OUT] size of the array\n* **session_id** [IN] after generate buffer space, user can set this field if session_id > 0\n* **do_commit** [IN] if true, the framework tries to commit after buffer allocation  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "The EnnModelId is a parameter in the EnnAllocateAllBuffers function, which is used to specify the model_id from OpenModel().",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__memory.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:58:44.679786+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What does EnnReturn indicate in the context of buffer allocation?",
    "reference_contexts": [
      "```\nEnnReturn EnnAllocateAllBuffers(\nconst EnnModelId model_id,\nEnnBufferPtr ** out_buffers,\nNumberOfBuffersInfo * buf_info,\nconst int session_id =0,\nconst bool do_commit =true\n)\n```  \nAllocate all buffers which a caller should allocate.  \n**Parameters**:  \n* **model_id** model_id from OpenModel()\n* **out_buffers** [OUT] pointer of EnnBuffer array\n* **buf_info** [OUT] size of the array\n* **session_id** [IN] after generate buffer space, user can set this field if session_id > 0\n* **do_commit** [IN] if true, the framework tries to commit after buffer allocation  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "EnnReturn indicates the result of the buffer allocation process, where a return value of 0 signifies success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__memory.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:58:44.679786+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the purpose of the EnnReturn in the EnnReleaseBuffers API?",
    "reference_contexts": [
      "```\nEnnReturn EnnReleaseBuffers(\nEnnBufferPtr * buffers,\nconst int32_t numOfBuffers\n)\n```  \nRelease buffer array from EnnAllocatedAllBuffers() This API includes releasing all elements in the array.  \n**Parameters**:  \n* **buffers** [IN] pointer of buffer array\n* **numOfBuffers** [IN] size of bufefr array  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "The EnnReturn in the EnnReleaseBuffers API indicates the result of the operation, where a return value of 0 signifies success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__memory.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:58:44.679786+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the significance of the numOfBuffers parameter in the EnnReleaseBuffers API?",
    "reference_contexts": [
      "```\nEnnReturn EnnReleaseBuffers(\nEnnBufferPtr * buffers,\nconst int32_t numOfBuffers\n)\n```  \nRelease buffer array from EnnAllocatedAllBuffers() This API includes releasing all elements in the array.  \n**Parameters**:  \n* **buffers** [IN] pointer of buffer array\n* **numOfBuffers** [IN] size of bufefr array  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "The numOfBuffers parameter in the EnnReleaseBuffers API specifies the size of the buffer array that is being released. It is an input parameter that indicates how many elements are present in the buffers array.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__memory.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:58:44.679786+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the process of buffer creation using EnnCreateBuffer, and how does it relate to the release of buffers with EnnReleaseBuffer?",
    "reference_contexts": [
      "<1-hop>\n\n```\nEnnReturn EnnReleaseBuffer(\nEnnBufferPtr buffer\n)\n```  \nrelease buffer from [EnnCreateBuffer()](Modules/group__api__memory.md#function-enncreatebuffer)  \n**Parameters**:  \n* **buffer** [IN] buffer object from [EnnCreateBuffer()](Modules/group__api__memory.md#function-enncreatebuffer)  \n**Return**: EnnReturn result, 0 is success  \n-------------------------------  \nUpdated on 2023-08-11 at 16:24:05 +0900",
      "<2-hop>\n\n|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnCreateBuffer](Modules/group__api__memory.md#function-enncreatebuffer)**([EnnBufferPtr](Classes/struct__enn_buffer.md) * out, const uint32_t req_size, const bool is_cached =true)<br>Create Buffer with request size support ION or dmabufheap which can be used in a device(DSP/CPU/NPU/GPU)  |\n| EnnReturn | **[EnnAllocateAllBuffers](Modules/group__api__memory.md#function-ennallocateallbuffers)**(const EnnModelId model_id, [EnnBufferPtr](Classes/struct__enn_buffer.md) ** out_buffers, [NumberOfBuffersInfo](Classes/struct___number_of_buffers_info.md) * buf_info, const int session_id =0, const bool do_commit =true)<br>Allocate all buffers which a caller should allocate.  |\n| EnnReturn | **[EnnReleaseBuffers](Modules/group__api__memory.md#function-ennreleasebuffers)**([EnnBufferPtr](Classes/struct__enn_buffer.md) * buffers, const int32_t numOfBuffers)<br>Release buffer array from EnnAllocatedAllBuffers() This API includes releasing all elements in the array.  |\n| EnnReturn | **[EnnReleaseBuffer](Modules/group__api__memory.md#function-ennreleasebuffer)**([EnnBufferPtr](Classes/struct__enn_buffer.md) buffer)<br>release buffer from [EnnCreateBuffer()](Modules/group__api__memory.md#function-enncreatebuffer) |"
    ],
    "reference": "The process of buffer creation using EnnCreateBuffer involves calling the function with parameters that include a pointer to an output buffer, the requested size, and an optional boolean indicating if the buffer should be cached. This function creates a buffer that can be utilized by various devices such as DSP, CPU, NPU, or GPU. Once the buffer is no longer needed, it can be released using the EnnReleaseBuffer function, which takes the buffer object created by EnnCreateBuffer as a parameter. This ensures proper memory management by releasing the allocated resources back to the system.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__memory.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:58:44.679786+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the process for creating and releasing a buffer using EnnBufferPtr in the context of memory management?",
    "reference_contexts": [
      "<1-hop>\n\n```\nEnnReturn EnnCreateBuffer(\nEnnBufferPtr * out,\nconst uint32_t req_size,\nconst bool is_cached =true\n)\n```  \nCreate Buffer with request size support ION or dmabufheap which can be used in a device(DSP/CPU/NPU/GPU)  \n**Parameters**:  \n* **out** [OUT] output buffer pointer. User can get va, size, offset through *out\n* **req_size** [IN] request size\n* **is_cached** [IN] flag, the buffer uses cache or not  \n**Return**: EnnReturn",
      "<2-hop>\n\n```\nEnnReturn EnnReleaseBuffer(\nEnnBufferPtr buffer\n)\n```  \nrelease buffer from [EnnCreateBuffer()](Modules/group__api__memory.md#function-enncreatebuffer)  \n**Parameters**:  \n* **buffer** [IN] buffer object from [EnnCreateBuffer()](Modules/group__api__memory.md#function-enncreatebuffer)  \n**Return**: EnnReturn result, 0 is success  \n-------------------------------  \nUpdated on 2023-08-11 at 16:24:05 +0900"
    ],
    "reference": "The process for creating a buffer using EnnBufferPtr involves calling the function `EnnCreateBuffer`, which takes parameters such as an output buffer pointer (`out`), a requested size (`req_size`), and a flag indicating whether the buffer should use cache (`is_cached`). The function returns an `EnnReturn` value. After the buffer is created, it can be released by calling `EnnReleaseBuffer`, which takes the buffer object created by `EnnCreateBuffer` as a parameter. This function also returns an `EnnReturn` result, where a return value of 0 indicates success.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__memory.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:58:44.679786+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain the various functions associated with EnnReturn in the context of memory management?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnGetBuffersInfo](Modules/group__api__model.md#function-enngetbuffersinfo)**([NumberOfBuffersInfo](Classes/struct___number_of_buffers_info.md) * buffers_info, const EnnModelId model_id)<br>Get buffers information from loaded model.  |\n| EnnReturn | **[EnnGetBufferInfoByIndex](Modules/group__api__model.md#function-enngetbufferinfobyindex)**([EnnBufferInfo](Classes/struct__enn_buffer_info.md) * out_buf_info, const EnnModelId model_id, const enn_buf_dir_e direction, const uint32_t index)<br>Get one buffer information from loaded model.  |\n| EnnReturn | **[EnnGetBufferInfoByLabel](Modules/group__api__model.md#function-enngetbufferinfobylabel)**([EnnBufferInfo](Classes/struct__enn_buffer_info.md) * out_buf_info, const EnnModelId model_id, const char * label)<br>Get one buffer information from loaded model.  |\n| EnnReturn | **[EnnSetBufferByIndex](Modules/group__api__model.md#function-ennsetbufferbyindex)**(const EnnModelId model_id, const enn_buf_dir_e direction, const uint32_t index, [EnnBufferPtr](Classes/struct__enn_buffer.md) buf, const int session_id =0)<br>Set memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core.  |\n| EnnReturn | **[EnnSetBufferByLabel](Modules/group__api__model.md#function-ennsetbufferbylabel)**(const EnnModelId model_id, const char * label, [EnnBufferPtr](Classes/struct__enn_buffer.md) buf, const int session_id =0)<br>Set memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core.  |"
    ],
    "reference": "EnnReturn is associated with several functions that retrieve and set buffer information from a loaded model. These include: 1. **EnnGetBuffersInfo**: Retrieves information about all buffers in the model. 2. **EnnGetBufferInfoByIndex**: Gets information for a specific buffer based on its index. 3. **EnnGetBufferInfoByLabel**: Retrieves information for a buffer identified by its label. 4. **EnnSetBufferByIndex**: Allows a user to set a memory object to a specific buffer space, enabling the generation of buffer space for committing. 5. **EnnSetBufferByLabel**: Similar to the previous function, but sets a memory object based on a label. Both setting functions involve committing the memory-buffer set to the service core for execution.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:10.475461+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the function of EnnBufferInfo in the context of model execution?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnGetBuffersInfo](Modules/group__api__model.md#function-enngetbuffersinfo)**([NumberOfBuffersInfo](Classes/struct___number_of_buffers_info.md) * buffers_info, const EnnModelId model_id)<br>Get buffers information from loaded model.  |\n| EnnReturn | **[EnnGetBufferInfoByIndex](Modules/group__api__model.md#function-enngetbufferinfobyindex)**([EnnBufferInfo](Classes/struct__enn_buffer_info.md) * out_buf_info, const EnnModelId model_id, const enn_buf_dir_e direction, const uint32_t index)<br>Get one buffer information from loaded model.  |\n| EnnReturn | **[EnnGetBufferInfoByLabel](Modules/group__api__model.md#function-enngetbufferinfobylabel)**([EnnBufferInfo](Classes/struct__enn_buffer_info.md) * out_buf_info, const EnnModelId model_id, const char * label)<br>Get one buffer information from loaded model.  |\n| EnnReturn | **[EnnSetBufferByIndex](Modules/group__api__model.md#function-ennsetbufferbyindex)**(const EnnModelId model_id, const enn_buf_dir_e direction, const uint32_t index, [EnnBufferPtr](Classes/struct__enn_buffer.md) buf, const int session_id =0)<br>Set memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core.  |\n| EnnReturn | **[EnnSetBufferByLabel](Modules/group__api__model.md#function-ennsetbufferbylabel)**(const EnnModelId model_id, const char * label, [EnnBufferPtr](Classes/struct__enn_buffer.md) buf, const int session_id =0)<br>Set memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core.  |"
    ],
    "reference": "EnnBufferInfo is used in functions like EnnGetBufferInfoByIndex and EnnGetBufferInfoByLabel to retrieve information about a specific buffer from a loaded model.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:10.475461+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the purpose of the OpenModel function in relation to buffer management?",
    "reference_contexts": [
      "```\nEnnReturn EnnGetBuffersInfo(\nNumberOfBuffersInfo * buffers_info,\nconst EnnModelId model_id\n)\n```  \nGet buffers information from loaded model.  \n**Parameters**:  \n* **buffers_info** [OUT] number of in / out buffer which caller should commit.\n* **model_id** [IN] model id from OpenModel()  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "The OpenModel function is used to obtain the model id, which is necessary for retrieving buffers information from the loaded model. This is crucial for managing the number of in/out buffers that the caller should commit.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:10.475461+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What does the buffers_info parameter represent in the EnnGetBuffersInfo function?",
    "reference_contexts": [
      "```\nEnnReturn EnnGetBuffersInfo(\nNumberOfBuffersInfo * buffers_info,\nconst EnnModelId model_id\n)\n```  \nGet buffers information from loaded model.  \n**Parameters**:  \n* **buffers_info** [OUT] number of in / out buffer which caller should commit.\n* **model_id** [IN] model id from OpenModel()  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "The buffers_info parameter represents the number of in/out buffers that the caller should commit, as specified in the EnnGetBuffersInfo function.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:10.475461+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the role of model_id in the EnnGetBufferInfoByIndex function?",
    "reference_contexts": [
      "```\nEnnReturn EnnGetBufferInfoByIndex(\nEnnBufferInfo * out_buf_info,\nconst EnnModelId model_id,\nconst enn_buf_dir_e direction,\nconst uint32_t index\n)\n```  \nGet one buffer information from loaded model.  \n**Parameters**:  \n* **out_buf_info** [OUT] output buffer information\n* **model_id** [IN] model ID from load_model\n* **direction** [IN] direction (IN, OUT)\n* **index** [IN] buffer's index number in model  \n**Return**: EnnReturn result, 0 is success  \n```cpp\ntypedef struct _ennBufferInfo {\nbool     is_able_to_update;  // this is not used\nuint32_t n;\nuint32_t width;\nuint32_t height;\nuint32_t channel;\nuint32_t size;\nconst char *label;\n} EnnBufferInfo;\n```  \na caller can identify a buffer as {DIR, Index} such as {IN, 0}"
    ],
    "reference": "In the EnnGetBufferInfoByIndex function, model_id is an input parameter that represents the model ID from load_model, which is used to retrieve information about a specific buffer from the loaded model.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:10.475461+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Wht is model_id in the context of buffer information?",
    "reference_contexts": [
      "```\nEnnReturn EnnGetBufferInfoByIndex(\nEnnBufferInfo * out_buf_info,\nconst EnnModelId model_id,\nconst enn_buf_dir_e direction,\nconst uint32_t index\n)\n```  \nGet one buffer information from loaded model.  \n**Parameters**:  \n* **out_buf_info** [OUT] output buffer information\n* **model_id** [IN] model ID from load_model\n* **direction** [IN] direction (IN, OUT)\n* **index** [IN] buffer's index number in model  \n**Return**: EnnReturn result, 0 is success  \n```cpp\ntypedef struct _ennBufferInfo {\nbool     is_able_to_update;  // this is not used\nuint32_t n;\nuint32_t width;\nuint32_t height;\nuint32_t channel;\nuint32_t size;\nconst char *label;\n} EnnBufferInfo;\n```  \na caller can identify a buffer as {DIR, Index} such as {IN, 0}"
    ],
    "reference": "The model_id is the model ID from load_model, which is used as an input parameter to get buffer information from the loaded model.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:10.475461+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What information can be obtained using the EnnGetBufferInfoByLabel function?",
    "reference_contexts": [
      "```\nEnnReturn EnnGetBufferInfoByLabel(\nEnnBufferInfo * out_buf_info,\nconst EnnModelId model_id,\nconst char * label\n)\n```  \nGet one buffer information from loaded model.  \n**Parameters**:  \n* **out_buf_info** [OUT] output buffer information\n* **model_id** [IN] model ID from load_model\n* **label** [IN] label. if .nnc includes redundent label, the framework returns information of the first founded tensor. C-style string type.  \n**Return**: EnnReturn result, 0 is success  \n```cpp\ntypedef struct _ennBufferInfo {\nbool     is_able_to_update;  // this is not used\nuint32_t n;\nuint32_t width;\nuint32_t height;\nuint32_t channel;\nuint32_t size;\nconst char *label;\n} EnnBufferInfo;\n```  \na caller can identify a buffer as {label} or {tensor name}"
    ],
    "reference": "The EnnGetBufferInfoByLabel function retrieves information about a specific buffer from a loaded model. It requires the model ID and a label as parameters, and it outputs the buffer information through the out_buf_info parameter. The function returns an EnnReturn result, where 0 indicates success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:10.475461+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the function of EnnGetBufferInfoByLabel in model execution frameworks?",
    "reference_contexts": [
      "```\nEnnReturn EnnGetBufferInfoByLabel(\nEnnBufferInfo * out_buf_info,\nconst EnnModelId model_id,\nconst char * label\n)\n```  \nGet one buffer information from loaded model.  \n**Parameters**:  \n* **out_buf_info** [OUT] output buffer information\n* **model_id** [IN] model ID from load_model\n* **label** [IN] label. if .nnc includes redundent label, the framework returns information of the first founded tensor. C-style string type.  \n**Return**: EnnReturn result, 0 is success  \n```cpp\ntypedef struct _ennBufferInfo {\nbool     is_able_to_update;  // this is not used\nuint32_t n;\nuint32_t width;\nuint32_t height;\nuint32_t channel;\nuint32_t size;\nconst char *label;\n} EnnBufferInfo;\n```  \na caller can identify a buffer as {label} or {tensor name}"
    ],
    "reference": "EnnGetBufferInfoByLabel retrieves information about a specific buffer from a loaded model. It takes parameters such as out_buf_info for outputting the buffer information, model_id to specify the model, and label to identify the buffer. If the model contains redundant labels, the framework returns information for the first tensor found. The function returns an EnnReturn result, where 0 indicates success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:10.475461+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the purpose of the EnnGetBuffersInfo function in relation to buffer information retrieval, and how does it compare to the EnnGetBufferInfoByIndex function?",
    "reference_contexts": [
      "<1-hop>\n\n```\nEnnReturn EnnGetBufferInfoByIndex(\nEnnBufferInfo * out_buf_info,\nconst EnnModelId model_id,\nconst enn_buf_dir_e direction,\nconst uint32_t index\n)\n```  \nGet one buffer information from loaded model.  \n**Parameters**:  \n* **out_buf_info** [OUT] output buffer information\n* **model_id** [IN] model ID from load_model\n* **direction** [IN] direction (IN, OUT)\n* **index** [IN] buffer's index number in model  \n**Return**: EnnReturn result, 0 is success  \n```cpp\ntypedef struct _ennBufferInfo {\nbool     is_able_to_update;  // this is not used\nuint32_t n;\nuint32_t width;\nuint32_t height;\nuint32_t channel;\nuint32_t size;\nconst char *label;\n} EnnBufferInfo;\n```  \na caller can identify a buffer as {DIR, Index} such as {IN, 0}",
      "<2-hop>\n\n|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnGetBuffersInfo](Modules/group__api__model.md#function-enngetbuffersinfo)**([NumberOfBuffersInfo](Classes/struct___number_of_buffers_info.md) * buffers_info, const EnnModelId model_id)<br>Get buffers information from loaded model.  |\n| EnnReturn | **[EnnGetBufferInfoByIndex](Modules/group__api__model.md#function-enngetbufferinfobyindex)**([EnnBufferInfo](Classes/struct__enn_buffer_info.md) * out_buf_info, const EnnModelId model_id, const enn_buf_dir_e direction, const uint32_t index)<br>Get one buffer information from loaded model.  |\n| EnnReturn | **[EnnGetBufferInfoByLabel](Modules/group__api__model.md#function-enngetbufferinfobylabel)**([EnnBufferInfo](Classes/struct__enn_buffer_info.md) * out_buf_info, const EnnModelId model_id, const char * label)<br>Get one buffer information from loaded model.  |\n| EnnReturn | **[EnnSetBufferByIndex](Modules/group__api__model.md#function-ennsetbufferbyindex)**(const EnnModelId model_id, const enn_buf_dir_e direction, const uint32_t index, [EnnBufferPtr](Classes/struct__enn_buffer.md) buf, const int session_id =0)<br>Set memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core.  |\n| EnnReturn | **[EnnSetBufferByLabel](Modules/group__api__model.md#function-ennsetbufferbylabel)**(const EnnModelId model_id, const char * label, [EnnBufferPtr](Classes/struct__enn_buffer.md) buf, const int session_id =0)<br>Set memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core.  |"
    ],
    "reference": "The EnnGetBuffersInfo function is designed to retrieve information about all buffers from a loaded model, providing a comprehensive overview of the buffer resources available. In contrast, the EnnGetBufferInfoByIndex function retrieves information for a specific buffer based on its index, allowing for targeted access to individual buffer details. Both functions are essential for effective buffer information retrieval in the model execution framework.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:10.475461+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the role of the enn_buf_dir_e parameter in the EnnSetBufferByIndex function, and how does it relate to buffer management in the context of memory allocation?",
    "reference_contexts": [
      "<1-hop>\n\n|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnGetBuffersInfo](Modules/group__api__model.md#function-enngetbuffersinfo)**([NumberOfBuffersInfo](Classes/struct___number_of_buffers_info.md) * buffers_info, const EnnModelId model_id)<br>Get buffers information from loaded model.  |\n| EnnReturn | **[EnnGetBufferInfoByIndex](Modules/group__api__model.md#function-enngetbufferinfobyindex)**([EnnBufferInfo](Classes/struct__enn_buffer_info.md) * out_buf_info, const EnnModelId model_id, const enn_buf_dir_e direction, const uint32_t index)<br>Get one buffer information from loaded model.  |\n| EnnReturn | **[EnnGetBufferInfoByLabel](Modules/group__api__model.md#function-enngetbufferinfobylabel)**([EnnBufferInfo](Classes/struct__enn_buffer_info.md) * out_buf_info, const EnnModelId model_id, const char * label)<br>Get one buffer information from loaded model.  |\n| EnnReturn | **[EnnSetBufferByIndex](Modules/group__api__model.md#function-ennsetbufferbyindex)**(const EnnModelId model_id, const enn_buf_dir_e direction, const uint32_t index, [EnnBufferPtr](Classes/struct__enn_buffer.md) buf, const int session_id =0)<br>Set memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core.  |\n| EnnReturn | **[EnnSetBufferByLabel](Modules/group__api__model.md#function-ennsetbufferbylabel)**(const EnnModelId model_id, const char * label, [EnnBufferPtr](Classes/struct__enn_buffer.md) buf, const int session_id =0)<br>Set memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core.  |",
      "<2-hop>\n\n```\nEnnReturn EnnSetBufferByIndex(\nconst EnnModelId model_id,\nconst enn_buf_dir_e direction,\nconst uint32_t index,\nEnnBufferPtr buf,\nconst int session_id =0\n)\n```  \nSet memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core.  \n**Parameters**:  \n* **model_id** [IN] model ID from load_model\n* **direction** [IN] Direction (IN/OUT)\n* **index** [IN] index number of buffer\n* **buf** [IN] memory object from EnnCreateBufferXXX()\n* **session_id** [IN] If a caller generates 2 or more buffer space, session_id can be an identifier  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "The enn_buf_dir_e parameter in the EnnSetBufferByIndex function specifies the direction of the buffer, which can be either IN or OUT. This parameter is crucial for memory management as it determines how the memory object is utilized during the buffer setting process. The function allows a user to generate buffer space to commit, effectively managing memory allocation by enabling the caller to place its memory object into a designated space. This process is essential for ensuring that the memory-buffer set can run the opened model completely within the service core.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:10.475461+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is CloseModel in relation to OpenModel?",
    "reference_contexts": [
      "---\ntitle: OpenModel / CloseModel related  \n---"
    ],
    "reference": "CloseModel is related to OpenModel, but the context does not provide specific details about its function or characteristics.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:25.676949+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is OpenModel?",
    "reference_contexts": [
      "---\ntitle: OpenModel / CloseModel related  \n---"
    ],
    "reference": "OpenModel is referenced in the context of OpenModel / CloseModel related topics.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:25.676949+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What does EnnOpenModelFromMemory do?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnOpenModel](Modules/group__api__openmodel.md#function-ennopenmodel)**(const char * model_file, EnnModelId * model_id)<br>OpenModel with model file.  |\n| EnnReturn | **[EnnOpenModelFromMemory](Modules/group__api__openmodel.md#function-ennopenmodelfrommemory)**(const char * va, const uint32_t size, EnnModelId * model_id)<br>OpenModel from memory buffer.  |"
    ],
    "reference": "EnnOpenModelFromMemory opens a model from a memory buffer.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:25.676949+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What functions are associated with the EnnOpenModel in the context of machine learning applications?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnOpenModel](Modules/group__api__openmodel.md#function-ennopenmodel)**(const char * model_file, EnnModelId * model_id)<br>OpenModel with model file.  |\n| EnnReturn | **[EnnOpenModelFromMemory](Modules/group__api__openmodel.md#function-ennopenmodelfrommemory)**(const char * va, const uint32_t size, EnnModelId * model_id)<br>OpenModel from memory buffer.  |"
    ],
    "reference": "The EnnOpenModel is associated with the function EnnOpenModel, which takes a model file as input and returns an EnnModelId. Additionally, there is the function EnnOpenModelFromMemory, which allows for creating an OpenModel from a memory buffer.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:25.676949+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is EnnModelId in the context of EnnOpenModel?",
    "reference_contexts": [
      "```\nEnnReturn EnnOpenModel(\nconst char * model_file,\nEnnModelId * model_id\n)\n```  \nOpenModel with model file.  \n**Parameters**:  \n* **model_file** [IN] model_file, output from graph-gen. A caller should access the file.\n* **model_id** [OUT] model_id, 64 bit unsigned int  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "EnnModelId is a 64 bit unsigned int that serves as an output parameter in the EnnOpenModel function, which is used to open a model file.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:25.676949+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the parameters required for the EnnModelId in the EnnOpenModel function?",
    "reference_contexts": [
      "```\nEnnReturn EnnOpenModel(\nconst char * model_file,\nEnnModelId * model_id\n)\n```  \nOpenModel with model file.  \n**Parameters**:  \n* **model_file** [IN] model_file, output from graph-gen. A caller should access the file.\n* **model_id** [OUT] model_id, 64 bit unsigned int  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "The parameters required for the EnnModelId in the EnnOpenModel function are model_file, which is an input parameter representing the model file output from graph-gen, and model_id, which is an output parameter that is a 64 bit unsigned int.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:25.676949+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the purpose of model_id in the EnnOpenModelFromMemory function?",
    "reference_contexts": [
      "```\nEnnReturn EnnOpenModelFromMemory(\nconst char * va,\nconst uint32_t size,\nEnnModelId * model_id\n)\n```  \nOpenModel from memory buffer.  \n**Parameters**:  \n* **va** [IN] address which a model loaded from\n* **size** [IN] size of the buffer\n* **model_id** [OUT] model_id, 64 bit unsigned int  \n**Return**: EnnReturn result, 0 is success  \n-------------------------------  \nUpdated on 2023-08-11 at 16:24:05 +0900"
    ],
    "reference": "In the EnnOpenModelFromMemory function, model_id is an output parameter that represents the model identifier, which is a 64-bit unsigned integer.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:25.676949+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What does the term 'va' refer to in the context of the EnnReturn EnnOpenModelFromMemory function?",
    "reference_contexts": [
      "```\nEnnReturn EnnOpenModelFromMemory(\nconst char * va,\nconst uint32_t size,\nEnnModelId * model_id\n)\n```  \nOpenModel from memory buffer.  \n**Parameters**:  \n* **va** [IN] address which a model loaded from\n* **size** [IN] size of the buffer\n* **model_id** [OUT] model_id, 64 bit unsigned int  \n**Return**: EnnReturn result, 0 is success  \n-------------------------------  \nUpdated on 2023-08-11 at 16:24:05 +0900"
    ],
    "reference": "'va' refers to the address from which a model is loaded in the EnnReturn EnnOpenModelFromMemory function.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:25.676949+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the purpose of the EnnOpenModelFromMemory function and how does it relate to the EnnOpenModel function that uses a model file?",
    "reference_contexts": [
      "<1-hop>\n\n```\nEnnReturn EnnOpenModelFromMemory(\nconst char * va,\nconst uint32_t size,\nEnnModelId * model_id\n)\n```  \nOpenModel from memory buffer.  \n**Parameters**:  \n* **va** [IN] address which a model loaded from\n* **size** [IN] size of the buffer\n* **model_id** [OUT] model_id, 64 bit unsigned int  \n**Return**: EnnReturn result, 0 is success  \n-------------------------------  \nUpdated on 2023-08-11 at 16:24:05 +0900",
      "<2-hop>\n\n|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnOpenModel](Modules/group__api__openmodel.md#function-ennopenmodel)**(const char * model_file, EnnModelId * model_id)<br>OpenModel with model file.  |\n| EnnReturn | **[EnnOpenModelFromMemory](Modules/group__api__openmodel.md#function-ennopenmodelfrommemory)**(const char * va, const uint32_t size, EnnModelId * model_id)<br>OpenModel from memory buffer.  |"
    ],
    "reference": "The EnnOpenModelFromMemory function is designed to open a model from a memory buffer, taking parameters such as the address of the model and the size of the buffer, with the model_id being an output parameter. In contrast, the EnnOpenModel function opens a model using a model file. Both functions return an EnnReturn result, where a return value of 0 indicates success.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:25.676949+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the difference between EnnOpenModel and EnnOpenModelFromMemory?",
    "reference_contexts": [
      "<1-hop>\n\n```\nEnnReturn EnnOpenModel(\nconst char * model_file,\nEnnModelId * model_id\n)\n```  \nOpenModel with model file.  \n**Parameters**:  \n* **model_file** [IN] model_file, output from graph-gen. A caller should access the file.\n* **model_id** [OUT] model_id, 64 bit unsigned int  \n**Return**: EnnReturn result, 0 is success",
      "<2-hop>\n\n|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnOpenModel](Modules/group__api__openmodel.md#function-ennopenmodel)**(const char * model_file, EnnModelId * model_id)<br>OpenModel with model file.  |\n| EnnReturn | **[EnnOpenModelFromMemory](Modules/group__api__openmodel.md#function-ennopenmodelfrommemory)**(const char * va, const uint32_t size, EnnModelId * model_id)<br>OpenModel from memory buffer.  |"
    ],
    "reference": "EnnOpenModel is used to open a model with a model file, while EnnOpenModelFromMemory is used to open a model from a memory buffer. Both functions return an EnnReturn result, indicating success or failure.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:25.676949+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How can I use mobile devices in AI Studio Farm for testing applications and what options do I have for connecting to the service?",
    "reference_contexts": [
      "---  \nAI Studio Farm is a service that provides a customized environment for easily and efficiently testing applications on real mobile devices.  \nTo use the AI Studio Farm remote service, you can instantly access it or schedule a connection for a desired date and time."
    ],
    "reference": "AI Studio Farm is a service that provides a customized environment for easily and efficiently testing applications on real mobile devices. To use the AI Studio Farm remote service, you can instantly access it or schedule a connection for a desired date and time.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:59.308402+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How can I check the details of my reservation status after making a reservation using the AI Studio Farm service?",
    "reference_contexts": [
      "If you want to use the AI Studio Farm service on a specific date and time, you can make a reservation using the **Reservation** feature.  \n![](images/Reserve_Here_1.png)  \n1. Log in to use the AI Studio Farm service,\nthen click the Start Now button.  \n2. Set your desired date and time, then click\nthe Reserve button. Check the required tickets\nfor your reservation and your available ticket balance.  \n![](images/Reserve_Here_2.png)  \n3. After completing the reservation,\nyou can check your reservation details in Reservation Status.  \n**Tip\\!**  \n* Click the Cancel button to cancel your reservation.  \n[**Go to Reservation List**](https://prd.ai-studio-farm.com/global/remotelab/reservation-status?deviceTypeId=000d4a92-5b7c-4d17-83e3-b2015630a566)"
    ],
    "reference": "After completing the reservation using the AI Studio Farm service, you can check your reservation details in the Reservation Status section.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:59.308402+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How can I check my credit balance when using the Device Remote service on the AI Studio Farm page?",
    "reference_contexts": [
      "To use **the Device Remote service** and make **reservations** on the **AI Studio Farm** page, you need **Tickets**.  \n**How to Earn Tickets:**  \n* **Log in**: Get **10 Tickets** (Max **10 Tickets per day**)  \n* **Write a forum post**: Get **1 Ticket** per post (Max **5 Tickets per day**)  \n* **Create a project post**: Get **10 Tickets** per post (Max **20 Tickets per day**)  \n* **Convert an NNC file**: Get **2 Tickets** per conversion (Max **10 Tickets per day**)  \n**Reservation Ticket Policies**  \n* **Reservation Credit**: Credits used for reservations will be refunded if you do not connect within 30 minutes. If you cancel the reservation before the scheduled time, all credits will be refunded.  \n* **Service Interruption**: 1 credit will be deducted every 30 minutes, and the remaining credits will be refunded.  \n* **Validity**: Free credits will expire after 90 days and will be forfeited without prior notice.  \n* **Credit Balance Check**: Your credit balance will be updated immediately each time you use a credit.  \n* **Management**: You can check the expiration date, acquisition, and usage history of your credits in the \"My Credits\" menu.  \n![](images/Ticket_Management.png)  \n**Tip\\!**  \n* Click the Ticket icon in the top right corner of the page to check your Ticket usage history."
    ],
    "reference": "You can check your credit balance immediately after each use of a credit, as it will be updated right away. Additionally, you can view the expiration date, acquisition, and usage history of your credits in the 'My Credits' menu.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:59.308402+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the function of the Device in the Remote Streaming Service?",
    "reference_contexts": [
      "The **File Upload** feature is a useful tool in the **Remote Streaming Service**. It allows you to transfer files from your local storage to the **Device** for various purposes. **Supports most file formats**, **APK files** are **automatically installed and executed** on the device  \n( If an **APK file is not installed**, it may not be supported by the device)  \n![](images/Uploading_File.App.png)  \n**Tip\\!**  \n* All files except APK are uploaded to the /sdcard/download/ directory.\nYou can upload files up to 200MB each, with a total upload limit of 2GB."
    ],
    "reference": "The Device in the Remote Streaming Service allows you to transfer files from your local storage for various purposes, supporting most file formats, with APK files being automatically installed and executed on the device.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:59.308402+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Where are screenshots and recorded videos saved in the Remote Streaming Service?",
    "reference_contexts": [
      "**[Screenshot]**  \nWhile using the **Remote Streaming Service**, you can capture the **Device screen** for documentation or review.\nScreenshots are saved in the **/sdcard/Download/** directory.  \n![](images/Screenshot.Recording_1.png)  \n**[Recording]**  \nYou can record the **Device screen** on the **Remote Streaming** page and save it as a video file.The recorded video file is saved in the **/sdcard/Download/** directory.  \n![](images/Screenshot_Recording_2.png)"
    ],
    "reference": "Screenshots and recorded video files are saved in the /sdcard/Download/ directory.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:59.308402+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do I use the folder icon in the File Directory feature of the Remote Streaming Service to manage my files?",
    "reference_contexts": [
      "The **File Directory** feature in the **Remote Streaming Service** allows you to access and manage files on the **Device**.  \n![](images/File_Directory.Management.png)  \nDouble-click the **folder icon** to view the desired files. Double-click the **folder icon** to view the desired files.  \n* After uploading a file, access the **/sdcard/download/** directory to check the uploaded file.  \n* Double-click the file to **download it to your local PC**."
    ],
    "reference": "To manage your files using the folder icon in the File Directory feature of the Remote Streaming Service, you need to double-click the folder icon to view the desired files. After uploading a file, you can access the /sdcard/download/ directory to check the uploaded file, and you can double-click the file to download it to your local PC.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:59.308402+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do you use cp command in Command Shell for copying files?",
    "reference_contexts": [
      "The **Command Shell** feature in the **Remote Streaming Service** allows you to control the device by entering commands.  \n![](images/Command_Shell_Access.png)  \n**Command Shell Rules:**  \n* Enter commands in the **Command Input** field to control the device.  \n* To view in a larger window, click the **Command Shell Window** button to continue entering commands in a modal window.  \nTry This Command to Control the Device.  \n1. View File  \nUse the **cat** command to display the contents of a file:\n```sh\ncat/sdcard/path/to/file.txt\n```  \n2. Move or Rename File  \nUse the **mv** command to move or rename a file.\n```sh\nmv /sdcard/path/to/file.txt /sdcard/path/to/new_name.txt\n```  \n3. Copy File  \nUse the **cp** command to copy a file:\n```sh\ncp /sdcard/path/to/file.txt /sdcard/path/to/backup_file.txt\n```"
    ],
    "reference": "To use the cp command in the Command Shell for copying files, you can enter the command in the Command Input field. For example, to copy a file, you would use the command: `cp /sdcard/path/to/file.txt /sdcard/path/to/backup_file.txt`.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:59.308402+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How can I access and download logs in the Remote Streaming Service while developing an Android application?",
    "reference_contexts": [
      "You can review **INFO logs**, which record usage history in real-time, in the **Remote Streaming Service**.  \nClick the **Log** tab to review **system logs and debugging information**.\nTo download system logs for the current session, click the **Download** button.  \n![](images/Logcat_Window.png)  \n**Tip\\!**  \n* Logs can only be downloaded while the remote service is connected.  \n* To view logs more comfortably, click the expand icon to enlarge the window."
    ],
    "reference": "In the Remote Streaming Service, you can review INFO logs that record usage history in real-time by clicking the Log tab. To download system logs for the current session, you need to click the Download button. However, it's important to note that logs can only be downloaded while the remote service is connected. For a more comfortable viewing experience, you can also click the expand icon to enlarge the log window.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:59.308402+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How can I use the Command Shell feature to manage my reservations in the AI Studio Farm service effectively?",
    "reference_contexts": [
      "<1-hop>\n\n![](images/Start_Now.png)  \n1. Log in to use the AI Studio Farm service,\nthen click the Start Now button.  \n2. Select your desired time in the Duration window,\nthen click the Start button. One ticket is used\nper 30 minutes, and you can check your remaining tickets.  \n**Go to the Ticket Management Guide**  \n**Tip\\!**  \n* You cannot select dates that are already reserved or have no available devices.  \n* You can make up to 5 reservations, with a maximum usage of 8 hours per day.",
      "<2-hop>\n\nThe **Command Shell** feature in the **Remote Streaming Service** allows you to control the device by entering commands.  \n![](images/Command_Shell_Access.png)  \n**Command Shell Rules:**  \n* Enter commands in the **Command Input** field to control the device.  \n* To view in a larger window, click the **Command Shell Window** button to continue entering commands in a modal window.  \nTry This Command to Control the Device.  \n1. View File  \nUse the **cat** command to display the contents of a file:\n```sh\ncat/sdcard/path/to/file.txt\n```  \n2. Move or Rename File  \nUse the **mv** command to move or rename a file.\n```sh\nmv /sdcard/path/to/file.txt /sdcard/path/to/new_name.txt\n```  \n3. Copy File  \nUse the **cp** command to copy a file:\n```sh\ncp /sdcard/path/to/file.txt /sdcard/path/to/backup_file.txt\n```"
    ],
    "reference": "To manage your reservations in the AI Studio Farm service effectively using the Command Shell feature, first log in and click the Start Now button to make a reservation. You can select your desired time in the Duration window and check your remaining tickets, as one ticket is used per 30 minutes. While the Command Shell allows you to control the device by entering commands, it does not directly manage reservations. However, you can use commands to view files related to your reservations or manage any associated data. For example, you can use the 'cat' command to display the contents of a file that may contain your reservation details, or the 'mv' command to rename a file if you need to organize your reservation records.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:59.308402+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do you upload files to the devce using the Remote Streaming Service?",
    "reference_contexts": [
      "<1-hop>\n\nThe **File Directory** feature in the **Remote Streaming Service** allows you to access and manage files on the **Device**.  \n![](images/File_Directory.Management.png)  \nDouble-click the **folder icon** to view the desired files. Double-click the **folder icon** to view the desired files.  \n* After uploading a file, access the **/sdcard/download/** directory to check the uploaded file.  \n* Double-click the file to **download it to your local PC**.",
      "<2-hop>\n\nThe **File Upload** feature is a useful tool in the **Remote Streaming Service**. It allows you to transfer files from your local storage to the **Device** for various purposes. **Supports most file formats**, **APK files** are **automatically installed and executed** on the device  \n( If an **APK file is not installed**, it may not be supported by the device)  \n![](images/Uploading_File.App.png)  \n**Tip\\!**  \n* All files except APK are uploaded to the /sdcard/download/ directory.\nYou can upload files up to 200MB each, with a total upload limit of 2GB."
    ],
    "reference": "To upload files to the device using the Remote Streaming Service, you can use the File Upload feature, which allows you to transfer files from your local storage to the device. You can upload files up to 200MB each, with a total upload limit of 2GB. All files except APK files are uploaded to the /sdcard/download/ directory. If you upload an APK file, it will be automatically installed and executed on the device, provided that the device supports it.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 15:59:59.308402+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What optimization scenarios are currently predetermined for large vision models (LVM)?",
    "reference_contexts": [
      "This section details the optimization flow according to model type. Currently, optimization scenarios are predetermined for specific model types, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). We plan to update the features to allow flexible scheduling of optimization functions."
    ],
    "reference": "Currently, optimization scenarios are predetermined for specific model types, including large vision models (LVM), along with computer vision (CV) models and large language models (LLM).",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:00:23.127745+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What optimization scenarios are predetermined for large language models?",
    "reference_contexts": [
      "This section details the optimization flow according to model type. Currently, optimization scenarios are predetermined for specific model types, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). We plan to update the features to allow flexible scheduling of optimization functions."
    ],
    "reference": "Currently, optimization scenarios are predetermined for specific model types, including large language models (LLM).",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:00:23.127745+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What role does the Simplifier play in the optimization process of a CNNX model?",
    "reference_contexts": [
      "![cv_flow.png](images/cv.png)  \n1. **ONNX-to-CNNX Conversion**  \nConverts an opset16 ONNX model to CNNX format.  \noutput_path : `{result_dir}/cnnx`  \n2. **Optimization**  \nPasses the CNNX model through Simplifier and applies an Optimization template.  \noutput_path : `{result_dir}/optimized`  \n3. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \n4. **Quantization**  \nApplies Fixed Precision Quantization to the optimized CNNX model.  \noutput_path : `{result_dir}/quantized`  \n5. **Performance Evaluation of Quantization**  \nCompares the inference results of the model before and after quantization.  \n6. **CNNX-to-SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`"
    ],
    "reference": "The Simplifier passes the CNNX model through and applies an Optimization template, which is a crucial step in the optimization process.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:00:23.127745+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What ONNX do?",
    "reference_contexts": [
      "![cv_flow.png](images/cv.png)  \n1. **ONNX-to-CNNX Conversion**  \nConverts an opset16 ONNX model to CNNX format.  \noutput_path : `{result_dir}/cnnx`  \n2. **Optimization**  \nPasses the CNNX model through Simplifier and applies an Optimization template.  \noutput_path : `{result_dir}/optimized`  \n3. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \n4. **Quantization**  \nApplies Fixed Precision Quantization to the optimized CNNX model.  \noutput_path : `{result_dir}/quantized`  \n5. **Performance Evaluation of Quantization**  \nCompares the inference results of the model before and after quantization.  \n6. **CNNX-to-SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`"
    ],
    "reference": "ONNX converts an opset16 ONNX model to CNNX format.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:00:23.127745+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain what 4-Dimensional Conversion is and how it fits into the process of optimizing neural network models?",
    "reference_contexts": [
      "![lvm_flow.png](images/lvm.png)  \n1. **ONNX-to-CNNX Conversion**  \nConverts an opset16 ONNX model to CNNX format.  \noutput_path : `{result_dir}/cnnx`  \n2. **Optimization**  \nPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.  \noutput_path : `{result_dir}/optimized`  \n3. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \n4. **Quantization**  \nApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.  \noutput_path :  \n`{result_dir}/smoothquant`  \n`{result_dir}/mixed_precision_quant`  \n`{result_dir}/softmax_bias_correction`  \n5. **Performance Evaluation of Quantization**  \nCompares the inference results of the model before and after quantization.  \n6. **CNNX-to-SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`"
    ],
    "reference": "4-Dimensional Conversion is part of the optimization process where the CNNX model is passed through Simplifier and then undergoes this conversion. It is applied after the ONNX-to-CNNX conversion and before the application of an Optimization template.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:00:23.127745+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What 4-Dimensional Conversion do in optimization process?",
    "reference_contexts": [
      "![lvm_flow.png](images/lvm.png)  \n1. **ONNX-to-CNNX Conversion**  \nConverts an opset16 ONNX model to CNNX format.  \noutput_path : `{result_dir}/cnnx`  \n2. **Optimization**  \nPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.  \noutput_path : `{result_dir}/optimized`  \n3. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \n4. **Quantization**  \nApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.  \noutput_path :  \n`{result_dir}/smoothquant`  \n`{result_dir}/mixed_precision_quant`  \n`{result_dir}/softmax_bias_correction`  \n5. **Performance Evaluation of Quantization**  \nCompares the inference results of the model before and after quantization.  \n6. **CNNX-to-SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`"
    ],
    "reference": "4-Dimensional Conversion is part of the optimization process where the CNNX model is passed through Simplifier and then undergoes 4-Dimensional Conversion before applying an Optimization template.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:00:23.127745+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the steps involved in optimizing a CNNX model and converting it to SNC format?",
    "reference_contexts": [
      "![llm_flow.png](images/llm.png)  \n1. **Optimization**  \nPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.  \noutput_path : `{result_dir}/cnnx`  \n2. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \noutput_path : `{result_dir}/optimized`  \n3. **CNNX-to SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`"
    ],
    "reference": "The steps involved in optimizing a CNNX model include passing the model through Simplifier and 4-Dimensional Conversion, followed by applying an Optimization template. After optimization, the performance evaluation compares the inference results of the model before and after optimization. Finally, the CNNX model is converted to SNC format.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:00:23.127745+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the process of CNNX-to SNC convrsion in machine learning?",
    "reference_contexts": [
      "![llm_flow.png](images/llm.png)  \n1. **Optimization**  \nPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.  \noutput_path : `{result_dir}/cnnx`  \n2. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \noutput_path : `{result_dir}/optimized`  \n3. **CNNX-to SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`"
    ],
    "reference": "The CNNX-to SNC conversion involves converting the CNNX model to SNC format, with the output path specified as `{result_dir}/snc`.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:00:23.127745+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the steps involved in the ONNX-to-CNNX conversion process, and how does optimization play a role in enhancing model performance?",
    "reference_contexts": [
      "<1-hop>\n\n![llm_flow.png](images/llm.png)  \n1. **Optimization**  \nPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.  \noutput_path : `{result_dir}/cnnx`  \n2. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \noutput_path : `{result_dir}/optimized`  \n3. **CNNX-to SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`",
      "<2-hop>\n\n![lvm_flow.png](images/lvm.png)  \n1. **ONNX-to-CNNX Conversion**  \nConverts an opset16 ONNX model to CNNX format.  \noutput_path : `{result_dir}/cnnx`  \n2. **Optimization**  \nPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.  \noutput_path : `{result_dir}/optimized`  \n3. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \n4. **Quantization**  \nApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.  \noutput_path :  \n`{result_dir}/smoothquant`  \n`{result_dir}/mixed_precision_quant`  \n`{result_dir}/softmax_bias_correction`  \n5. **Performance Evaluation of Quantization**  \nCompares the inference results of the model before and after quantization.  \n6. **CNNX-to-SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`"
    ],
    "reference": "The ONNX-to-CNNX conversion process involves converting an opset16 ONNX model to CNNX format, which is the first step. After this conversion, the CNNX model undergoes optimization, where it is passed through Simplifier and 4-Dimensional Conversion, followed by the application of an Optimization template. This optimization is crucial as it enhances the model's performance by comparing the inference results before and after the optimization process.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:00:23.127745+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the steps involved in converting a CNNX model to SNC format, and how do the optimization and quantization processes differ between the two contexts provided?",
    "reference_contexts": [
      "<1-hop>\n\n![lvm_flow.png](images/lvm.png)  \n1. **ONNX-to-CNNX Conversion**  \nConverts an opset16 ONNX model to CNNX format.  \noutput_path : `{result_dir}/cnnx`  \n2. **Optimization**  \nPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.  \noutput_path : `{result_dir}/optimized`  \n3. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \n4. **Quantization**  \nApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.  \noutput_path :  \n`{result_dir}/smoothquant`  \n`{result_dir}/mixed_precision_quant`  \n`{result_dir}/softmax_bias_correction`  \n5. **Performance Evaluation of Quantization**  \nCompares the inference results of the model before and after quantization.  \n6. **CNNX-to-SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`",
      "<2-hop>\n\n![cv_flow.png](images/cv.png)  \n1. **ONNX-to-CNNX Conversion**  \nConverts an opset16 ONNX model to CNNX format.  \noutput_path : `{result_dir}/cnnx`  \n2. **Optimization**  \nPasses the CNNX model through Simplifier and applies an Optimization template.  \noutput_path : `{result_dir}/optimized`  \n3. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \n4. **Quantization**  \nApplies Fixed Precision Quantization to the optimized CNNX model.  \noutput_path : `{result_dir}/quantized`  \n5. **Performance Evaluation of Quantization**  \nCompares the inference results of the model before and after quantization.  \n6. **CNNX-to-SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`"
    ],
    "reference": "The steps involved in converting a CNNX model to SNC format include the final conversion process where the CNNX model is transformed into SNC format, with the output path specified as `{result_dir}/snc}",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:00:23.127745+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is Exynos used for in model optimization?",
    "reference_contexts": [
      "Models are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.  \n- shape_inference\nIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\n- 4dim conversion\nTo deploy a model with a .nnc extension on a device, all operators must have input and output shapes with four dimensions. To facilitate this, the 4D conversion process identifies and converts all channel axes of the operators to 4D while maintaining their original structure and channel axes. The channel axes can be determined by certain layers, such as Convolutional and Linear layers, which have fixed channel axes at positions 1 and -1, respectively. However, they can be modified by operators like MatMul and Reshape, which can expand or reduce them. To account for these changes, the conversion process tracks channel axes in both the forward and backward pass. As a result, the 4D conversion process may require additional time to complete.\n- support optimization method.\nOptimizer supports many optimization features so that the model works efficiently on the device.\n- Fold\n- GeGLU\n- GeLU\n- GroupNorm\n- LayerNorm\n- PReLU\n- RMSNorm\n- SiLU\n- Fuse\n- SiLU (to GroupNorm)\n- BatchNorm into Convolution\n- Cast\n- Deconvolution bias\n- Math\n- multiple reshape and transpose in a row (when possible)\n- multiple concat in a row\n- Insert\n- Depthwise Convolution for activation\n- Remove\n- unecesary slices\n- Replace\n- Average Pooling to Depthwise convolution\n- Eltwise concat convolution\n- expand with concat (by concatenating the same input multiple times)\n- Convolution kernel 1 to 3\n- Matrix multiplication to dynamic convolution\n- ReduceMean to Global Average Pool\n- ReduceSum to Convolution\n- Slice to Split\n- Global Average Pool to 2 Average Pool\n- Change attribute\n- axis of softmax  \nIn addition, Optimizer Template, one of the Optimizer's powerful features, supports custom optimization methods made by users."
    ],
    "reference": "Optimizer provides optimization methods for these models to perform best on Exynos chips.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\optimizer\\optimizer.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:00:51.600674+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does the MatMul operator affect the 4D conversion process in model optimization?",
    "reference_contexts": [
      "Models are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.  \n- shape_inference\nIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\n- 4dim conversion\nTo deploy a model with a .nnc extension on a device, all operators must have input and output shapes with four dimensions. To facilitate this, the 4D conversion process identifies and converts all channel axes of the operators to 4D while maintaining their original structure and channel axes. The channel axes can be determined by certain layers, such as Convolutional and Linear layers, which have fixed channel axes at positions 1 and -1, respectively. However, they can be modified by operators like MatMul and Reshape, which can expand or reduce them. To account for these changes, the conversion process tracks channel axes in both the forward and backward pass. As a result, the 4D conversion process may require additional time to complete.\n- support optimization method.\nOptimizer supports many optimization features so that the model works efficiently on the device.\n- Fold\n- GeGLU\n- GeLU\n- GroupNorm\n- LayerNorm\n- PReLU\n- RMSNorm\n- SiLU\n- Fuse\n- SiLU (to GroupNorm)\n- BatchNorm into Convolution\n- Cast\n- Deconvolution bias\n- Math\n- multiple reshape and transpose in a row (when possible)\n- multiple concat in a row\n- Insert\n- Depthwise Convolution for activation\n- Remove\n- unecesary slices\n- Replace\n- Average Pooling to Depthwise convolution\n- Eltwise concat convolution\n- expand with concat (by concatenating the same input multiple times)\n- Convolution kernel 1 to 3\n- Matrix multiplication to dynamic convolution\n- ReduceMean to Global Average Pool\n- ReduceSum to Convolution\n- Slice to Split\n- Global Average Pool to 2 Average Pool\n- Change attribute\n- axis of softmax  \nIn addition, Optimizer Template, one of the Optimizer's powerful features, supports custom optimization methods made by users."
    ],
    "reference": "The MatMul operator can modify channel axes, which are crucial for the 4D conversion process. This process identifies and converts all channel axes of the operators to 4D while maintaining their original structure. Since MatMul can expand or reduce channel axes, the conversion process must track these changes during both the forward and backward pass, potentially requiring additional time to complete.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\optimizer\\optimizer.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:00:51.600674+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does the trigger_op method function within the Optimizer Templates?",
    "reference_contexts": [
      "The Optimizer Templates are executed as shown in the flow chart below. In searching for the model, the node meets the trigger_op method of the template for the first time. Then it loops through the origin_condition of the case. If the condition is correct, it optimizes the module through steps.  \n![flow](images/flow.png)"
    ],
    "reference": "The trigger_op method is met for the first time when searching for the model, after which the node loops through the origin_condition of the case. If the condition is correct, it optimizes the module through subsequent steps.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\optimizer\\optimizer.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:00:51.600674+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the role of the moduel in the optimization process?",
    "reference_contexts": [
      "The Optimizer Templates are executed as shown in the flow chart below. In searching for the model, the node meets the trigger_op method of the template for the first time. Then it loops through the origin_condition of the case. If the condition is correct, it optimizes the module through steps.  \n![flow](images/flow.png)"
    ],
    "reference": "The module is optimized through steps if the origin_condition of the case is correct, as indicated when the node meets the trigger_op method of the template for the first time.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\optimizer\\optimizer.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:00:51.600674+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain what a CnnxNode is and how it is used in the context of optimization in machine learning?",
    "reference_contexts": [
      "In this part, we introduce how to write custom templates step by step. As an example, we use a template that fuses basic arithmetic operations layers into a convolution layer, when possible. We provide `TemplateStepInternal`, `TemplateCaseInternal`, `OptimizerTemplateInternal` to prevent abnormal behavior of the code.  \n```python\nclass TemplateStepInternal(TemplateStep):\n@property\ndef name(self) -> AnyStr:\nreturn self._name\n\n@name.setter\ndef name(self, value: AnyStr):\nself._name = value\n\n@property\ndef origin(self) -> Dict:\nreturn self._origin\n\n@origin.setter\ndef origin(self, value: Dict):\nself._origin = value\n\ndef optimization(self, model: CnnxModel, node: CnnxNode) -> bool:\nreturn NotImplementedError\n\n\nclass TemplateCaseInternal(TemplateCase):\n@property\ndef name(self) -> AnyStr:\nreturn self._name\n\n@name.setter\ndef name(self, value: AnyStr):\nself._name = value\n\n@property\ndef origin(self) -> Dict:\nreturn self._origin\n\n@origin.setter\ndef origin(self, value: Dict):\nself._originn = value\n\n@property\ndef step_list(self) -> Dict:\nreturn self._step_list\n\n@step_list.setter\ndef step_list(self, value: Dict):\nself._step_list = value\n\ndef origin_condition(self, model: CnnxModel, node: CnnxNode) -> bool:\nraise NotImplementedError\n\ndef launch(self, model: CnnxModel, node: CnnxNode) -> bool:\nif self.origin_condition(model, node):\nfor st in self.step_list:\nif not st.optimization(model, node):\nreturn False\nreturn True\n\n\n\nclass OptimizerTemplateInternal(OptimizerTemplate):\ndef __repr__(self):\nreturn '{}(name={})'.format(\nself.__class__.__name__, self.name)\n\n@property\ndef name(self) -> AnyStr:\nreturn self._name\n\n@name.setter\ndef name(self, value: AnyStr):\nself._name = value\n\n@property\ndef case_list(self) -> List[TemplateCaseInternal]:\nreturn self._case_list\n\n@case_list.setter\ndef case_list(self, value: List[TemplateCaseInternal]):\nself._case_list = value\n\ndef trigger_op(self, node: CnnxNode) -> bool:\nraise NotImplementedError\n\n\ndef launch(self, model: CnnxModel, node: CnnxNode) -> bool:\nif self.trigger_op(node):\nfor case in self.case_list:\nif case.launch(model, node):\nreturn True\nreturn False\n```  \nInherit the above classes and declare each class corresponding to Step, Case, and Template. Import package classes of model_editor for updating the model. If you need more information about model_editor, please see the guide about model_editor.  \nWe **activate the trigger_op method** when the node is one of the four basic arithmetic operators and the input feature map has 4 dimension. In the **origin_condition method of the case class**, more detailled conditions are checked: the previous node of the arithmetic operator must be a convolution layer, the arithmetic operator must be the only node following the convolution node, and it must not be folded yet.\nIn the **optimization method of Step class**, update the value of the convolution node using the value of the arithmetic node. Once the value has been updated, you no longer need the original node. Create a port to clear the node from the graph. update the model using the port you created. First, remove the node from the model. Then, connect the next node of the removed node to the convolution node.  \n```python\n# That classes prevent the abnormal behavior of the code\nfrom optimizer.core.templates.base.template_internal import (\nTemplateStepInternal,\nTemplateCaseInternal,\nOptimizerTemplateInternal\n)\n# These packages have the ability to add, delete, and update nodes of the model.\n# Please see the guide about model_editor for more information.\nfrom model_editor import (\nCnnxNode,\nCnnxModel,\nCnnxInOut,\nCnnxAttribute,\n)\n\n# declare Step class\nclass StepFuseMath(TemplateStepInternal):\ndef __init__(self, device_info):\nself.name = 'Step Fusing Math'\nself.origin = 'Origin Operator, and Parameters'\nself.device_info = device_info\n\n# create bias\ndef make_bias_input(self, node: CnnxNode):\nbias_data = np.zeros(shape=node.outputs[0].inout.shape[1]).astype(np.float32)\nreturn CnnxInOut(\nname=f\"{node.name}_bias\",\ndtype=\"float\",\nshape=[node.outputs[0].inout.shape[1]],\ndata=bias_data\n)\n\ndef optimization(self, model: CnnxModel, node: CnnxNode) -> bool:\n# Set constant Value\n# Apply constant Value to weight, and bias\nparent_conv = node.prev_nodes[0]\nif not node.inputs[1].inout.shape or len(node.inputs[0].inout.shape) > 2 and len(node.inputs[1].inout.shape) > 2:\nshape_0 = node.inputs[0].inout.shape if node.inputs[0].inout.shape is not None else []\nshape_1 = node.inputs[1].inout.shape if node.inputs[1].inout.shape is not None else []\nif shape_0[-2:] == shape_1[-2:]:\nkernel_data = node.inputs[1].inout.data if node.inputs[1].inout.data is not None else node.inputs[0].inout.data\nbias_data = node.inputs[1].inout.data if node.inputs[1].inout.data is not None else node.inputs[0].inout.data\nelse:\nkernel_data = node.inputs[1].inout.data.reshape((-1, 1, 1, 1)) if node.inputs[1].inout.data is not None else node.inputs[0].inout.data.reshape((-1, 1, 1, 1))\nbias_data = node.inputs[1].inout.data.reshape((-1)) if node.inputs[1].inout.data is not None else node.inputs[0].inout.data.reshape((-1))\nif node.op_type == 'Add':\nif len(parent_conv.inputs) != 3:\nparent_conv.inputs.append(self.make_bias_input(parent_conv))\nnew_data = parent_conv.inputs.data[2].inout.data + bias_data\nparent_conv.inputs.data[2].inout.data = new_data\nparent_conv.inputs.data[2].inout.shape = new_data.shape\n\nelif node.op_type == 'Sub':\nif len(parent_conv.inputs) != 3:\nparent_conv.inputs.append(self.make_bias_input(parent_conv))\nnew_data = parent_conv.inputs.data[2].inout.data - bias_data\nparent_conv.inputs.data[2].inout.data = new_data\nparent_conv.inputs.data[2].inout.shape = new_data.shape\nelif node.op_type == 'Mul':\nparent_conv.inputs.data[1].inout.data = parent_conv.inputs.data[1].inout.data * kernel_data\nif len(parent_conv.inputs) == 3:\nparent_conv.inputs.data[2].inout.data = parent_conv.inputs.data[2].inout.data * bias_data\n\nelif node.op_type == 'Div':\nparent_conv.inputs.data[1].inout.data = parent_conv.inputs.data[1].inout.data / kernel_data\nif len(parent_conv.inputs) == 3:\nparent_conv.inputs.data[2].inout.data = parent_conv.inputs.data[2].inout.data / bias_data\n\nconnections = []\nfor next_node in node.next_nodes:\nfor output_tensor in node.outputs:\nport_from = node.prev_nodes[0].outputs[0]\nfor i in next_node.inputs:\nif i.name == output_tensor.name:\nconnections.append((port_from, i))\n\noptimized_nodes = [parent_conv]\nprevious_nodes = [node]\n\ntry:\nmodel.graph.nodes.remove(node)\nfor connection in connections:\nmodel.graph.nodes.connect(connection[0], connection[1])\nlogger.info(f'Fuse {node.op_type}({node.name}) into Deconv({parent_conv.name})')\nreturn True\nexcept Exception as e:\nlogger.error(e)\nreturn False\n\n\nclass CaseFuseMath(TemplateCaseInternal):\ndef __init__(self, device_info):\nself.name = 'Fuse Math to DWCONV'\nself.step_list = [StepFuseMath(device_info)]\n\n# Check the number of Child of [CONV, DWCONV]\ndef origin_condition(self, model: CnnxModel, node: CnnxNode) -> bool:\nif node.prev_nodes == []:\nreturn False\nif node.inputs[0].inout.data is None and node.inputs[1].inout.data is None:\nreturn False\nfm_shape = node.inputs[0].inout.shape if node.inputs[0].inout.data is None else node.inputs[1].inout.shape\nbias_data = node.inputs[0].inout.data if node.inputs[0].inout.data is not None else node.inputs[1].inout.data\n\n# if bias data just have one value\n# if length of bias data same with output_channel of prev conv node\nif isinstance(bias_data.tolist(), list) and fm_shape[1] != bias_data.reshape((-1)).shape[0]:\nreturn False\nif len(node.prev_nodes[0].next_nodes) == 1 and \\\nlen(node.prev_nodes) == 1 and \\\nnode.inputs[0].inout.shape != node.inputs[1].inout.shape:\nif node.prev_nodes[0].op_type in ['Conv', 'ConvTranspose'] and \\\nnode.module is None:\nreturn True\nreturn False\n\n\nclass TemplateFuseMath(OptimizerTemplateInternal):\ndef __init__(self):\nself.name = 'FuseMath'\n# declare Case\nself.case_list = [\nCaseFuseMath()\n]\n'''\naccording to Flow, The first thing you encounter when navigating the nodes in the graph is trigger_op.\nIn this case, it was written to operate only when the input shape of the four arithmetic operations operator\nand a input feature map of node has 4 dimension.\n'''\ndef trigger_op(self, node: CnnxNode):\noptypes = ['Add', 'Sub', 'Mul', 'Div']\nif node.op_type in optypes:\nif len(node.inputs[0].shape) == 4 or len(node.inputs[1].shape) == 4:\nreturn True\nreturn False\n```"
    ],
    "reference": "A CnnxNode is a component used in the context of machine learning models, specifically within the optimization process. It represents a node in the computational graph that can perform operations such as addition, subtraction, multiplication, or division. In the optimization method of the Step class, the CnnxNode is utilized to update the value of a convolution node based on the values of arithmetic nodes. The optimization process involves checking conditions related to the node's inputs and outputs, and if certain criteria are met, the node can be modified or removed from the graph to enhance the model's performance.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\optimizer\\optimizer.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:00:51.600674+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the role of TemplateCaseInternal in the context of custom templates?",
    "reference_contexts": [
      "In this part, we introduce how to write custom templates step by step. As an example, we use a template that fuses basic arithmetic operations layers into a convolution layer, when possible. We provide `TemplateStepInternal`, `TemplateCaseInternal`, `OptimizerTemplateInternal` to prevent abnormal behavior of the code.  \n```python\nclass TemplateStepInternal(TemplateStep):\n@property\ndef name(self) -> AnyStr:\nreturn self._name\n\n@name.setter\ndef name(self, value: AnyStr):\nself._name = value\n\n@property\ndef origin(self) -> Dict:\nreturn self._origin\n\n@origin.setter\ndef origin(self, value: Dict):\nself._origin = value\n\ndef optimization(self, model: CnnxModel, node: CnnxNode) -> bool:\nreturn NotImplementedError\n\n\nclass TemplateCaseInternal(TemplateCase):\n@property\ndef name(self) -> AnyStr:\nreturn self._name\n\n@name.setter\ndef name(self, value: AnyStr):\nself._name = value\n\n@property\ndef origin(self) -> Dict:\nreturn self._origin\n\n@origin.setter\ndef origin(self, value: Dict):\nself._originn = value\n\n@property\ndef step_list(self) -> Dict:\nreturn self._step_list\n\n@step_list.setter\ndef step_list(self, value: Dict):\nself._step_list = value\n\ndef origin_condition(self, model: CnnxModel, node: CnnxNode) -> bool:\nraise NotImplementedError\n\ndef launch(self, model: CnnxModel, node: CnnxNode) -> bool:\nif self.origin_condition(model, node):\nfor st in self.step_list:\nif not st.optimization(model, node):\nreturn False\nreturn True\n\n\n\nclass OptimizerTemplateInternal(OptimizerTemplate):\ndef __repr__(self):\nreturn '{}(name={})'.format(\nself.__class__.__name__, self.name)\n\n@property\ndef name(self) -> AnyStr:\nreturn self._name\n\n@name.setter\ndef name(self, value: AnyStr):\nself._name = value\n\n@property\ndef case_list(self) -> List[TemplateCaseInternal]:\nreturn self._case_list\n\n@case_list.setter\ndef case_list(self, value: List[TemplateCaseInternal]):\nself._case_list = value\n\ndef trigger_op(self, node: CnnxNode) -> bool:\nraise NotImplementedError\n\n\ndef launch(self, model: CnnxModel, node: CnnxNode) -> bool:\nif self.trigger_op(node):\nfor case in self.case_list:\nif case.launch(model, node):\nreturn True\nreturn False\n```  \nInherit the above classes and declare each class corresponding to Step, Case, and Template. Import package classes of model_editor for updating the model. If you need more information about model_editor, please see the guide about model_editor.  \nWe **activate the trigger_op method** when the node is one of the four basic arithmetic operators and the input feature map has 4 dimension. In the **origin_condition method of the case class**, more detailled conditions are checked: the previous node of the arithmetic operator must be a convolution layer, the arithmetic operator must be the only node following the convolution node, and it must not be folded yet.\nIn the **optimization method of Step class**, update the value of the convolution node using the value of the arithmetic node. Once the value has been updated, you no longer need the original node. Create a port to clear the node from the graph. update the model using the port you created. First, remove the node from the model. Then, connect the next node of the removed node to the convolution node.  \n```python\n# That classes prevent the abnormal behavior of the code\nfrom optimizer.core.templates.base.template_internal import (\nTemplateStepInternal,\nTemplateCaseInternal,\nOptimizerTemplateInternal\n)\n# These packages have the ability to add, delete, and update nodes of the model.\n# Please see the guide about model_editor for more information.\nfrom model_editor import (\nCnnxNode,\nCnnxModel,\nCnnxInOut,\nCnnxAttribute,\n)\n\n# declare Step class\nclass StepFuseMath(TemplateStepInternal):\ndef __init__(self, device_info):\nself.name = 'Step Fusing Math'\nself.origin = 'Origin Operator, and Parameters'\nself.device_info = device_info\n\n# create bias\ndef make_bias_input(self, node: CnnxNode):\nbias_data = np.zeros(shape=node.outputs[0].inout.shape[1]).astype(np.float32)\nreturn CnnxInOut(\nname=f\"{node.name}_bias\",\ndtype=\"float\",\nshape=[node.outputs[0].inout.shape[1]],\ndata=bias_data\n)\n\ndef optimization(self, model: CnnxModel, node: CnnxNode) -> bool:\n# Set constant Value\n# Apply constant Value to weight, and bias\nparent_conv = node.prev_nodes[0]\nif not node.inputs[1].inout.shape or len(node.inputs[0].inout.shape) > 2 and len(node.inputs[1].inout.shape) > 2:\nshape_0 = node.inputs[0].inout.shape if node.inputs[0].inout.shape is not None else []\nshape_1 = node.inputs[1].inout.shape if node.inputs[1].inout.shape is not None else []\nif shape_0[-2:] == shape_1[-2:]:\nkernel_data = node.inputs[1].inout.data if node.inputs[1].inout.data is not None else node.inputs[0].inout.data\nbias_data = node.inputs[1].inout.data if node.inputs[1].inout.data is not None else node.inputs[0].inout.data\nelse:\nkernel_data = node.inputs[1].inout.data.reshape((-1, 1, 1, 1)) if node.inputs[1].inout.data is not None else node.inputs[0].inout.data.reshape((-1, 1, 1, 1))\nbias_data = node.inputs[1].inout.data.reshape((-1)) if node.inputs[1].inout.data is not None else node.inputs[0].inout.data.reshape((-1))\nif node.op_type == 'Add':\nif len(parent_conv.inputs) != 3:\nparent_conv.inputs.append(self.make_bias_input(parent_conv))\nnew_data = parent_conv.inputs.data[2].inout.data + bias_data\nparent_conv.inputs.data[2].inout.data = new_data\nparent_conv.inputs.data[2].inout.shape = new_data.shape\n\nelif node.op_type == 'Sub':\nif len(parent_conv.inputs) != 3:\nparent_conv.inputs.append(self.make_bias_input(parent_conv))\nnew_data = parent_conv.inputs.data[2].inout.data - bias_data\nparent_conv.inputs.data[2].inout.data = new_data\nparent_conv.inputs.data[2].inout.shape = new_data.shape\nelif node.op_type == 'Mul':\nparent_conv.inputs.data[1].inout.data = parent_conv.inputs.data[1].inout.data * kernel_data\nif len(parent_conv.inputs) == 3:\nparent_conv.inputs.data[2].inout.data = parent_conv.inputs.data[2].inout.data * bias_data\n\nelif node.op_type == 'Div':\nparent_conv.inputs.data[1].inout.data = parent_conv.inputs.data[1].inout.data / kernel_data\nif len(parent_conv.inputs) == 3:\nparent_conv.inputs.data[2].inout.data = parent_conv.inputs.data[2].inout.data / bias_data\n\nconnections = []\nfor next_node in node.next_nodes:\nfor output_tensor in node.outputs:\nport_from = node.prev_nodes[0].outputs[0]\nfor i in next_node.inputs:\nif i.name == output_tensor.name:\nconnections.append((port_from, i))\n\noptimized_nodes = [parent_conv]\nprevious_nodes = [node]\n\ntry:\nmodel.graph.nodes.remove(node)\nfor connection in connections:\nmodel.graph.nodes.connect(connection[0], connection[1])\nlogger.info(f'Fuse {node.op_type}({node.name}) into Deconv({parent_conv.name})')\nreturn True\nexcept Exception as e:\nlogger.error(e)\nreturn False\n\n\nclass CaseFuseMath(TemplateCaseInternal):\ndef __init__(self, device_info):\nself.name = 'Fuse Math to DWCONV'\nself.step_list = [StepFuseMath(device_info)]\n\n# Check the number of Child of [CONV, DWCONV]\ndef origin_condition(self, model: CnnxModel, node: CnnxNode) -> bool:\nif node.prev_nodes == []:\nreturn False\nif node.inputs[0].inout.data is None and node.inputs[1].inout.data is None:\nreturn False\nfm_shape = node.inputs[0].inout.shape if node.inputs[0].inout.data is None else node.inputs[1].inout.shape\nbias_data = node.inputs[0].inout.data if node.inputs[0].inout.data is not None else node.inputs[1].inout.data\n\n# if bias data just have one value\n# if length of bias data same with output_channel of prev conv node\nif isinstance(bias_data.tolist(), list) and fm_shape[1] != bias_data.reshape((-1)).shape[0]:\nreturn False\nif len(node.prev_nodes[0].next_nodes) == 1 and \\\nlen(node.prev_nodes) == 1 and \\\nnode.inputs[0].inout.shape != node.inputs[1].inout.shape:\nif node.prev_nodes[0].op_type in ['Conv', 'ConvTranspose'] and \\\nnode.module is None:\nreturn True\nreturn False\n\n\nclass TemplateFuseMath(OptimizerTemplateInternal):\ndef __init__(self):\nself.name = 'FuseMath'\n# declare Case\nself.case_list = [\nCaseFuseMath()\n]\n'''\naccording to Flow, The first thing you encounter when navigating the nodes in the graph is trigger_op.\nIn this case, it was written to operate only when the input shape of the four arithmetic operations operator\nand a input feature map of node has 4 dimension.\n'''\ndef trigger_op(self, node: CnnxNode):\noptypes = ['Add', 'Sub', 'Mul', 'Div']\nif node.op_type in optypes:\nif len(node.inputs[0].shape) == 4 or len(node.inputs[1].shape) == 4:\nreturn True\nreturn False\n```"
    ],
    "reference": "TemplateCaseInternal is a class that defines a case in the optimization process. It includes properties such as name, origin, and step_list, and methods like origin_condition and launch, which are used to determine if certain conditions are met for optimization and to execute the optimization steps.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\optimizer\\optimizer.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:00:51.600674+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the role of the optimizer in enhancing model performance on Exynos chips, and how can a custom quantizer template be integrated into this process?",
    "reference_contexts": [
      "<1-hop>\n\nModels are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.  \n- shape_inference\nIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\n- 4dim conversion\nTo deploy a model with a .nnc extension on a device, all operators must have input and output shapes with four dimensions. To facilitate this, the 4D conversion process identifies and converts all channel axes of the operators to 4D while maintaining their original structure and channel axes. The channel axes can be determined by certain layers, such as Convolutional and Linear layers, which have fixed channel axes at positions 1 and -1, respectively. However, they can be modified by operators like MatMul and Reshape, which can expand or reduce them. To account for these changes, the conversion process tracks channel axes in both the forward and backward pass. As a result, the 4D conversion process may require additional time to complete.\n- support optimization method.\nOptimizer supports many optimization features so that the model works efficiently on the device.\n- Fold\n- GeGLU\n- GeLU\n- GroupNorm\n- LayerNorm\n- PReLU\n- RMSNorm\n- SiLU\n- Fuse\n- SiLU (to GroupNorm)\n- BatchNorm into Convolution\n- Cast\n- Deconvolution bias\n- Math\n- multiple reshape and transpose in a row (when possible)\n- multiple concat in a row\n- Insert\n- Depthwise Convolution for activation\n- Remove\n- unecesary slices\n- Replace\n- Average Pooling to Depthwise convolution\n- Eltwise concat convolution\n- expand with concat (by concatenating the same input multiple times)\n- Convolution kernel 1 to 3\n- Matrix multiplication to dynamic convolution\n- ReduceMean to Global Average Pool\n- ReduceSum to Convolution\n- Slice to Split\n- Global Average Pool to 2 Average Pool\n- Change attribute\n- axis of softmax  \nIn addition, Optimizer Template, one of the Optimizer's powerful features, supports custom optimization methods made by users.",
      "<2-hop>\n\nWrite the path of the custom template and the class name of the template. Following the above example, it can be written as follows:  \n```yaml\ninput_model_path: {INPUT_MODEL_PATH}\noutput_folder_path: {OUTPUT_MODEL_PATH}\ninput_model_format: onnx\nmodel_type: CV\n\nquantizer:\n~~~~~\n\nsimulator:\n~~~~~~\n\noptimizer:\ncustom_template_path: {}\noverwrite_input_shapes:\ndata: [1,4,384,384]\nskip_4_dim_conversion: false\ndevice_name: default\ncustom_template_path:\n- TemplateFuseMath: /path/to/your/custome/template.py:\n```"
    ],
    "reference": "The optimizer plays a crucial role in enhancing model performance on Exynos chips by providing various optimization methods that ensure the models operate efficiently despite hardware constraints. It supports features like shape inference, 4D conversion, and a range of optimization techniques such as Fold, GeGLU, and Depthwise Convolution. To integrate a custom quantizer template into this process, one would specify the path of the custom template and the class name in a configuration file. For example, the custom template path can be included under the optimizer section in the YAML configuration, allowing for tailored optimization methods to be applied during model training and deployment.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\optimizer\\optimizer.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:00:51.600674+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain how the OptimizerTemplateInternal is utilized in the optimization process, particularly in relation to the trigger_op method and the origin_condition method as described in the context?",
    "reference_contexts": [
      "<1-hop>\n\nIn this part, we introduce how to write custom templates step by step. As an example, we use a template that fuses basic arithmetic operations layers into a convolution layer, when possible. We provide `TemplateStepInternal`, `TemplateCaseInternal`, `OptimizerTemplateInternal` to prevent abnormal behavior of the code.  \n```python\nclass TemplateStepInternal(TemplateStep):\n@property\ndef name(self) -> AnyStr:\nreturn self._name\n\n@name.setter\ndef name(self, value: AnyStr):\nself._name = value\n\n@property\ndef origin(self) -> Dict:\nreturn self._origin\n\n@origin.setter\ndef origin(self, value: Dict):\nself._origin = value\n\ndef optimization(self, model: CnnxModel, node: CnnxNode) -> bool:\nreturn NotImplementedError\n\n\nclass TemplateCaseInternal(TemplateCase):\n@property\ndef name(self) -> AnyStr:\nreturn self._name\n\n@name.setter\ndef name(self, value: AnyStr):\nself._name = value\n\n@property\ndef origin(self) -> Dict:\nreturn self._origin\n\n@origin.setter\ndef origin(self, value: Dict):\nself._originn = value\n\n@property\ndef step_list(self) -> Dict:\nreturn self._step_list\n\n@step_list.setter\ndef step_list(self, value: Dict):\nself._step_list = value\n\ndef origin_condition(self, model: CnnxModel, node: CnnxNode) -> bool:\nraise NotImplementedError\n\ndef launch(self, model: CnnxModel, node: CnnxNode) -> bool:\nif self.origin_condition(model, node):\nfor st in self.step_list:\nif not st.optimization(model, node):\nreturn False\nreturn True\n\n\n\nclass OptimizerTemplateInternal(OptimizerTemplate):\ndef __repr__(self):\nreturn '{}(name={})'.format(\nself.__class__.__name__, self.name)\n\n@property\ndef name(self) -> AnyStr:\nreturn self._name\n\n@name.setter\ndef name(self, value: AnyStr):\nself._name = value\n\n@property\ndef case_list(self) -> List[TemplateCaseInternal]:\nreturn self._case_list\n\n@case_list.setter\ndef case_list(self, value: List[TemplateCaseInternal]):\nself._case_list = value\n\ndef trigger_op(self, node: CnnxNode) -> bool:\nraise NotImplementedError\n\n\ndef launch(self, model: CnnxModel, node: CnnxNode) -> bool:\nif self.trigger_op(node):\nfor case in self.case_list:\nif case.launch(model, node):\nreturn True\nreturn False\n```  \nInherit the above classes and declare each class corresponding to Step, Case, and Template. Import package classes of model_editor for updating the model. If you need more information about model_editor, please see the guide about model_editor.  \nWe **activate the trigger_op method** when the node is one of the four basic arithmetic operators and the input feature map has 4 dimension. In the **origin_condition method of the case class**, more detailled conditions are checked: the previous node of the arithmetic operator must be a convolution layer, the arithmetic operator must be the only node following the convolution node, and it must not be folded yet.\nIn the **optimization method of Step class**, update the value of the convolution node using the value of the arithmetic node. Once the value has been updated, you no longer need the original node. Create a port to clear the node from the graph. update the model using the port you created. First, remove the node from the model. Then, connect the next node of the removed node to the convolution node.  \n```python\n# That classes prevent the abnormal behavior of the code\nfrom optimizer.core.templates.base.template_internal import (\nTemplateStepInternal,\nTemplateCaseInternal,\nOptimizerTemplateInternal\n)\n# These packages have the ability to add, delete, and update nodes of the model.\n# Please see the guide about model_editor for more information.\nfrom model_editor import (\nCnnxNode,\nCnnxModel,\nCnnxInOut,\nCnnxAttribute,\n)\n\n# declare Step class\nclass StepFuseMath(TemplateStepInternal):\ndef __init__(self, device_info):\nself.name = 'Step Fusing Math'\nself.origin = 'Origin Operator, and Parameters'\nself.device_info = device_info\n\n# create bias\ndef make_bias_input(self, node: CnnxNode):\nbias_data = np.zeros(shape=node.outputs[0].inout.shape[1]).astype(np.float32)\nreturn CnnxInOut(\nname=f\"{node.name}_bias\",\ndtype=\"float\",\nshape=[node.outputs[0].inout.shape[1]],\ndata=bias_data\n)\n\ndef optimization(self, model: CnnxModel, node: CnnxNode) -> bool:\n# Set constant Value\n# Apply constant Value to weight, and bias\nparent_conv = node.prev_nodes[0]\nif not node.inputs[1].inout.shape or len(node.inputs[0].inout.shape) > 2 and len(node.inputs[1].inout.shape) > 2:\nshape_0 = node.inputs[0].inout.shape if node.inputs[0].inout.shape is not None else []\nshape_1 = node.inputs[1].inout.shape if node.inputs[1].inout.shape is not None else []\nif shape_0[-2:] == shape_1[-2:]:\nkernel_data = node.inputs[1].inout.data if node.inputs[1].inout.data is not None else node.inputs[0].inout.data\nbias_data = node.inputs[1].inout.data if node.inputs[1].inout.data is not None else node.inputs[0].inout.data\nelse:\nkernel_data = node.inputs[1].inout.data.reshape((-1, 1, 1, 1)) if node.inputs[1].inout.data is not None else node.inputs[0].inout.data.reshape((-1, 1, 1, 1))\nbias_data = node.inputs[1].inout.data.reshape((-1)) if node.inputs[1].inout.data is not None else node.inputs[0].inout.data.reshape((-1))\nif node.op_type == 'Add':\nif len(parent_conv.inputs) != 3:\nparent_conv.inputs.append(self.make_bias_input(parent_conv))\nnew_data = parent_conv.inputs.data[2].inout.data + bias_data\nparent_conv.inputs.data[2].inout.data = new_data\nparent_conv.inputs.data[2].inout.shape = new_data.shape\n\nelif node.op_type == 'Sub':\nif len(parent_conv.inputs) != 3:\nparent_conv.inputs.append(self.make_bias_input(parent_conv))\nnew_data = parent_conv.inputs.data[2].inout.data - bias_data\nparent_conv.inputs.data[2].inout.data = new_data\nparent_conv.inputs.data[2].inout.shape = new_data.shape\nelif node.op_type == 'Mul':\nparent_conv.inputs.data[1].inout.data = parent_conv.inputs.data[1].inout.data * kernel_data\nif len(parent_conv.inputs) == 3:\nparent_conv.inputs.data[2].inout.data = parent_conv.inputs.data[2].inout.data * bias_data\n\nelif node.op_type == 'Div':\nparent_conv.inputs.data[1].inout.data = parent_conv.inputs.data[1].inout.data / kernel_data\nif len(parent_conv.inputs) == 3:\nparent_conv.inputs.data[2].inout.data = parent_conv.inputs.data[2].inout.data / bias_data\n\nconnections = []\nfor next_node in node.next_nodes:\nfor output_tensor in node.outputs:\nport_from = node.prev_nodes[0].outputs[0]\nfor i in next_node.inputs:\nif i.name == output_tensor.name:\nconnections.append((port_from, i))\n\noptimized_nodes = [parent_conv]\nprevious_nodes = [node]\n\ntry:\nmodel.graph.nodes.remove(node)\nfor connection in connections:\nmodel.graph.nodes.connect(connection[0], connection[1])\nlogger.info(f'Fuse {node.op_type}({node.name}) into Deconv({parent_conv.name})')\nreturn True\nexcept Exception as e:\nlogger.error(e)\nreturn False\n\n\nclass CaseFuseMath(TemplateCaseInternal):\ndef __init__(self, device_info):\nself.name = 'Fuse Math to DWCONV'\nself.step_list = [StepFuseMath(device_info)]\n\n# Check the number of Child of [CONV, DWCONV]\ndef origin_condition(self, model: CnnxModel, node: CnnxNode) -> bool:\nif node.prev_nodes == []:\nreturn False\nif node.inputs[0].inout.data is None and node.inputs[1].inout.data is None:\nreturn False\nfm_shape = node.inputs[0].inout.shape if node.inputs[0].inout.data is None else node.inputs[1].inout.shape\nbias_data = node.inputs[0].inout.data if node.inputs[0].inout.data is not None else node.inputs[1].inout.data\n\n# if bias data just have one value\n# if length of bias data same with output_channel of prev conv node\nif isinstance(bias_data.tolist(), list) and fm_shape[1] != bias_data.reshape((-1)).shape[0]:\nreturn False\nif len(node.prev_nodes[0].next_nodes) == 1 and \\\nlen(node.prev_nodes) == 1 and \\\nnode.inputs[0].inout.shape != node.inputs[1].inout.shape:\nif node.prev_nodes[0].op_type in ['Conv', 'ConvTranspose'] and \\\nnode.module is None:\nreturn True\nreturn False\n\n\nclass TemplateFuseMath(OptimizerTemplateInternal):\ndef __init__(self):\nself.name = 'FuseMath'\n# declare Case\nself.case_list = [\nCaseFuseMath()\n]\n'''\naccording to Flow, The first thing you encounter when navigating the nodes in the graph is trigger_op.\nIn this case, it was written to operate only when the input shape of the four arithmetic operations operator\nand a input feature map of node has 4 dimension.\n'''\ndef trigger_op(self, node: CnnxNode):\noptypes = ['Add', 'Sub', 'Mul', 'Div']\nif node.op_type in optypes:\nif len(node.inputs[0].shape) == 4 or len(node.inputs[1].shape) == 4:\nreturn True\nreturn False\n```",
      "<2-hop>\n\nThe Optimizer Templates are executed as shown in the flow chart below. In searching for the model, the node meets the trigger_op method of the template for the first time. Then it loops through the origin_condition of the case. If the condition is correct, it optimizes the module through steps.  \n![flow](images/flow.png)"
    ],
    "reference": "The OptimizerTemplateInternal is utilized in the optimization process by first executing the trigger_op method, which checks if the node's operation type is one of the four basic arithmetic operations (Add, Sub, Mul, Div) and whether the input feature map has four dimensions. If these conditions are met, the process continues to the origin_condition method of the case. This method further checks specific conditions, such as ensuring that the previous node is a convolution layer and that the arithmetic operator is the only node following it. If these conditions are satisfied, the optimization steps are executed to fuse the arithmetic operations into the convolution layer, thereby enhancing the model's performance.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\optimizer\\optimizer.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:00:51.600674+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the Exynos Neural Network Software Development Kit used for in Android app development?",
    "reference_contexts": [
      "This guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (Exynos AI Studio).\nIt outlines the steps for creating an application that utilizes the Exynos AI Studio to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the Exynos AI Studio in their Android applications."
    ],
    "reference": "The Exynos Neural Network Software Development Kit (Exynos AI Studio) is used for developing an image classification Android application. It provides the necessary steps for creating an application that utilizes the Exynos AI Studio to run neural network models on Samsung Exynos hardware, including Android project setup, function implementation, and NNC model conversion.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:01:17.722069+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How can Samsung Exynos be utilized in developing an Android application for image classification?",
    "reference_contexts": [
      "This guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (Exynos AI Studio).\nIt outlines the steps for creating an application that utilizes the Exynos AI Studio to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the Exynos AI Studio in their Android applications."
    ],
    "reference": "Samsung Exynos can be utilized in developing an image classification Android application by using the Exynos Neural Network Software Development Kit (Exynos AI Studio). This guide outlines the steps for creating an application that runs neural network models on Samsung Exynos hardware, covering important aspects such as Android project setup, function implementation, and NNC model conversion.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:01:17.722069+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does the Exynos AI Studio enhance the performance of neural network models on Samsung Exynos hardware?",
    "reference_contexts": [
      "The [Exynos AI Studio](https://soc-developer.semiconductor.samsung.com/enn-sdk) provides a tool for converting [TFLite](https://www.tensorflow.org/lite) neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the [Samsung Exynos](https://semiconductor.samsung.com/processor/) hardware, to ensure optimal performance.\nThe Exynos AI Studio provides a framework that facilitates the execution of NNC models on the Exynos platform."
    ],
    "reference": "The Exynos AI Studio enhances the performance of neural network models on Samsung Exynos hardware by providing a tool for converting TFLite neural network models into models in Neural Network Container (NNC) format. This conversion allows the NN models to operate efficiently on the Exynos hardware, ensuring optimal performance. Additionally, the Exynos AI Studio offers a framework that facilitates the execution of NNC models on the Exynos platform.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:01:17.722069+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does the Exynos AI Studio enhance the performance of neural network models on Exynos hardware?",
    "reference_contexts": [
      "The [Exynos AI Studio](https://soc-developer.semiconductor.samsung.com/enn-sdk) provides a tool for converting [TFLite](https://www.tensorflow.org/lite) neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the [Samsung Exynos](https://semiconductor.samsung.com/processor/) hardware, to ensure optimal performance.\nThe Exynos AI Studio provides a framework that facilitates the execution of NNC models on the Exynos platform."
    ],
    "reference": "The Exynos AI Studio enhances the performance of neural network models on Exynos hardware by providing a tool for converting TFLite neural network models into models in Neural Network Container (NNC) format. This conversion allows the NN models to operate efficiently on the Samsung Exynos hardware, ensuring optimal performance.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:01:17.722069+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does Exynos contribute to the sample application's functionality?",
    "reference_contexts": [
      "The sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the Exynos AI Studio to efficiently execute the NN model on the Exynos platform."
    ],
    "reference": "Exynos contributes to the sample application's functionality by efficiently executing the NN model on the Exynos platform, leveraging the Exynos AI Studio.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:01:17.722069+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does the Exynos AI Stodio enhance the performance of Android applications that utilize camera input for object classification?",
    "reference_contexts": [
      "The sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the Exynos AI Studio to efficiently execute the NN model on the Exynos platform."
    ],
    "reference": "The Exynos AI Studio enhances the performance of Android applications by efficiently executing the NN model on the Exynos platform, which is utilized in the sample application that takes input from a camera feed or an image to classify the object in the input.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:01:17.722069+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What TFLite model is used for image classification?",
    "reference_contexts": [
      "This example uses the quantized `Inception v4` TFLite model from [TensorFlow Hub](https://tfhub.dev/tensorflow/lite-model/inception_v4_quant/1/default/1) for image classification."
    ],
    "reference": "This example uses the quantized `Inception v4` TFLite model from TensorFlow Hub for image classification.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:01:17.722069+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the TFLite model used for in this example?",
    "reference_contexts": [
      "This example uses the quantized `Inception v4` TFLite model from [TensorFlow Hub](https://tfhub.dev/tensorflow/lite-model/inception_v4_quant/1/default/1) for image classification."
    ],
    "reference": "The TFLite model used in this example is the quantized `Inception v4` model, which is utilized for image classification.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:01:17.722069+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does the Exynos AI Studio enhance image processing in applications that utilize camera and image modes?",
    "reference_contexts": [
      "<1-hop>\n\nThe [Exynos AI Studio](https://soc-developer.semiconductor.samsung.com/enn-sdk) provides a tool for converting [TFLite](https://www.tensorflow.org/lite) neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the [Samsung Exynos](https://semiconductor.samsung.com/processor/) hardware, to ensure optimal performance.\nThe Exynos AI Studio provides a framework that facilitates the execution of NNC models on the Exynos platform.",
      "<2-hop>\n\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.  \nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.  \n<img src=\"./guide/img/sample1.png\" alt=\"drawing\" width=\"200\"/>\n<img src=\"./guide/img/sample2.png\" alt=\"drawing\" width=\"200\"/>\n<img src=\"./guide/img/sample3.png\" alt=\"drawing\" width=\"200\"/>"
    ],
    "reference": "The Exynos AI Studio enhances image processing in applications by providing a tool for converting TFLite neural network models into Neural Network Container (NNC) format, which allows these models to operate efficiently on Samsung Exynos hardware. In the sample application, users can select between camera and image modes, where the application processes images from the library or the camera feed, displaying the classified items and their scores as processed output.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:01:17.722069+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does the Exynos AI Studio help in executing NN models in a sample application?",
    "reference_contexts": [
      "<1-hop>\n\nThe [Exynos AI Studio](https://soc-developer.semiconductor.samsung.com/enn-sdk) provides a tool for converting [TFLite](https://www.tensorflow.org/lite) neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the [Samsung Exynos](https://semiconductor.samsung.com/processor/) hardware, to ensure optimal performance.\nThe Exynos AI Studio provides a framework that facilitates the execution of NNC models on the Exynos platform.",
      "<2-hop>\n\nThe sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the Exynos AI Studio to efficiently execute the NN model on the Exynos platform."
    ],
    "reference": "The Exynos AI Studio helps in executing NN models in a sample application by providing a tool for converting TFLite neural network models into Neural Network Container (NNC) format, which allows these models to operate efficiently on the Samsung Exynos hardware, ensuring optimal performance. The sample application utilizes this framework to classify objects from a camera feed or an image.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:01:17.722069+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do I add C++ to module in Android project?",
    "reference_contexts": [
      "1. Right click **Project** panel with **Android** option being selected.  \n<img src=\"./img/cpp1.png\" alt=\"drawing\" width=\"500\"/>  \n1. Select the **Add C++ to Module** option and click OK.  \n<img src=\"./img/cpp2.png\" alt=\"drawing\" width=\"500\"/>"
    ],
    "reference": "To add C++ to a module in an Android project, you need to right click on the Project panel with the Android option selected, then select the Add C++ to Module option and click OK.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:01:43.570409+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the steps to include enn_api-type_ndk_v1.h in an Android project?",
    "reference_contexts": [
      "Download the ENN framework library from [resources](https://soc-developer.semiconductor.samsung.com/support/resource).\nTo load the necessary libraries, perform the folowing steps:  \n1. Modiy Android Manifest to:\n```xml\n<manifest>\n<application>\n...\n<!-- Declare the native library in the Android Manifest -->\n<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />\n...\n</application>\n</manifest>\n```  \n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/app/src/main/jniLibs/arm64-v8a`.\n1. copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/app/src/main/cpp/include`.\n1. Modify `CMakeLists.txt` that is created [here](#adding-c-to-module).  \n```cmake\n# Include the directory where the header files are located\ninclude_directories(include)\n\n# Declare the imported shared library\nadd_library(\nenn_service_so\nSHARED\nIMPORTED\n)\n\n# Set the location of the imported library\nset_target_properties(\nenn_service_so\nPROPERTIES IMPORTED_LOCATION\n${CMAKE_SOURCE_DIR}/../jniLibs/${ANDROID_ABI}/libenn_public_api_ndk_v1.so\n)\n\n# Link the imported library to the target library\ntarget_link_libraries(\n...\nenn_service_so\n...\n)\n```"
    ],
    "reference": "To include enn_api-type_ndk_v1.h in an Android project, you need to copy it to `${APP_ROOT}/app/src/main/cpp/include` and modify the CMakeLists.txt to include the directory where the header files are located by using the command 'include_directories(include)'.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:01:43.570409+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the steps to configure the arm64-v8a ABI in the ENN framework's build.gradle file?",
    "reference_contexts": [
      "The ENN framework currently supports only the `arm64-v8a` ABI. To set ABI, modify the `build.gradle` file and specify the ABI as `arm64-v8a`.\n```\ndefaultConfig {\n...\nndk {\nabiFilters \"arm64-v8a\"\n}\n...\n}\n```"
    ],
    "reference": "To configure the arm64-v8a ABI in the ENN framework, you need to modify the build.gradle file. Specifically, you should set the ABI by including the following configuration in the defaultConfig section: `ndk { abiFilters \"arm64-v8a\" }`.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:01:43.570409+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the benefits of using C++ in conjunction with the Java Native Interface (JNI) for accessing libraries?",
    "reference_contexts": [
      "The Java Native Interface (JNI) is a framework that allows Java code to interact with the code written in other languages such as C or C++. In the sample application, JNI accesses features or libraries such as the ENN framework that are written in C or C++.  \n1. Create `enn_jni.cc` in `cpp` directory.\n1. Modify `CMakeLists.txt` created [here](#adding-c-to-module).  \n```cmake\nadd_library(\nenn_jni\nSHARED\nenn_jni.cc\n)\n\ntarget_link_libraries(\nenn_jni\n)\n```"
    ],
    "reference": "The Java Native Interface (JNI) allows Java code to interact with code written in other languages such as C or C++. This enables the use of features or libraries, like the ENN framework, that are implemented in C or C++, providing performance benefits and access to existing codebases.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:01:43.570409+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What Android do for JNI wrapper?",
    "reference_contexts": [
      "Following function is an example of an implemented JNI wrapper.\nFor more information, refer to the  [Android Developer Documentation](https://developer.android.com/ndk/samples/sample_hellojni).  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/cpp/enn_jni.cc#L57)):\n```cpp\nextern \"C\"\nJNIEXPORT jlong JNICALL\nJava_com_samsung_imageclassification_executor_ModelExecutor_ennOpenModel(\nJNIEnv *env,\njobject thiz,\njstring j_filename\n) {\nEnnModelId model_id;\nconst char *filename = env->GetStringUTFChars(j_filename, 0);\n\nif (enn::api::EnnOpenModel(filename, &model_id)) {\n__android_log_print(ANDROID_LOG_ERROR, LOG_TAG, \"EnnOpenModel of [%s] Failed\", filename);\n}\n\nreturn static_cast<jlong>(model_id);\n}\n```"
    ],
    "reference": "The Android Developer Documentation provides information on implementing JNI wrappers, as shown in the example function that demonstrates how to open a model using JNI.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:01:43.570409+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How can JNI be utilized in Kotlin for machine learning applications?",
    "reference_contexts": [
      "Following is an example for using JNI function in Kotlin.\n1. Load the JNI library\n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/executor/ModelExecutor.kt#L42)):\n```kotlin\ninit {\nSystem.loadLibrary(\"enn_jni\")\n...\n}\n```\n2. Declare the external function\n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/executor/ModelExecutor.kt#L28)):\n```kotlin\nprivate external fun ennOpenModel(filename: String): Long\n...\n```\n3. Use the JNI function ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/executor/ModelExecutor.kt#L54)):\n```kotlin\n//  to open a model\nprivate fun setupENN() {\n...\nmodelId = ennOpenModel(fileAbsoluteDirectory)\n...\n}\n```"
    ],
    "reference": "JNI can be utilized in Kotlin for machine learning applications by following these steps: First, load the JNI library using `System.loadLibrary(\"enn_jni\")`. Next, declare the external function with `private external fun ennOpenModel(filename: String): Long`. Finally, use the JNI function in a method, such as `private fun setupENN() { modelId = ennOpenModel(fileAbsoluteDirectory) }` to open a model.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:01:43.570409+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain the function of ennReleaseBuffers in the context of buffer management within the ENN framework?",
    "reference_contexts": [
      "Following table describes the implemented native functions for the sample application.  \n| Method | Description | Inputs | Outputs |\n| --- | --- | --- | --- |\n| [`ennInitialize`](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/cpp/enn_jni.cc#L35) | Initialize the ENN framework | - | - |\n| [`ennDeinitialize`](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/cpp/enn_jni.cc#L46) | Deinitialize the ENN framework | - | - |\n| [`ennOpenModel`](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/cpp/enn_jni.cc#L57) | Open a Model from the model file. | `filename`: String - Directory of the model `nnc` file | `modelID`: Long - Model ID. |\n| [`ennCloseModel`](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/cpp/enn_jni.cc#L74) | Close the model and free up all resources from `ennOpenModel`. | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| [`ennAllocateAllBuffers`](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/cpp/enn_jni.cc#L86)  | Allocate all buffers that a caller should allocate. | `modelID`: Long - Model ID from `ennOpenModel` | `bufferSetInfo`: BufferSetInfo - Custom class that includes pointer and size of the EnnBuffer array. |\n| [`ennReleaseBuffers`](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/cpp/enn_jni.cc#L103) | Release buffer array from `ennAllocateAllBuffers` | `bufferSet`: Long - pointer of buffer set array.<br>`bufferSize`: Int - total number of buffers. | - |\n| [`ennExecute`](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/cpp/enn_jni.cc#L118) | Request to service core to execute model with committed buffers | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| [`ennMemcpyHostToDevice`](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/cpp/enn_jni.cc#L130) | Copy ByteArray to buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer.<br>`data`: ByteArray - ByteArray to copy. | - |\n| [`ennMemcpyDeviceToHost`](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/cpp/enn_jni.cc#L150) | Copy ByteArray from buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer. | `data`: ByteArray - Copied ByteArray. |"
    ],
    "reference": "The function ennReleaseBuffers is designed to release the buffer array that was allocated using ennAllocateAllBuffers. It takes two inputs: bufferSet, which is a pointer to the buffer set array, and bufferSize, which indicates the total number of buffers. This function is essential for managing memory effectively within the ENN framework by ensuring that allocated resources are properly freed when they are no longer needed.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:01:43.570409+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain the role of the `ennInitialize` function in the lifecycle of executing neural network models?",
    "reference_contexts": [
      "To execute the NN models with implemented native functions, perform the following steps.  \n1. Preparing Execution of Model\n1. Initialize the framework using the `ennInitialize` function.\n2. Load the ML model into the framework using the `ennOpenModel` function.\n3. Allocate and commit the necessary buffers using the `ennAllocateAllBuffers` function.\n2. Executing a Model\n1. Set input data as a parameter.\n2. Call the `ennExecute` function.\n3. Get the execution result as a return.\n3. Deinitializing the Framework\n1. Release the allocated memory of the buffers with the `ennReleaseBuffers` function.\n2. Close the model and release other resources with the `ennCloseModel` function.\n3. Deinitialize the framework with the `ennDeinitialize` function."
    ],
    "reference": "The `ennInitialize` function is used to initialize the framework, which is the first step in preparing for the execution of neural network models. This function sets up the necessary environment for the model to be loaded and executed.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:01:43.570409+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do you use the ENN framework library with Java Native Interface to implement a model opening function?",
    "reference_contexts": [
      "<1-hop>\n\nDownload the ENN framework library from [resources](https://soc-developer.semiconductor.samsung.com/support/resource).\nTo load the necessary libraries, perform the folowing steps:  \n1. Modiy Android Manifest to:\n```xml\n<manifest>\n<application>\n...\n<!-- Declare the native library in the Android Manifest -->\n<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />\n...\n</application>\n</manifest>\n```  \n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/app/src/main/jniLibs/arm64-v8a`.\n1. copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/app/src/main/cpp/include`.\n1. Modify `CMakeLists.txt` that is created [here](#adding-c-to-module).  \n```cmake\n# Include the directory where the header files are located\ninclude_directories(include)\n\n# Declare the imported shared library\nadd_library(\nenn_service_so\nSHARED\nIMPORTED\n)\n\n# Set the location of the imported library\nset_target_properties(\nenn_service_so\nPROPERTIES IMPORTED_LOCATION\n${CMAKE_SOURCE_DIR}/../jniLibs/${ANDROID_ABI}/libenn_public_api_ndk_v1.so\n)\n\n# Link the imported library to the target library\ntarget_link_libraries(\n...\nenn_service_so\n...\n)\n```",
      "<2-hop>\n\nFollowing function is an example of an implemented JNI wrapper.\nFor more information, refer to the  [Android Developer Documentation](https://developer.android.com/ndk/samples/sample_hellojni).  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/cpp/enn_jni.cc#L57)):\n```cpp\nextern \"C\"\nJNIEXPORT jlong JNICALL\nJava_com_samsung_imageclassification_executor_ModelExecutor_ennOpenModel(\nJNIEnv *env,\njobject thiz,\njstring j_filename\n) {\nEnnModelId model_id;\nconst char *filename = env->GetStringUTFChars(j_filename, 0);\n\nif (enn::api::EnnOpenModel(filename, &model_id)) {\n__android_log_print(ANDROID_LOG_ERROR, LOG_TAG, \"EnnOpenModel of [%s] Failed\", filename);\n}\n\nreturn static_cast<jlong>(model_id);\n}\n```"
    ],
    "reference": "To use the ENN framework library with Java Native Interface (JNI) for implementing a model opening function, first download the ENN framework library and modify the Android Manifest to declare the native library. Then, copy the necessary shared library and header files to the appropriate directories in your project. Finally, you can implement a JNI wrapper function, such as the example provided, which uses the `EnnOpenModel` function to open a model and handle any errors that may occur.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:01:43.570409+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do you add C++ to an Android project and implement a JNI wrapper for model execution?",
    "reference_contexts": [
      "<1-hop>\n\n1. Right click **Project** panel with **Android** option being selected.  \n<img src=\"./img/cpp1.png\" alt=\"drawing\" width=\"500\"/>  \n1. Select the **Add C++ to Module** option and click OK.  \n<img src=\"./img/cpp2.png\" alt=\"drawing\" width=\"500\"/>",
      "<2-hop>\n\nFollowing function is an example of an implemented JNI wrapper.\nFor more information, refer to the  [Android Developer Documentation](https://developer.android.com/ndk/samples/sample_hellojni).  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/cpp/enn_jni.cc#L57)):\n```cpp\nextern \"C\"\nJNIEXPORT jlong JNICALL\nJava_com_samsung_imageclassification_executor_ModelExecutor_ennOpenModel(\nJNIEnv *env,\njobject thiz,\njstring j_filename\n) {\nEnnModelId model_id;\nconst char *filename = env->GetStringUTFChars(j_filename, 0);\n\nif (enn::api::EnnOpenModel(filename, &model_id)) {\n__android_log_print(ANDROID_LOG_ERROR, LOG_TAG, \"EnnOpenModel of [%s] Failed\", filename);\n}\n\nreturn static_cast<jlong>(model_id);\n}\n```"
    ],
    "reference": "To add C++ to an Android project, right-click the Project panel with the Android option selected, then select the Add C++ to Module option and click OK. After that, you can implement a JNI wrapper for model execution using the provided example function, which demonstrates how to open a model using JNI in an Android application.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:01:43.570409+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the role of CameraFragment.kt in the sample application?",
    "reference_contexts": [
      "Following classes are required to create the sample application:  \n- `app/java/package/executor`\n- `ModelExecutor.kt`: Includes methods for processing images and return classification results.\n- `app/java/package/fragments`\n- `CameraFragment.kt`: Handles user interactions and updates the UI in Camera mode.\n- `ImageFragment.kt`: Handles user interactions and updates the UI in Image mode.\n- `app/java/package/enn_type`\n- `BufferSetInfo`: Data class that holds information about the buffer set and number of input/output layers"
    ],
    "reference": "CameraFragment.kt handles user interactions and updates the UI in Camera mode.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\function-implementation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:02:14.552316+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What does ImageClassifierHelper.ClassifierListener do in image classification?",
    "reference_contexts": [
      "The `ExecutorListener` interface in `ModelExecutor.kt` is a custom listener that provides two methods such as `onError` and `onResults`.  \n- `onError`: Triggered when an error occurs during image classification. It logs the error or displays an error message.\n- `onResults`: Triggered when image classification is successfully completed. It updates the UI with the classification results and the time taken for classification.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/executor/ModelExecutor.kt#L258)):\n```kotlin\ninterface ExecutorListener {\nfun onError(error: String)\nfun onResults(\nresult: Map<String, Float>, inferenceTime: Long\n)\n}\n```  \nImplement the interface in the `fragment` class.\nThe `onError` method logs the error and the `onResults` method updates the UI.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L185)):  \n```kotlin\nclass SampleFragment : Fragment(), ImageClassifierHelper.ClassifierListener {\n...\noverride fun onError(error: String) {\nLog.e(TAG, \"ModelExecutor error: $error\")\n}\n\noverride fun onResults(\nresult: Map<String, Float>, inferenceTime: Long\n) {\nactivity?.runOnUiThread {\nbinding.processData.inferenceTime.text = \"$inferenceTime ms\"\nupdateUI(result)\n}\n}\n}\n```"
    ],
    "reference": "ImageClassifierHelper.ClassifierListener is implemented in the fragment class to handle image classification results. It has two methods: onError, which logs errors during classification, and onResults, which updates the UI with the classification results and the time taken for classification.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\function-implementation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:02:14.552316+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How is the ModelExecutor used in image classification?",
    "reference_contexts": [
      "Create the `ModelExecutor` object iin the `fragment` class with the current context and the fragment as `executorListener`. The `process` method of `modelExecutor` is called to start the image classification.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L53)):\n```kotlin\nmodelExecutor = ModelExecutor(\ncontext = requireContext(), executorListener = this\n)\n...\nmodelExecutor.process(bitmapBuffer)\n```  \nIn the `ModelExecutor.kt` class, the `process` method processes the image and calls the `onResults` method of `executorListener` to pass the results back to the fragment.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/executor/ModelExecutor.kt#L76)):\n```kotlin\nfun process(image: Bitmap) {\n...\n\nexecutorListener?.onResults(\nresult, inferenceTime\n)\n}\n```"
    ],
    "reference": "The `ModelExecutor` object is created in the `fragment` class with the current context and the fragment as `executorListener`. The `process` method of `modelExecutor` is called to start the image classification, processing the image and calling the `onResults` method of `executorListener` to pass the results back to the fragment.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\function-implementation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:02:14.552316+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does the cameraSelector function relate to the setImageAnalyzer method in mobile application development?",
    "reference_contexts": [
      "The `setImageAnalyzer` function sets an `ImageAnalysis` object to process the camera feed.\nIt creates a bitmap buffer and processes each image frame.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L100)):\n```kotlin\nprivate fun setImageAnalyzer() {\nimageAnalyzer =\nImageAnalysis.Builder()\n.setTargetRotation(binding.viewFinder.display.rotation)\n.setBackpressureStrategy(ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST)\n.setOutputImageFormat(ImageAnalysis.OUTPUT_IMAGE_FORMAT_RGBA_8888)\n.build().also {\nit.setAnalyzer(cameraExecutor) { image ->\nif (!::bitmapBuffer.isInitialized) {\nbitmapBuffer = Bitmap.createBitmap(\nimage.width, image.height, Bitmap.Config.ARGB_8888\n)\n}\nprocess(image)\n}\n}\n}\n```  \nThe `setCamera` function introduced [here](getting-started-with-android-samples/setting-necessary-ui#camera-preview) is updated to include the `setImageAnalyzer` method.\nThis inclusion allows the camera feed to be analyzed and processed.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L65)):\n```kotlin\ncameraProviderFuture.addListener(\n{\n...\nsetPreview()\nsetImageAnalyzer()\n\ntry {\ncameraProvider.unbindAll()\ncamera = cameraProvider.bindToLifecycle(\nthis, cameraSelector, preview, imageAnalyzer\n)\npreview?.setSurfaceProvider(binding.viewFinder.surfaceProvider)\n}\n...\n},\n...\n)\n```"
    ],
    "reference": "The cameraSelector function is used in conjunction with the setImageAnalyzer method to bind the camera to the lifecycle of the application. The setImageAnalyzer method allows the camera feed to be analyzed and processed, which is essential for functionalities like image processing in mobile applications.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\function-implementation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:02:14.552316+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does ColorSpace relate to image processing in the provided context?",
    "reference_contexts": [
      "The `ActivityResult` object `getContent` retrieves an image from the device media.\nThe selected image is displayed in an ImageView and converted to a bitmap.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/ImageFragment.kt#L28)):\n```kotlin\nprivate val getContent =\nregisterForActivityResult(ActivityResultContracts.GetContent()) { uri: Uri? ->\nuri?.let {\nbinding.imageView.setImageURI(it)\nbinding.buttonProcess.isEnabled = true\n\nbitmapBuffer = ImageDecoder.decodeBitmap(\nImageDecoder.createSource(\nrequireContext().contentResolver,\nit\n)\n) { decoder, _, _ ->\ndecoder.setTargetColorSpace(ColorSpace.get(ColorSpace.Named.SRGB))\ndecoder.allocator = ImageDecoder.ALLOCATOR_SOFTWARE\ndecoder.setTargetSampleSize(1)\n}\n}\n}\n```  \nClick **Load** to launch the `ActivityResult`.\nIt allows the user to select an image from their device.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/ImageFragment.kt#L69)):\n```kotlin\nbinding.buttonLoad.setOnClickListener {\ngetContent.launch(\"image/*\")\n}\n```"
    ],
    "reference": "In the provided context, ColorSpace is used in the image processing workflow where the decoder is set to target the SRGB color space. This is done during the bitmap decoding process to ensure that the image is processed with the correct color representation.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\function-implementation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:02:14.552316+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do you use the copyNNCFromAssetsToInternalStorage function for NNC?",
    "reference_contexts": [
      "Use `copyNNCFromAssetsToInternalStorage` function to copy the NNC model file from the asset directory of app to its internal storage.\nIt is necessary to copy the NNC model file because the model file needs to be accessed from the internal storage when used by the ENN Framework.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/executor/ModelExecutor.kt#L231)):\n```kotlin\nprivate fun copyNNCFromAssetsToInternalStorage(filename: String) {\ntry {\nval inputStream = context.assets.open(filename)\nval outputFile = File(context.filesDir, filename)\nval outputStream = FileOutputStream(outputFile)\nval buffer = ByteArray(2048)\nvar bytesRead: Int\n\nwhile (inputStream.read(buffer).also { bytesRead = it } != -1) {\noutputStream.write(buffer, 0, bytesRead)\n}\ninputStream.close()\noutputStream.close()\n} catch (e: IOException) {\ne.printStackTrace()\n}\n}\n```"
    ],
    "reference": "The `copyNNCFromAssetsToInternalStorage` function is used to copy the NNC model file from the asset directory of the app to its internal storage. This is necessary because the model file needs to be accessed from the internal storage when used by the ENN Framework.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\function-implementation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:02:14.552316+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What information does the BufferSetInfo data class contain regarding the buffer set in a neural network model?",
    "reference_contexts": [
      "The `BufferSetInfo` data class holds the information about the buffer set used in the neural network model.\nIt includes the memory location of the buffer set (`buffer_set`), the number of input buffers (`n_in_buf`), and the number of output buffers (`n_out_buf`).\nUse the data class to return this information from the JNI library.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/enn_type/BufferSetInfo.kt)):\n```kotlin\nclass BufferSetInfo {\nvar buffer_set: Long = 0\nvar n_in_buf: Int = 0\nvar n_out_buf: Int = 0\n}\n```"
    ],
    "reference": "The `BufferSetInfo` data class holds information about the buffer set used in the neural network model, including the memory location of the buffer set (`buffer_set`), the number of input buffers (`n_in_buf`), and the number of output buffers (`n_out_buf`).",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\function-implementation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:02:14.552316+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does the `setImageAnalyzer` function integrate with the `ExecutorListener` interface in the context of image classification?",
    "reference_contexts": [
      "<1-hop>\n\nThe `ExecutorListener` interface in `ModelExecutor.kt` is a custom listener that provides two methods such as `onError` and `onResults`.  \n- `onError`: Triggered when an error occurs during image classification. It logs the error or displays an error message.\n- `onResults`: Triggered when image classification is successfully completed. It updates the UI with the classification results and the time taken for classification.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/executor/ModelExecutor.kt#L258)):\n```kotlin\ninterface ExecutorListener {\nfun onError(error: String)\nfun onResults(\nresult: Map<String, Float>, inferenceTime: Long\n)\n}\n```  \nImplement the interface in the `fragment` class.\nThe `onError` method logs the error and the `onResults` method updates the UI.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L185)):  \n```kotlin\nclass SampleFragment : Fragment(), ImageClassifierHelper.ClassifierListener {\n...\noverride fun onError(error: String) {\nLog.e(TAG, \"ModelExecutor error: $error\")\n}\n\noverride fun onResults(\nresult: Map<String, Float>, inferenceTime: Long\n) {\nactivity?.runOnUiThread {\nbinding.processData.inferenceTime.text = \"$inferenceTime ms\"\nupdateUI(result)\n}\n}\n}\n```",
      "<2-hop>\n\nThe `setImageAnalyzer` function sets an `ImageAnalysis` object to process the camera feed.\nIt creates a bitmap buffer and processes each image frame.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L100)):\n```kotlin\nprivate fun setImageAnalyzer() {\nimageAnalyzer =\nImageAnalysis.Builder()\n.setTargetRotation(binding.viewFinder.display.rotation)\n.setBackpressureStrategy(ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST)\n.setOutputImageFormat(ImageAnalysis.OUTPUT_IMAGE_FORMAT_RGBA_8888)\n.build().also {\nit.setAnalyzer(cameraExecutor) { image ->\nif (!::bitmapBuffer.isInitialized) {\nbitmapBuffer = Bitmap.createBitmap(\nimage.width, image.height, Bitmap.Config.ARGB_8888\n)\n}\nprocess(image)\n}\n}\n}\n```  \nThe `setCamera` function introduced [here](getting-started-with-android-samples/setting-necessary-ui#camera-preview) is updated to include the `setImageAnalyzer` method.\nThis inclusion allows the camera feed to be analyzed and processed.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L65)):\n```kotlin\ncameraProviderFuture.addListener(\n{\n...\nsetPreview()\nsetImageAnalyzer()\n\ntry {\ncameraProvider.unbindAll()\ncamera = cameraProvider.bindToLifecycle(\nthis, cameraSelector, preview, imageAnalyzer\n)\npreview?.setSurfaceProvider(binding.viewFinder.surfaceProvider)\n}\n...\n},\n...\n)\n```"
    ],
    "reference": "The `setImageAnalyzer` function sets up an `ImageAnalysis` object to process the camera feed, which is essential for analyzing each image frame. This function is called within the camera setup process, allowing the camera feed to be analyzed. Meanwhile, the `ExecutorListener` interface provides methods like `onError` and `onResults` that handle the outcomes of the image classification process. When the image classification is completed, the `onResults` method updates the UI with the classification results, while the `onError` method logs any errors that occur during this process. Together, these components ensure that the camera feed is effectively analyzed and that the results are communicated back to the user interface.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\function-implementation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:02:14.552316+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How can the `BufferSetInfo` data class be utilized in conjunction with the `ActivityResult` object to process an image selected from the device media in a Kotlin mobile application?",
    "reference_contexts": [
      "<1-hop>\n\nThe `BufferSetInfo` data class holds the information about the buffer set used in the neural network model.\nIt includes the memory location of the buffer set (`buffer_set`), the number of input buffers (`n_in_buf`), and the number of output buffers (`n_out_buf`).\nUse the data class to return this information from the JNI library.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/enn_type/BufferSetInfo.kt)):\n```kotlin\nclass BufferSetInfo {\nvar buffer_set: Long = 0\nvar n_in_buf: Int = 0\nvar n_out_buf: Int = 0\n}\n```",
      "<2-hop>\n\nThe `ActivityResult` object `getContent` retrieves an image from the device media.\nThe selected image is displayed in an ImageView and converted to a bitmap.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/ImageFragment.kt#L28)):\n```kotlin\nprivate val getContent =\nregisterForActivityResult(ActivityResultContracts.GetContent()) { uri: Uri? ->\nuri?.let {\nbinding.imageView.setImageURI(it)\nbinding.buttonProcess.isEnabled = true\n\nbitmapBuffer = ImageDecoder.decodeBitmap(\nImageDecoder.createSource(\nrequireContext().contentResolver,\nit\n)\n) { decoder, _, _ ->\ndecoder.setTargetColorSpace(ColorSpace.get(ColorSpace.Named.SRGB))\ndecoder.allocator = ImageDecoder.ALLOCATOR_SOFTWARE\ndecoder.setTargetSampleSize(1)\n}\n}\n}\n```  \nClick **Load** to launch the `ActivityResult`.\nIt allows the user to select an image from their device.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/ImageFragment.kt#L69)):\n```kotlin\nbinding.buttonLoad.setOnClickListener {\ngetContent.launch(\"image/*\")\n}\n```"
    ],
    "reference": "The `BufferSetInfo` data class can be utilized to manage the buffer set information required for processing images in a Kotlin mobile application. It holds details such as the memory location of the buffer set, the number of input buffers, and the number of output buffers. When an image is selected using the `ActivityResult` object `getContent`, the selected image is displayed in an ImageView and converted to a bitmap. This bitmap can then be processed using the buffer set information from the `BufferSetInfo` class, allowing for efficient image processing in the neural network model.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\function-implementation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:02:14.552316+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Wht is the role of TextView in an Android app?",
    "reference_contexts": [
      "- **ImageView**: Displays the image that must be classified.\n- **Image Load Button**: Triggers the loading of an image.\n- **Inference Button**: Initiates the image classification process.\n- **TextView**: Displays the classification result and score."
    ],
    "reference": "TextView displays the classification result and score.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\setting-necessary-ui.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:02:54.374499+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What role does TextView play in the context of mobile app development, particularly in relation to real-time functionalities?",
    "reference_contexts": [
      "- **PreviewView**: Displays a live camera feed for real-time classification.\n- **TextView**: Displays the classification result and score.  \nThe implemented UI resembles the figure as illustrated below.  \n<img src=\"./img/ui1.png\" alt=\"drawing\" width=\"200\"/>\n<img src=\"./img/ui2.png\" alt=\"drawing\" width=\"200\"/>"
    ],
    "reference": "TextView displays the classification result and score, which is essential for providing feedback to users in applications that utilize real-time functionalities like camera previews.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\setting-necessary-ui.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:02:54.374499+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How I use XML for show classification result and score in Android app?",
    "reference_contexts": [
      "The TextView displays the classification result and score to the user.\nIn the layout XML file, define a TextView with a unique ID.\nIn the Kotlin file, use this ID to reference the TextView and set its text to the classification result and score.  \n- Layout `app/res/layout/*.xml` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/res/layout/enn_info.xml#L18)):\n```xml\n<TextView\nandroid:id=\"@+id/sample_text\" />\n```  \n- Kotlin `app/java/package/fragments/Fragment.kt` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L194)):\n```kotlin\nbinding.sampleText.text = \"Sample Text\"\n```"
    ],
    "reference": "In the layout XML file, you define a TextView with a unique ID. Then, in the Kotlin file, you use this ID to reference the TextView and set its text to the classification result and score.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\setting-necessary-ui.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:02:54.374499+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do you use an ImageVew to display an image in a mobile application?",
    "reference_contexts": [
      "The ImageView displays the image that is classified.\nIn the layout XML file, define an ImageView with a unique ID.\nIn the Kotlin file, use this ID to reference the ImageView and set its image to the selected image.  \n- Layout `app/res/layout/*.xml` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/res/layout/fragment_image.xml#L19)):\n```xml\n<ImageView\nandroid:id=\"@+id/sample_image\" />\n```  \n- Kotlin `app/java/package/fragments/Fragment.kt` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/ImageFragment.kt#L41)):\n```kotlin\nbinding.imageView.setImageBitmap(resizedImage)\n```"
    ],
    "reference": "To use an ImageView to display an image in a mobile application, you first define an ImageView in the layout XML file with a unique ID. Then, in the Kotlin file, you reference this ID to set the image to the selected image. For example, in the layout XML, you would define it as `<ImageView android:id=\"@+id/sample_image\" />`, and in the Kotlin file, you would use `binding.imageView.setImageBitmap(resizedImage)` to set the image.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\setting-necessary-ui.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:02:54.374499+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do you implement a PreviewView for real-time camera feed in an Android application?",
    "reference_contexts": [
      "The PreviewView displays a live camera feed for real-time classification.\nIn the layout XML file, define a PreviewView with a unique ID.\nIn the Kotlin file, use this ID to reference the PreviewView and set up the camera preview.\nThis process involves creating a Preview object, binding it to the lifecycle of the Fragment, and setting its surface provider to the surface provider of PreviewView.  \n- Layout `app/res/layout/*.xml` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/res/layout/fragment_camera.xml#L9)):\n```xml\n<androidx.camera.view.PreviewView\nandroid:id=\"@+id/view_finder\" />\n```  \n- Kotlin `app/java/package/fragments/Fragment.kt` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L61)):\n```kotlin\nprivate fun setCamera() {\ncameraExecutor = Executors.newSingleThreadExecutor()\n\nval cameraProviderFuture = ProcessCameraProvider.getInstance(requireContext())\ncameraProviderFuture.addListener(\n{\nval cameraProvider = cameraProviderFuture.get()\nval cameraSelector = CameraSelector.DEFAULT_BACK_CAMERA\n\nsetPreview()\nsetImageAnalyzer()\n\ntry {\ncameraProvider.unbindAll()\ncamera = cameraProvider.bindToLifecycle(\nthis, cameraSelector, preview, imageAnalyzer\n)\n\npreview?.setSurfaceProvider(binding.viewFinder.surfaceProvider)\n} catch (exc: java.lang.Exception) {\nLog.e(TAG, \"Camera binding failed\", exc)\n}\n}, ContextCompat.getMainExecutor(requireContext())\n)\n}\n\nprivate fun setPreview() {\npreview = Preview.Builder().setTargetRotation(binding.viewFinder.display.rotation).build()\n}\n```"
    ],
    "reference": "To implement a PreviewView for real-time camera feed in an Android application, you need to define a PreviewView in your layout XML file with a unique ID. In the Kotlin file, reference this ID to set up the camera preview. This involves creating a Preview object, binding it to the lifecycle of the Fragment, and setting its surface provider to the surface provider of the PreviewView. For example, in your layout XML, you would include `<androidx.camera.view.PreviewView android:id=\"@+id/view_finder\" />`. In your Kotlin code, you would set up the camera by creating a camera executor, obtaining a camera provider, and binding the camera to the lifecycle while setting the surface provider to `binding.viewFinder.surfaceProvider`.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\setting-necessary-ui.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:02:54.374499+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do you display the classification result and score in a TextView after classifying an image shown in an ImageView?",
    "reference_contexts": [
      "<1-hop>\n\nThe ImageView displays the image that is classified.\nIn the layout XML file, define an ImageView with a unique ID.\nIn the Kotlin file, use this ID to reference the ImageView and set its image to the selected image.  \n- Layout `app/res/layout/*.xml` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/res/layout/fragment_image.xml#L19)):\n```xml\n<ImageView\nandroid:id=\"@+id/sample_image\" />\n```  \n- Kotlin `app/java/package/fragments/Fragment.kt` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/ImageFragment.kt#L41)):\n```kotlin\nbinding.imageView.setImageBitmap(resizedImage)\n```",
      "<2-hop>\n\nThe TextView displays the classification result and score to the user.\nIn the layout XML file, define a TextView with a unique ID.\nIn the Kotlin file, use this ID to reference the TextView and set its text to the classification result and score.  \n- Layout `app/res/layout/*.xml` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/res/layout/enn_info.xml#L18)):\n```xml\n<TextView\nandroid:id=\"@+id/sample_text\" />\n```  \n- Kotlin `app/java/package/fragments/Fragment.kt` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L194)):\n```kotlin\nbinding.sampleText.text = \"Sample Text\"\n```"
    ],
    "reference": "To display the classification result and score in a TextView after classifying an image shown in an ImageView, you first define a TextView with a unique ID in the layout XML file, similar to how you define an ImageView. For example, in the layout `app/res/layout/*.xml`, you would include `<TextView android:id=\"@+id/sample_text\" />`. In the Kotlin file, you would reference this TextView using its ID and set its text to the classification result and score, as shown in the Kotlin example: `binding.sampleText.text = \"Sample Text\"`. This process complements the display of the classified image in the ImageView, where you set the image using `binding.imageView.setImageBitmap(resizedImage)`.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\setting-necessary-ui.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:02:54.374499+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do you integrate the PreviewView for live camera feed and the TextView for displaying classification results in a Kotlin mobile application?",
    "reference_contexts": [
      "<1-hop>\n\nThe PreviewView displays a live camera feed for real-time classification.\nIn the layout XML file, define a PreviewView with a unique ID.\nIn the Kotlin file, use this ID to reference the PreviewView and set up the camera preview.\nThis process involves creating a Preview object, binding it to the lifecycle of the Fragment, and setting its surface provider to the surface provider of PreviewView.  \n- Layout `app/res/layout/*.xml` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/res/layout/fragment_camera.xml#L9)):\n```xml\n<androidx.camera.view.PreviewView\nandroid:id=\"@+id/view_finder\" />\n```  \n- Kotlin `app/java/package/fragments/Fragment.kt` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L61)):\n```kotlin\nprivate fun setCamera() {\ncameraExecutor = Executors.newSingleThreadExecutor()\n\nval cameraProviderFuture = ProcessCameraProvider.getInstance(requireContext())\ncameraProviderFuture.addListener(\n{\nval cameraProvider = cameraProviderFuture.get()\nval cameraSelector = CameraSelector.DEFAULT_BACK_CAMERA\n\nsetPreview()\nsetImageAnalyzer()\n\ntry {\ncameraProvider.unbindAll()\ncamera = cameraProvider.bindToLifecycle(\nthis, cameraSelector, preview, imageAnalyzer\n)\n\npreview?.setSurfaceProvider(binding.viewFinder.surfaceProvider)\n} catch (exc: java.lang.Exception) {\nLog.e(TAG, \"Camera binding failed\", exc)\n}\n}, ContextCompat.getMainExecutor(requireContext())\n)\n}\n\nprivate fun setPreview() {\npreview = Preview.Builder().setTargetRotation(binding.viewFinder.display.rotation).build()\n}\n```",
      "<2-hop>\n\nThe TextView displays the classification result and score to the user.\nIn the layout XML file, define a TextView with a unique ID.\nIn the Kotlin file, use this ID to reference the TextView and set its text to the classification result and score.  \n- Layout `app/res/layout/*.xml` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/res/layout/enn_info.xml#L18)):\n```xml\n<TextView\nandroid:id=\"@+id/sample_text\" />\n```  \n- Kotlin `app/java/package/fragments/Fragment.kt` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L194)):\n```kotlin\nbinding.sampleText.text = \"Sample Text\"\n```"
    ],
    "reference": "To integrate the PreviewView for live camera feed and the TextView for displaying classification results in a Kotlin mobile application, you first define a PreviewView in your layout XML file with a unique ID, such as `@+id/view_finder`. In your Kotlin file, you reference this ID to set up the camera preview by creating a Preview object and binding it to the lifecycle of the Fragment. You also define a TextView in your layout XML with a unique ID, like `@+id/sample_text`, and in your Kotlin code, you set its text to display the classification result and score. This involves using the `binding` object to update the TextView after processing the camera feed.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\setting-necessary-ui.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:02:54.374499+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the Navigation Component in Android development?",
    "reference_contexts": [
      "Before you proceed, ensure you have a basic understanding of the following Android development concepts:  \n- **Kotlin**: The primary programming language for Android development.\n- [**View Binding**](https://developer.android.com/topic/libraries/view-binding): A feature that allows you to easily write code that interacts with views.\n- [**Fragments**](https://developer.android.com/guide/fragments): A reusable piece of an user interface or behavior of Android application.\n- [**Navigation Component**](https://developer.android.com/guide/navigation): A component that helps to implement navigation.  \nIf you are new to Android, it is recommended to review Chapters 1 to 3 of the [Android Basics in Kotlin](https://developer.android.com/courses/android-basics-kotlin/course) course.\nThis course describes the basics of developing Android application using Kotlin."
    ],
    "reference": "The Navigation Component is a component that helps to implement navigation in Android applications.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:15.415295+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Wht is the Navigation Componet in Android devlopment?",
    "reference_contexts": [
      "Before you proceed, ensure you have a basic understanding of the following Android development concepts:  \n- **Kotlin**: The primary programming language for Android development.\n- [**View Binding**](https://developer.android.com/topic/libraries/view-binding): A feature that allows you to easily write code that interacts with views.\n- [**Fragments**](https://developer.android.com/guide/fragments): A reusable piece of an user interface or behavior of Android application.\n- [**Navigation Component**](https://developer.android.com/guide/navigation): A component that helps to implement navigation.  \nIf you are new to Android, it is recommended to review Chapters 1 to 3 of the [Android Basics in Kotlin](https://developer.android.com/courses/android-basics-kotlin/course) course.\nThis course describes the basics of developing Android application using Kotlin."
    ],
    "reference": "The Navigation Component is a component that helps to implement navigation in Android applications.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:15.415295+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do I install Android Studio?",
    "reference_contexts": [
      "1. **To Download Android studio**, visit the [official website](https://developer.android.com/studio) and click **Download Android Studio**.\n1. **To install and set up Android Studio**, execute the downloaded file and follow the instructions of installation wizard.  \n> For more information, such as requirement for each OS, refer to [Install Android Studio](https://developer.android.com/studio/install) from Android Developers.\n1. Add the path to the Platform Tools directory in environment variables\n- Windows\n1. Navigate to Start > Control Panel > System > Advanced System Settings > Environment Variables.\n1. Select `Path` User variable, then select **Edit**.\n1. Select New and add `%USERPROFILE%\\AppData\\Local\\Android\\sdk\\platform-tools` directory.\n1. Select **Ok** to close all the settings windows.S  \n<img src=\"./img/windows-env.jpg\" alt=\"drawing\" width=\"400\"/>"
    ],
    "reference": "To install and set up Android Studio, execute the downloaded file and follow the instructions of the installation wizard.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:15.415295+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How to dowload Andriod Studio?",
    "reference_contexts": [
      "1. **To Download Android studio**, visit the [official website](https://developer.android.com/studio) and click **Download Android Studio**.\n1. **To install and set up Android Studio**, execute the downloaded file and follow the instructions of installation wizard.  \n> For more information, such as requirement for each OS, refer to [Install Android Studio](https://developer.android.com/studio/install) from Android Developers.\n1. Add the path to the Platform Tools directory in environment variables\n- Windows\n1. Navigate to Start > Control Panel > System > Advanced System Settings > Environment Variables.\n1. Select `Path` User variable, then select **Edit**.\n1. Select New and add `%USERPROFILE%\\AppData\\Local\\Android\\sdk\\platform-tools` directory.\n1. Select **Ok** to close all the settings windows.S  \n<img src=\"./img/windows-env.jpg\" alt=\"drawing\" width=\"400\"/>"
    ],
    "reference": "To download Android Studio, visit the official website and click Download Android Studio.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:15.415295+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How can I enable Developer Mode on my ERD board to see the message 'You are now a developer'?",
    "reference_contexts": [
      "Enabling Developer Mode in ERD board\n1. Open the **Settings** app.\n2. Scroll down and tap **About phone**.\n3. Find the **Build number** and tap it a few times until the **You are now a developer** message appears.  \nEnabling USB Debugging in ERD board\n1. Navigate to the main **Settings** screen.\n2. Scroll down and tap **System**.\nThe **Developer options** is now displayed.\n3. Tap **Developer options**, then scroll down and turn on **USB debugging**."
    ],
    "reference": "To enable Developer Mode on your ERD board, open the **Settings** app, scroll down and tap **About phone**, find the **Build number**, and tap it a few times until the **You are now a developer** message appears.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:15.415295+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do I access the System settings on the ERD board?",
    "reference_contexts": [
      "Enabling Developer Mode in ERD board\n1. Open the **Settings** app.\n2. Scroll down and tap **About phone**.\n3. Find the **Build number** and tap it a few times until the **You are now a developer** message appears.  \nEnabling USB Debugging in ERD board\n1. Navigate to the main **Settings** screen.\n2. Scroll down and tap **System**.\nThe **Developer options** is now displayed.\n3. Tap **Developer options**, then scroll down and turn on **USB debugging**."
    ],
    "reference": "To access the System settings on the ERD board, navigate to the main Settings screen, scroll down, and tap System.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:15.415295+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the steps to connect the ERD board to a computer for use with Android Studio?",
    "reference_contexts": [
      "1. Connect the ERD board to your computer using a USB cable.\n1. In the pop-up that appears, select **Allow** to enable the USB debugging.\n1. Android Studio automatically detects the device. If the device is not detected, enable **File transfer** on the device."
    ],
    "reference": "To connect the ERD board to your computer for use with Android Studio, first connect the ERD board using a USB cable. Then, in the pop-up that appears, select **Allow** to enable USB debugging. Android Studio will automatically detect the device. If the device is not detected, ensure that **File transfer** is enabled on the device.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:15.415295+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What steps should I follow to connect the ERD board to my computer for Android development?",
    "reference_contexts": [
      "1. Connect the ERD board to your computer using a USB cable.\n1. In the pop-up that appears, select **Allow** to enable the USB debugging.\n1. Android Studio automatically detects the device. If the device is not detected, enable **File transfer** on the device."
    ],
    "reference": "To connect the ERD board to your computer, first use a USB cable to establish the connection. When a pop-up appears, select **Allow** to enable USB debugging. Android Studio will automatically detect the device; if it is not detected, ensure that **File transfer** is enabled on the device.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:15.415295+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Whtat are the key Android devlopment concepts to understand before updating the ERD board binary?",
    "reference_contexts": [
      "<1-hop>\n\n***<span style=\"color:red\">WARNING:</span>* Updating ERD board binary results in *erasing all data* on ERD board.**  \nTo update the binary of an ERD board (Windows):\n1. Download ERD board binary from [resources](https:/./soc-developer.semiconductor.samsung.com/support/resource).\n1. Extract the contents of the downloaded `zip` file.\n1. Enable USB debugging mode and connect the device as demonstrated [here](#configuring-the-erd-board).\n1. Boot ERD board to `fastboot` bootloader mode.\n```bash\nadb reboot bootloader\n```\n1. Check if ERD board is ready to flash by executing:\n```bash\nfastboot devices\n```\n1. From the extracted files, find and execute `ff_erd9925_all.exe`.\n1. Press any key to continue.\n1. After `ff_erd9925_all.exe` is executed, the ERD Board reboots automatically and the following message appears.\n```shell\n=======================================\nFINISHED\n2023/11/10 21:43:36\nSUCCESS 1/1 fastboot devices\n0) 00000a8fcf39b308, SUCCESS, elapsed=168s\n=======================================\n```",
      "<2-hop>\n\nBefore you proceed, ensure you have a basic understanding of the following Android development concepts:  \n- **Kotlin**: The primary programming language for Android development.\n- [**View Binding**](https://developer.android.com/topic/libraries/view-binding): A feature that allows you to easily write code that interacts with views.\n- [**Fragments**](https://developer.android.com/guide/fragments): A reusable piece of an user interface or behavior of Android application.\n- [**Navigation Component**](https://developer.android.com/guide/navigation): A component that helps to implement navigation.  \nIf you are new to Android, it is recommended to review Chapters 1 to 3 of the [Android Basics in Kotlin](https://developer.android.com/courses/android-basics-kotlin/course) course.\nThis course describes the basics of developing Android application using Kotlin."
    ],
    "reference": "Before updating the ERD board binary, it is essential to have a basic understanding of several Android development concepts, including Kotlin, which is the primary programming language for Android development, View Binding, which allows for easier interaction with views, Fragments, which are reusable pieces of the user interface or behavior of an Android application, and the Navigation Component, which helps implement navigation within the app. Reviewing Chapters 1 to 3 of the Android Basics in Kotlin course is also recommended for those new to Android development.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:15.415295+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What steps are necessary to enable USB debugging on the ERD board before updating its binary?",
    "reference_contexts": [
      "<1-hop>\n\n1. Connect the ERD board to your computer using a USB cable.\n1. In the pop-up that appears, select **Allow** to enable the USB debugging.\n1. Android Studio automatically detects the device. If the device is not detected, enable **File transfer** on the device.",
      "<2-hop>\n\n***<span style=\"color:red\">WARNING:</span>* Updating ERD board binary results in *erasing all data* on ERD board.**  \nTo update the binary of an ERD board (Windows):\n1. Download ERD board binary from [resources](https:/./soc-developer.semiconductor.samsung.com/support/resource).\n1. Extract the contents of the downloaded `zip` file.\n1. Enable USB debugging mode and connect the device as demonstrated [here](#configuring-the-erd-board).\n1. Boot ERD board to `fastboot` bootloader mode.\n```bash\nadb reboot bootloader\n```\n1. Check if ERD board is ready to flash by executing:\n```bash\nfastboot devices\n```\n1. From the extracted files, find and execute `ff_erd9925_all.exe`.\n1. Press any key to continue.\n1. After `ff_erd9925_all.exe` is executed, the ERD Board reboots automatically and the following message appears.\n```shell\n=======================================\nFINISHED\n2023/11/10 21:43:36\nSUCCESS 1/1 fastboot devices\n0) 00000a8fcf39b308, SUCCESS, elapsed=168s\n=======================================\n```"
    ],
    "reference": "To enable USB debugging on the ERD board before updating its binary, first connect the ERD board to your computer using a USB cable. In the pop-up that appears, select **Allow** to enable USB debugging. Additionally, ensure that the device is set to **File transfer** mode if it is not automatically detected by Android Studio.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:15.415295+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does the adb shell environment facilitate the development of native programs using the Exynos AI Studio?",
    "reference_contexts": [
      "This guide provides a detailed walkthrough for developing a native program using the Exynos Neural Network Software Development Kit (Exynos AI Studio).\nIt describes the steps for developing a native program that leverages the Exynos AI Studio to execute neural network models on Samsung Exynos hardware within the adb shell environment.\nThe guide aims at equipping developers with the necessary knowledge so that they can create native binaries for testing their models without having to create an Android application."
    ],
    "reference": "The adb shell environment facilitates the development of native programs by allowing developers to execute neural network models on Samsung Exynos hardware. This guide provides the necessary steps for creating native binaries for testing models without the need to create an Android application.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:31.420904+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the Exynos for Samsung Exynos?",
    "reference_contexts": [
      "This guide provides a detailed walkthrough for developing a native program using the Exynos Neural Network Software Development Kit (Exynos AI Studio).\nIt describes the steps for developing a native program that leverages the Exynos AI Studio to execute neural network models on Samsung Exynos hardware within the adb shell environment.\nThe guide aims at equipping developers with the necessary knowledge so that they can create native binaries for testing their models without having to create an Android application."
    ],
    "reference": "The Exynos Neural Network Software Development Kit (Exynos AI Studio) is used for developing native programs that execute neural network models on Samsung Exynos hardware.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:31.420904+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does the Exynos AI Studio contribute to the performance of neural network models on Samsung Exynos hardware?",
    "reference_contexts": [
      "The [Exynos AI Studio](https://soc-developer.semiconductor.samsung.com/enn-sdk) tool facilitates the conversion of [TFLite](https://www.tensorflow.org/lite) neural network models into NNC format models.\nThis conversion enables the NN models to execute efficiently on the [Samsung Exynos](https://semiconductor.samsung.com/processor/) hardware to ensure optimal performance.\nThe Exynos AI Studio provides a framework that facilitates the execution of NNC models on the Exynos platform."
    ],
    "reference": "The Exynos AI Studio facilitates the conversion of TFLite neural network models into NNC format models, enabling these models to execute efficiently on Samsung Exynos hardware, which ensures optimal performance. Additionally, it provides a framework that supports the execution of NNC models on the Exynos platform.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:31.420904+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What files are included in the enn-sdk-samples-9925 sample?",
    "reference_contexts": [
      "In this sample, a converted NNC file and raw input/output file available in the [Github Repository](https://github.com/exynos-eco/enn-sdk-samples-9925) are used."
    ],
    "reference": "The enn-sdk-samples-9925 sample includes a converted NNC file and a raw input/output file.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:31.420904+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What resources are available in the enn-sdk-samples-9925 sample?",
    "reference_contexts": [
      "In this sample, a converted NNC file and raw input/output file available in the [Github Repository](https://github.com/exynos-eco/enn-sdk-samples-9925) are used."
    ],
    "reference": "In the enn-sdk-samples-9925 sample, a converted NNC file and a raw input/output file are available in the Github Repository.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:31.420904+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do I run the nnc-modle-tester?",
    "reference_contexts": [
      "To execute the sample native program, refer to [README](https://github.com/exynos-eco/enn-sdk-samples-9925/tree/main/nnc-model-tester#readme) file."
    ],
    "reference": "To execute the sample native program, refer to [README](https://github.com/exynos-eco/enn-sdk-samples-9925/tree/main/nnc-model-tester#readme) file.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:31.420904+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do I exectute the sample native program using exynos-eco?",
    "reference_contexts": [
      "To execute the sample native program, refer to [README](https://github.com/exynos-eco/enn-sdk-samples-9925/tree/main/nnc-model-tester#readme) file."
    ],
    "reference": "To execute the sample native program, refer to the README file.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:31.420904+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the process of writing and executing a C++ program using the Exynos AI Studio and how does it relate to the conversion of TFLite models?",
    "reference_contexts": [
      "<1-hop>\n\nThe [Exynos AI Studio](https://soc-developer.semiconductor.samsung.com/enn-sdk) tool facilitates the conversion of [TFLite](https://www.tensorflow.org/lite) neural network models into NNC format models.\nThis conversion enables the NN models to execute efficiently on the [Samsung Exynos](https://semiconductor.samsung.com/processor/) hardware to ensure optimal performance.\nThe Exynos AI Studio provides a framework that facilitates the execution of NNC models on the Exynos platform.",
      "<2-hop>\n\nThis guide comprises the following sections:\n1. [**Writing Native Program**](getting-started-with-native-samples/writing-native-program): This section provides the process of writing a C++ program to implement the ENN framework.\n1. [**Compiling Using NDK**](getting-started-with-native-samples/compiling-using-ndk): This section provides the step-by-step process to compile the native program using NDK.\n1. [**Using ADB to Execute Native Program**](getting-started-with-native-samples/using-adb): This section explains the method to execute the native program using ADB.  \nThe general workflow of writing and executing a native program using the Exynos AI Studio is described in the following flowchart.  \n```mermaid\ngraph TB\n\nA[Start]\nB[Write Native Program]\nsubgraph C[Android Native Development Kit]\nC1[Create Makefile]-->C2[Compile Native Program]\nend\nsubgraph D[Android Debug Bridge]\nD1[Push Native Program and Data]--> D2[Execute Native Program]\nend\nE[End]\nA-->B-->C-->D-->E\n\n```"
    ],
    "reference": "The process of writing and executing a C++ program using the Exynos AI Studio involves several steps. First, developers write a native program that implements the ENN framework. This is followed by compiling the program using the Android Native Development Kit (NDK), which includes creating a Makefile and compiling the native program. After compilation, the Android Debug Bridge (ADB) is used to push the native program and its data to the device and execute it. The Exynos AI Studio facilitates the conversion of TFLite neural network models into NNC format models, allowing these models to run efficiently on Samsung Exynos hardware, thus ensuring optimal performance when integrated with the C++ programs developed using the studio.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:31.420904+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What resources are available in the exynos-eco repository for executing the sample native program, and how can one access the necessary files for this execution?",
    "reference_contexts": [
      "<1-hop>\n\nIn this sample, a converted NNC file and raw input/output file available in the [Github Repository](https://github.com/exynos-eco/enn-sdk-samples-9925) are used.",
      "<2-hop>\n\nTo execute the sample native program, refer to [README](https://github.com/exynos-eco/enn-sdk-samples-9925/tree/main/nnc-model-tester#readme) file."
    ],
    "reference": "In the exynos-eco repository, a converted NNC file and raw input/output file are available for use. To execute the sample native program, one should refer to the README file found in the repository, which provides detailed instructions on how to proceed.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:31.420904+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do you execute a native program on an ERD board?",
    "reference_contexts": [
      "This section provides the detailed information on using the ADB to execute the native program on the ERD board.\nThis process comprises of two main steps such as copying data to the board and executing the native program on the ERD board."
    ],
    "reference": "To execute a native program on the ERD board, you need to follow two main steps: first, copy data to the board, and then execute the native program.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:46.565378+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the two main steeps involved in using ADB to execute the native program on the ERD board?",
    "reference_contexts": [
      "This section provides the detailed information on using the ADB to execute the native program on the ERD board.\nThis process comprises of two main steps such as copying data to the board and executing the native program on the ERD board."
    ],
    "reference": "The two main steps involved in using ADB to execute the native program on the ERD board are copying data to the board and executing the native program on the ERD board.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:46.565378+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the main steps involved in using ADB to execute a native program on the ERD board?",
    "reference_contexts": [
      "This section provides the detailed information on using the ADB to execute the native program on the ERD board.\nThis process comprises of two main steps such as copying data to the board and executing the native program on the ERD board."
    ],
    "reference": "The main steps involved in using ADB to execute a native program on the ERD board are copying data to the board and executing the native program on the ERD board.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:46.565378+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What does the libenn_public_api_ndk_v1.so file do?",
    "reference_contexts": [
      "The following commands copy the necessary files to the ERD board:  \n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/* /data/local/tmp/\n```  \nThese commands perform the following actions:  \n- `adb push` is used to copy the model file (`model.nnc`), input data file (`input.bin`), golden data file (`golden.bin`), library file (`libenn_public_api_ndk_v1.so`), and the native program (`enn_nnc_model_tester`) to the `/data/vendor/enn/` directory on the ERD board."
    ],
    "reference": "The libenn_public_api_ndk_v1.so file is a library file that is copied to the ERD board using the adb push command, along with other necessary files for executing native programs.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:46.565378+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How is the model.nnc file transferred to the ERD board?",
    "reference_contexts": [
      "The following commands copy the necessary files to the ERD board:  \n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/* /data/local/tmp/\n```  \nThese commands perform the following actions:  \n- `adb push` is used to copy the model file (`model.nnc`), input data file (`input.bin`), golden data file (`golden.bin`), library file (`libenn_public_api_ndk_v1.so`), and the native program (`enn_nnc_model_tester`) to the `/data/vendor/enn/` directory on the ERD board."
    ],
    "reference": "The model.nnc file is transferred to the ERD board using the command `adb push`, which copies the model file along with other necessary files to the `/data/vendor/enn/` directory on the ERD board.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:46.565378+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain the process of copying files to the /data/vendor/enn/ directory on the ERD board?",
    "reference_contexts": [
      "The following commands copy the necessary files to the ERD board:  \n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/* /data/local/tmp/\n```  \nThese commands perform the following actions:  \n- `adb push` is used to copy the model file (`model.nnc`), input data file (`input.bin`), golden data file (`golden.bin`), library file (`libenn_public_api_ndk_v1.so`), and the native program (`enn_nnc_model_tester`) to the `/data/vendor/enn/` directory on the ERD board."
    ],
    "reference": "The process of copying files to the /data/vendor/enn/ directory on the ERD board involves using the `adb push` command. This command is utilized to transfer the model file (`model.nnc`), input data file (`input.bin`), golden data file (`golden.bin`), library file (`libenn_public_api_ndk_v1.so`), and the native program (`enn_nnc_model_tester`) to the specified directory.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:46.565378+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the steps to execute a native program on an ERD board after copying the necessary files?",
    "reference_contexts": [
      "After copying the necessary files to the ERD board, execute the native program using the following commands:  \n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```  \nThese commands perform the following actions:  \n- `adb shell` starts a shell session on the board.\n- `cd /data/local/tmp/` changes the current directory to `/data/local/tmp/`, where the necessary files are available.\n- The `LD_LIBRARY_PATH` environment variable sets the directory that contains `libenn_public_api_ndk_v1.so`.\n- The native program is executed with the `--model`, `--input`, `--golden`, and `--threshold` parameters, which specify the model file, input data file, golden data file, and threshold value, respectively."
    ],
    "reference": "To execute a native program on an ERD board after copying the necessary files, follow these steps: First, start a shell session on the board by using the command `adb shell`. Next, change the current directory to `/data/local/tmp/` with the command `cd /data/local/tmp/`, where the necessary files are located. Then, set the `LD_LIBRARY_PATH` environment variable to point to the directory containing `libenn_public_api_ndk_v1.so` by executing `export LD_LIBRARY_PATH=/data/local/tmp`. Finally, run the native program with the command `./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001`, specifying the model file, input data file, golden data file, and threshold value.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:46.565378+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do you use adb to execute a native program on an ERD board?",
    "reference_contexts": [
      "After copying the necessary files to the ERD board, execute the native program using the following commands:  \n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```  \nThese commands perform the following actions:  \n- `adb shell` starts a shell session on the board.\n- `cd /data/local/tmp/` changes the current directory to `/data/local/tmp/`, where the necessary files are available.\n- The `LD_LIBRARY_PATH` environment variable sets the directory that contains `libenn_public_api_ndk_v1.so`.\n- The native program is executed with the `--model`, `--input`, `--golden`, and `--threshold` parameters, which specify the model file, input data file, golden data file, and threshold value, respectively."
    ],
    "reference": "To execute a native program on an ERD board using adb, first start a shell session with the command `adb shell`. Then, change the current directory to `/data/local/tmp/` using `cd /data/local/tmp/`. Next, set the `LD_LIBRARY_PATH` environment variable to point to the directory containing `libenn_public_api_ndk_v1.so` with the command `export LD_LIBRARY_PATH=/data/local/tmp`. Finally, run the native program with the command `./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001`, specifying the model file, input data file, golden data file, and threshold value.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:46.565378+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Could you elaborate on the role of adb in executing native programs on the ERD board?",
    "reference_contexts": [
      "After copying the necessary files to the ERD board, execute the native program using the following commands:  \n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```  \nThese commands perform the following actions:  \n- `adb shell` starts a shell session on the board.\n- `cd /data/local/tmp/` changes the current directory to `/data/local/tmp/`, where the necessary files are available.\n- The `LD_LIBRARY_PATH` environment variable sets the directory that contains `libenn_public_api_ndk_v1.so`.\n- The native program is executed with the `--model`, `--input`, `--golden`, and `--threshold` parameters, which specify the model file, input data file, golden data file, and threshold value, respectively."
    ],
    "reference": "The adb (Android Debug Bridge) is utilized to start a shell session on the ERD board, allowing the execution of native programs. After copying the necessary files to the board, the command `adb shell` initiates this session. Following that, the command `cd /data/local/tmp/` changes the directory to where the required files are located. The `LD_LIBRARY_PATH` environment variable is then set to include the directory containing the necessary library, `libenn_public_api_ndk_v1.so`. Finally, the native program is executed with specific parameters that define the model file, input data file, golden data file, and threshold value.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:46.565378+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the ADB commands required to copy files to the ERD board, and how do you execute the native program after copying?",
    "reference_contexts": [
      "<1-hop>\n\nThe following commands copy the necessary files to the ERD board:  \n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/* /data/local/tmp/\n```  \nThese commands perform the following actions:  \n- `adb push` is used to copy the model file (`model.nnc`), input data file (`input.bin`), golden data file (`golden.bin`), library file (`libenn_public_api_ndk_v1.so`), and the native program (`enn_nnc_model_tester`) to the `/data/vendor/enn/` directory on the ERD board.",
      "<2-hop>\n\nAfter copying the necessary files to the ERD board, execute the native program using the following commands:  \n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```  \nThese commands perform the following actions:  \n- `adb shell` starts a shell session on the board.\n- `cd /data/local/tmp/` changes the current directory to `/data/local/tmp/`, where the necessary files are available.\n- The `LD_LIBRARY_PATH` environment variable sets the directory that contains `libenn_public_api_ndk_v1.so`.\n- The native program is executed with the `--model`, `--input`, `--golden`, and `--threshold` parameters, which specify the model file, input data file, golden data file, and threshold value, respectively."
    ],
    "reference": "To copy files to the ERD board, the following ADB commands are used: `adb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/`, `adb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/`, and `adb push example/* /data/local/tmp/`. After copying the necessary files, you can execute the native program using the commands: `adb shell`, `cd /data/local/tmp/`, and then `export LD_LIBRARY_PATH=/data/local/tmp` followed by `./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001`. This sequence starts a shell session, changes the directory to where the files are located, sets the library path, and runs the native program with the specified parameters.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:46.565378+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the steps involved in using adb to execute the native program on the ERD board?",
    "reference_contexts": [
      "<1-hop>\n\nThe following commands copy the necessary files to the ERD board:  \n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/* /data/local/tmp/\n```  \nThese commands perform the following actions:  \n- `adb push` is used to copy the model file (`model.nnc`), input data file (`input.bin`), golden data file (`golden.bin`), library file (`libenn_public_api_ndk_v1.so`), and the native program (`enn_nnc_model_tester`) to the `/data/vendor/enn/` directory on the ERD board.",
      "<2-hop>\n\nThis section provides the detailed information on using the ADB to execute the native program on the ERD board.\nThis process comprises of two main steps such as copying data to the board and executing the native program on the ERD board."
    ],
    "reference": "The steps involved in using adb to execute the native program on the ERD board include first copying the necessary files to the board using commands like `adb push`, which transfers the model file, input data file, golden data file, library file, and the native program to the appropriate directory. The second step is executing the native program on the ERD board after the files have been successfully copied.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:03:46.565378+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do I download the ENN frmaework?",
    "reference_contexts": [
      "Download the ENN framework library (ENN Public API NDK) from [resources](https://soc-developer.semiconductor.samsung.com/support/resource).\nNext, copy the necessary libraries by perform the following steps:  \n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/jni/lib64``\n2. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/jni/include`\n3. Add following code in `en_nnc_model_tester.cpp` to load ENN framework.\n([Example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L13)):\n```cpp\n#include \"include/enn_api-public_ndk_v1.hpp\"\n```"
    ],
    "reference": "To download the ENN framework library (ENN Public API NDK), you can access it from the provided resources link.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\writing-native-program.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:04:10.686936+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Could you elaborate on the importance of documentation in the context of executing neural network models on the ENN framework?",
    "reference_contexts": [
      "This section describes the steps for executing NN models on the ENN framework.\nFor more information on the ENN framework, refer to the [documentation](developer-guide#4-enn-framework-api)."
    ],
    "reference": "Documentation is crucial as it provides detailed steps for executing neural network models on the ENN framework. For further information regarding the ENN framework, one can refer to the specific documentation linked in the context.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\writing-native-program.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:04:10.686936+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What parameters are necessary for executing NNC models using the ENN framework, and what are their data types?",
    "reference_contexts": [
      "To execute NNC models using the ENN framework, the following parameters are required:  \n|Parameter|Data Type|Explanation|\n|--|--|--|\n|`model_name`|string|Path to the ML model file|\n|`inputs`|vector&lt;string&gt;|List of input file paths|\n|`goldens` (optional)|vector&lt;string&gt;|List of golden file paths for validation|\n|`threshold` (optional)|float|Threshold for golden matching, used to determine the acceptable deviation|"
    ],
    "reference": "To execute NNC models using the ENN framework, the following parameters are required: `model_name` (string) which is the path to the ML model file, `inputs` (vector<string>) which is a list of input file paths, and `goldens` (optional, vector<string>) which is a list of golden file paths for validation. Additionally, there is a `threshold` (optional, float) which is used for golden matching to determine the acceptable deviation.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\writing-native-program.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:04:10.686936+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do you use ENN framework for NN models?",
    "reference_contexts": [
      "Executing NN models on the ENN framework comprises of three steps such as initializing framework, inferring the models, and deinitializing the framework.  \nThe following chart describes the lifecycle and process of inferring NN models using the ENN framework.  \n```mermaid\ngraph TB\nsubgraph A[Initialize ENN Framework]\nA1[Initialize]\nA1 --> A2[Open Model]\nA2 --> A3[Allocate/Commit Buffers]\nend\nsubgraph B[Inference]\nB1[Copy Input Layer]\nB1 --> B2[Execute Model]\nB2 --> B3[Copy Output Layer]\nend\nsubgraph C[Deinitialize]\nC1[Release Buffers]\nC1 --> C2[Close Model]\nC2 --> C3[Deinitialize]\nend\nA --> B\nB --> C\n```  \nTo infer multiple data, repeat `Inference`."
    ],
    "reference": "Using the ENN framework for NN models involves three steps: initializing the framework, inferring the models, and deinitializing the framework.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\writing-native-program.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:04:10.686936+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How EnnInitialize work in ENN framework?",
    "reference_contexts": [
      "Before executing ML models on the ENN framework, initialize the framework, load the model, and allocate the necessary buffers.  \n1. [EnnInitialize](api-reference/enn-framework-api-functions#function-enninitialize):\nThis function initializes the ENN Framework.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L48)):\n```cpp\nenn::api::EnnInitialize();\n```  \n1. [EnnOpenModel](api-reference/enn-framework-api-functions#function-ennopenmodel):\nThis function opens the specified model and return a model ID.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L56)):\n```cpp\nEnnModelId model_id;\nenn::api::EnnOpenModel(model_name.c_str(), &model_id);\n```  \n1. [EnnAllocateAllBuffers](api-reference/enn-framework-api-functions#function-ennallocateallbuffers):\nThis function allocates all the necessary buffers for the model.\nIt also provides the number of input/output buffers (`buffer_info`), their locations, and sizes (`buffer_set`).  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L68)):\n```cpp\nEnnBufferPtr *buffer_set;\nNumberOfBuffersInfo buffer_info;\nenn::api::EnnAllocateAllBuffers(model_id, &buffer_set, &buffer_info);\n```  \nFollowing is the data structure of `EnnBufferPtr` and `NumberOfBuffersInfo`.  \n```cpp\ntypedef struct _ennBuffer {\nvoid *va;\nuint32_t size;\nuint32_t offset;\n} EnnBuffer;\n\ntypedef EnnBuffer* EnnBufferPtr;\n\ntypedef struct _NumberOfBuffersInfo {\nuint32_t n_in_buf;\nuint32_t n_out_buf;\n} NumberOfBuffersInfo;\n```"
    ],
    "reference": "EnnInitialize function initializes the ENN Framework before executing ML models. It is called using the syntax `enn::api::EnnInitialize();`.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\writing-native-program.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:04:10.686936+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the function of EnnDeinitialize in the ENN framework?",
    "reference_contexts": [
      "After executing the model, deinitialize the framework to release resources.  \n1. [EnnReleaseBuffers](api-reference/enn-framework-api-functions#function-ennreleasebuffers):\nThis function releases the allocated buffers.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L112)):\n```cpp\nenn::api::EnnReleaseBuffers(buffer_set, buffer_info.n_in_buf + buffer_info.n_out_buf)\n```  \n1. [EnnCloseModel](api-reference/enn-framework-api-functions#function-ennclosemodel):\nThis function closes the specified model.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L118)):\n```cpp\nenn::api::EnnCloseModel(model_id)\n```  \n1. [EnnDeinitialize](api-reference/enn-framework-api-functions#function-enndeinitialize):\nThis function deinitializes the ENN framework.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L124)):\n```cpp\nenn::api::EnnDeinitialize()\n```"
    ],
    "reference": "The EnnDeinitialize function deinitializes the ENN framework, which is essential for releasing resources after executing the model.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\writing-native-program.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:04:10.686936+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the steps involved in the execution phase of NN models on the ENN framework?",
    "reference_contexts": [
      "<1-hop>\n\nThe execution phase of model involves copying the input layer, executing the model, and copying the output layer.\nFor more information on copying data to the input layer and comparing data with the output layer, refer to Section [Processing Input and Outputs section](#processing-input-and-outputs).  \n1. [EnnExecuteModel](api-reference/enn-framework-api-functions#function-ennexecutemodel):\nThis function executes the model using the specified model ID.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L90)):\n```cpp\nenn::api::EnnExecuteModel(model_id)\n```",
      "<2-hop>\n\nExecuting NN models on the ENN framework comprises of three steps such as initializing framework, inferring the models, and deinitializing the framework.  \nThe following chart describes the lifecycle and process of inferring NN models using the ENN framework.  \n```mermaid\ngraph TB\nsubgraph A[Initialize ENN Framework]\nA1[Initialize]\nA1 --> A2[Open Model]\nA2 --> A3[Allocate/Commit Buffers]\nend\nsubgraph B[Inference]\nB1[Copy Input Layer]\nB1 --> B2[Execute Model]\nB2 --> B3[Copy Output Layer]\nend\nsubgraph C[Deinitialize]\nC1[Release Buffers]\nC1 --> C2[Close Model]\nC2 --> C3[Deinitialize]\nend\nA --> B\nB --> C\n```  \nTo infer multiple data, repeat `Inference`."
    ],
    "reference": "The execution phase of NN models on the ENN framework involves copying the input layer, executing the model, and copying the output layer. This process is part of a larger lifecycle that includes initializing the framework, inferring the models, and deinitializing the framework. Specifically, the inference step consists of copying the input layer, executing the model, and then copying the output layer.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\writing-native-program.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:04:10.686936+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the steps to deinitialize the ENN framework after executing NNC models, and what parameters are required to execute these models?",
    "reference_contexts": [
      "<1-hop>\n\nTo execute NNC models using the ENN framework, the following parameters are required:  \n|Parameter|Data Type|Explanation|\n|--|--|--|\n|`model_name`|string|Path to the ML model file|\n|`inputs`|vector&lt;string&gt;|List of input file paths|\n|`goldens` (optional)|vector&lt;string&gt;|List of golden file paths for validation|\n|`threshold` (optional)|float|Threshold for golden matching, used to determine the acceptable deviation|",
      "<2-hop>\n\nAfter executing the model, deinitialize the framework to release resources.  \n1. [EnnReleaseBuffers](api-reference/enn-framework-api-functions#function-ennreleasebuffers):\nThis function releases the allocated buffers.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L112)):\n```cpp\nenn::api::EnnReleaseBuffers(buffer_set, buffer_info.n_in_buf + buffer_info.n_out_buf)\n```  \n1. [EnnCloseModel](api-reference/enn-framework-api-functions#function-ennclosemodel):\nThis function closes the specified model.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L118)):\n```cpp\nenn::api::EnnCloseModel(model_id)\n```  \n1. [EnnDeinitialize](api-reference/enn-framework-api-functions#function-enndeinitialize):\nThis function deinitializes the ENN framework.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L124)):\n```cpp\nenn::api::EnnDeinitialize()\n```"
    ],
    "reference": "To execute NNC models using the ENN framework, the required parameters include `model_name` (a string representing the path to the ML model file), `inputs` (a vector of strings for the list of input file paths), and optionally, `goldens` (a vector of strings for the list of golden file paths for validation) and `threshold` (a float for the threshold of golden matching). After executing the model, the steps to deinitialize the ENN framework are as follows: first, release the allocated buffers using the function `EnnReleaseBuffers`, then close the specified model with `EnnCloseModel`, and finally, deinitialize the framework using `EnnDeinitialize`.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\writing-native-program.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:04:10.686936+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is Andriod Jetpack?",
    "reference_contexts": [
      "The CameraX library, an integral component of Android Jetpack, has been designed to simplify camera operations across various Android devices. It offers a consistent and intuitive API, ensuring developers face minimal challenges when incorporating camera functionalities. This documentation delves deeper into the integration process: from initializing the hardware to intricately binding the camera's lifecycle."
    ],
    "reference": "Android Jetpack includes the CameraX library, which simplifies camera operations across various Android devices and offers a consistent and intuitive API for developers.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\other\\WorkingWithTheCamera.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:04:33.831249+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the Android Studio?",
    "reference_contexts": [
      "1. Proficiency with Android application development.\n2. A configured Android Studio environment.\n3. CameraX library integrated into your project's Gradle dependencies."
    ],
    "reference": "Android Studio is a configured environment necessary for Android application development.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\other\\WorkingWithTheCamera.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:04:33.831249+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the role of ProcessCameraProvider in accessing camera hardware?",
    "reference_contexts": [
      "**Objective**: Understand the necessary steps to access and initialize the camera hardware in your application.  \n**Class Utilized**: `ProcessCameraProvider` - A class that manages camera devices' lifecycle. It acts as a bridge between your application and the camera hardware."
    ],
    "reference": "ProcessCameraProvider is a class that manages the lifecycle of camera devices and acts as a bridge between your application and the camera hardware.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\other\\WorkingWithTheCamera.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:04:33.831249+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does ProcessCameraProvider facilitate camera management in Android applications?",
    "reference_contexts": [
      "```kotlin\nprivate var preview: Preview? = null\nprivate var camera: Camera? = null\n\nprivate fun setCamera() {\ncameraExecutor = Executors.newSingleThreadExecutor()\nval cameraProviderFuture = ProcessCameraProvider.getInstance(requireContext())\n\ncameraProviderFuture.addListener(\n{\nval cameraProvider = cameraProviderFuture.get()\nval cameraSelector = CameraSelector.DEFAULT_BACK_CAMERA\nsetPreview()\n\ntry {\ncameraProvider.unbindAll()\ncamera = cameraProvider.bindToLifecycle(this, cameraSelector, preview)\npreview?.setSurfaceProvider(binding.viewFinder.surfaceProvider)\n} catch (exc: java.lang.Exception) {\nLog.e(TAG, \"Camera binding failed\", exc)\n}\n},\nContextCompat.getMainExecutor(requireContext())\n)\n}\n```\n**Annotations**:\n- `cameraExecutor`: A dedicated executor ensuring that camera operations run on a separate background thread, avoiding any UI slowdowns.\n- `cameraProviderFuture`: An asynchronous task to fetch an instance of `ProcessCameraProvider`. This instance helps control and manage camera devices.\n- `cameraSelector`: Utilized to specify preferences for camera initialization. Here, we select the device's default back camera."
    ],
    "reference": "ProcessCameraProvider is an asynchronous task that fetches an instance to control and manage camera devices. It allows developers to bind the camera to the lifecycle of the application, ensuring that camera operations are handled efficiently and without UI slowdowns.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\other\\WorkingWithTheCamera.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:04:33.831249+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the role of the Preview class in Android development?",
    "reference_contexts": [
      "**Objective**: Offer a real-time view of the camera feed to users, enabling them to see what the camera lens captures.  \n**Class Utilized**: `Preview` - A class designed to handle and showcase the real-time camera feed."
    ],
    "reference": "The Preview class is designed to handle and showcase the real-time camera feed, offering users a view of what the camera lens captures.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\other\\WorkingWithTheCamera.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:04:33.831249+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What does Preview.Builder do?",
    "reference_contexts": [
      "```xml\n<androidx.constraintlayout.widget.ConstraintLayout>\n...\n<androidx.camera.view.PreviewView\nandroid:id=\"@+id/viewFinder\"\nandroid:layout_width=\"match_parent\"\nandroid:layout_height=\"match_parent\" />\n...\n</androidx.constraintlayout.widget.ConstraintLayout>\n```  \n```kotlin\nprivate fun setPreview() {\npreview = Preview.Builder()\n.setTargetAspectRatio(AspectRatio.RATIO_4_3)\n.setTargetRotation(binding.viewFinder.display.rotation)\n.build()\n}\n```  \n**Annotations**:\n- Declare the component that will output the preview image, such as \"PreviewView\".\n- The `Preview.Builder` facilitates the configuration of the live feed properties.\n- `setTargetAspectRatio(AspectRatio.RATIO_4_3)`: Configures the aspect ratio of the camera preview. Here, a 4:3 ratio is selected.\n- `setTargetRotation()`: Aligns the camera feed's rotation with the device's current display rotation, ensuring the feed orientation is consistent with user expectations."
    ],
    "reference": "The Preview.Builder facilitates the configuration of the live feed properties for the camera preview.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\other\\WorkingWithTheCamera.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:04:33.831249+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the objective of synchronizing the camera's operations with the Fragment's lifecycle in Android development?",
    "reference_contexts": [
      "**Objective**: To harmonize the camera's operations with the Fragment's lifecycle. Such synchronization guarantees the camera's optimal performance, ensuring that it doesn't run unnecessarily, conserving both CPU cycles and battery life.  \n**Methods Utilized**:\n- `unbindAll()`: Clears any previous bindings, ensuring a fresh slate for binding. This step is crucial to prevent potential conflicts.\n- `bindToLifecycle()`: Merges the camera's lifecycle with the fragment's lifecycle."
    ],
    "reference": "The objective is to harmonize the camera's operations with the Fragment's lifecycle, which guarantees the camera's optimal performance. This synchronization ensures that the camera does not run unnecessarily, thereby conserving both CPU cycles and battery life.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\other\\WorkingWithTheCamera.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:04:33.831249+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain what happens when an ImageCaptureException occurs during the photo capture process in Android development?",
    "reference_contexts": [
      "```kotlin\nprivate fun takePhoto() {\nval imageCapture = imageCapture ?: return\n\n...\n\nval contentValues = ContentValues().apply {\nput(MediaStore.MediaColumns.DISPLAY_NAME, name)\nput(MediaStore.MediaColumns.MIME_TYPE, \"image/jpeg\")\nif(Build.VERSION.SDK_INT > Build.VERSION_CODES.P) {\nput(MediaStore.Images.Media.RELATIVE_PATH, \"Pictures/CameraX-Image\")\n}\n}\n\nval outputOptions = ImageCapture.OutputFileOptions\n.Builder(contentResolver, MediaStore.Images.Media.EXTERNAL_CONTENT_URI, contentValues)\n.build()\n\nimageCapture.takePicture(\noutputOptions,\nContextCompat.getMainExecutor(this),\nobject : ImageCapture.OnImageSavedCallback {\noverride fun onImageSaved(output: ImageCapture.OutputFileResults){\n<Success Capture to run for your code>\n}\noverride fun onError(exc: ImageCaptureException) {\n<Fail Capture to run for your code>\n}\n}\n)\n}\n```\n**Annotations**:\n- This allows you to capture an image and save it to your device.\n- `outputOptions`: You can specify how you want the output to appear. Additionally, you can set the save name, MIMETYPE, and more in `ContentValues`.\n- When the image capture function finishes executing, \"OnImageSavedCallback\" is executed based on the result.\n- `onImageSaved()`: Function executed on successful capture.\n- `onError()`: Function executed on failed capture."
    ],
    "reference": "When an ImageCaptureException occurs during the photo capture process, the onError() function is executed, which allows you to handle the failure of the image capture.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\other\\WorkingWithTheCamera.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:04:33.831249+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does the use of ConstraintLayout facilitate camera binding in an Android application?",
    "reference_contexts": [
      "<1-hop>\n\n```xml\n<androidx.constraintlayout.widget.ConstraintLayout>\n...\n<androidx.camera.view.PreviewView\nandroid:id=\"@+id/viewFinder\"\nandroid:layout_width=\"match_parent\"\nandroid:layout_height=\"match_parent\" />\n...\n</androidx.constraintlayout.widget.ConstraintLayout>\n```  \n```kotlin\nprivate fun setPreview() {\npreview = Preview.Builder()\n.setTargetAspectRatio(AspectRatio.RATIO_4_3)\n.setTargetRotation(binding.viewFinder.display.rotation)\n.build()\n}\n```  \n**Annotations**:\n- Declare the component that will output the preview image, such as \"PreviewView\".\n- The `Preview.Builder` facilitates the configuration of the live feed properties.\n- `setTargetAspectRatio(AspectRatio.RATIO_4_3)`: Configures the aspect ratio of the camera preview. Here, a 4:3 ratio is selected.\n- `setTargetRotation()`: Aligns the camera feed's rotation with the device's current display rotation, ensuring the feed orientation is consistent with user expectations.",
      "<2-hop>\n\n```kotlin\ncameraProviderFuture.addListener(\n{\n...\ntry {\ncameraProvider.unbindAll()\ncamera = cameraProvider.bindToLifecycle(this, cameraSelector, preview)\npreview?.setSurfaceProvider(binding.viewFinder.surfaceProvider)\n} catch (exc: java.lang.Exception) {\nLog.e(TAG, \"Camera binding failed\", exc)\n}\n},\nContextCompat.getMainExecutor(requireContext())\n)\n```\n**Annotations**:\n- This binding approach ensures that the camera activates or deactivates in conjunction with the Fragment's lifecycle events. For instance, the camera initiates when the Fragment starts and ceases when the Fragment stops."
    ],
    "reference": "The use of ConstraintLayout facilitates camera binding in an Android application by providing a flexible layout structure for the PreviewView component, which outputs the camera preview image. The PreviewView is defined within the ConstraintLayout, allowing it to match the parent's dimensions. This setup ensures that the camera feed can be properly displayed and managed in conjunction with the Fragment's lifecycle events, as the camera is bound to the lifecycle and the surface provider is set to the PreviewView.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\other\\WorkingWithTheCamera.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:04:33.831249+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the necessary steps to initialize the camera hardware using ProcessCameraProvider in an Android application?",
    "reference_contexts": [
      "<1-hop>\n\n**Objective**: Understand the necessary steps to access and initialize the camera hardware in your application.  \n**Class Utilized**: `ProcessCameraProvider` - A class that manages camera devices' lifecycle. It acts as a bridge between your application and the camera hardware.",
      "<2-hop>\n\n```kotlin\nprivate var preview: Preview? = null\nprivate var camera: Camera? = null\n\nprivate fun setCamera() {\ncameraExecutor = Executors.newSingleThreadExecutor()\nval cameraProviderFuture = ProcessCameraProvider.getInstance(requireContext())\n\ncameraProviderFuture.addListener(\n{\nval cameraProvider = cameraProviderFuture.get()\nval cameraSelector = CameraSelector.DEFAULT_BACK_CAMERA\nsetPreview()\n\ntry {\ncameraProvider.unbindAll()\ncamera = cameraProvider.bindToLifecycle(this, cameraSelector, preview)\npreview?.setSurfaceProvider(binding.viewFinder.surfaceProvider)\n} catch (exc: java.lang.Exception) {\nLog.e(TAG, \"Camera binding failed\", exc)\n}\n},\nContextCompat.getMainExecutor(requireContext())\n)\n}\n```\n**Annotations**:\n- `cameraExecutor`: A dedicated executor ensuring that camera operations run on a separate background thread, avoiding any UI slowdowns.\n- `cameraProviderFuture`: An asynchronous task to fetch an instance of `ProcessCameraProvider`. This instance helps control and manage camera devices.\n- `cameraSelector`: Utilized to specify preferences for camera initialization. Here, we select the device's default back camera."
    ],
    "reference": "To initialize the camera hardware using ProcessCameraProvider in an Android application, you need to follow these steps: First, create a dedicated executor for camera operations to ensure they run on a separate background thread. Then, obtain an instance of ProcessCameraProvider by calling `ProcessCameraProvider.getInstance(requireContext())`. After that, add a listener to the cameraProviderFuture to get the cameraProvider instance. Specify the cameraSelector to choose the default back camera. Finally, unbind all previous camera use cases and bind the camera to the lifecycle of your activity or fragment, setting the surface provider for the preview.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\other\\WorkingWithTheCamera.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:04:33.831249+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What does the Exynos Neural Network Software Development Kit help with?",
    "reference_contexts": [
      "This guide provides basic instructions for using Exynos Neural Network Software Development Kit (Exynos AI Studio).\nThis guide explains the method to convert Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also describes the execution of NNC models on Exynos devices."
    ],
    "reference": "The Exynos Neural Network Software Development Kit provides instructions for converting Neural Network models to Neural Network Container models and describes the execution of these models on Exynos devices.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:05:06.933965+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the role of the ENN framework in Exynos AI Studio?",
    "reference_contexts": [
      "[Exynos AI Studio](https://soc-developer.semiconductor.samsung.com/enn-sdk) allows users to convert the trained [TensorFlow Lite](https://www.tensorflow.org/lite) neural network models to a format that can run efficiently in [Samsung Exynos](https://semiconductor.samsung.com/processor/) hardware.\nExynos AI Studio contains ENNTools to convert trained NN models and ENN framework for executing converted models on Exynos platforms.  \nThis guide covers the basics of using [Exynos AI Studio service](https://soc-developer.semiconductor.samsung.com/enn-sdk/project/) and executing NN models with ENN framework."
    ],
    "reference": "The ENN framework is used for executing converted neural network models on Exynos platforms within the Exynos AI Studio.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:05:06.933965+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is TFLite used for in NN model processing?",
    "reference_contexts": [
      "Following figure illustrates the three steps for converting and executing an NN model:  \n```mermaid\nflowchart LR\nsubgraph \"Exynos AI Studio Service\"\ndirection LR\nconvert(\"Convert The Model\")\nend\nsubgraph \"ENN Framework\"\ndirection LR\nexecute(\"Execute The Model\")\nend\nmodel(\"Prepare Trained Model<br>(TFLite)\")-->convert-->execute\n```"
    ],
    "reference": "TFLite is used to prepare a trained model before it is converted and executed in the process of converting and executing an NN model.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:05:06.933965+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does the Samsung Exynos Eco-system facilitate the conversion of TensorFlow Lite models?",
    "reference_contexts": [
      "To convert TensorFlow Lite models, Exynos AI Studio provides an online conversion tool through the Samsung Exynos Eco-system [Portal](https://soc-developer.semiconductor.samsung.com/enn-sdk).\nThis online conversion tool allows users to upload their TFLite models, convert them to NNC models, and download the NNC models to their devices.  \nFor more information on the process of converting NN models, refer to [Converting NN Models with Exynos AI Studio Service](#converting-nn-models-with-enn-sdk-service)."
    ],
    "reference": "The Samsung Exynos Eco-system facilitates the conversion of TensorFlow Lite models through Exynos AI Studio, which provides an online conversion tool. This tool allows users to upload their TFLite models, convert them to NNC models, and download the NNC models to their devices.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:05:06.933965+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How can users implement a program with the ENN framework to execute NPU models on Exynos platforms, and what are the necessary steps involved?",
    "reference_contexts": [
      "To execute NNC models on Exynos platforms, users must implement a program with ENN framework.\nENN framework provides C++ APIs for utilizing the framework that accelerate graph-based NN applications using NPU/DSP.\nThe Exynos AI Studio provides only C++ APIs.\nTherefore, the user must implement the Java Native Interface (JNI) layer to use the ENN framework on Android applications.  \nFor more information on the process of executing NN models, refer to [Executing Models Using Native Program](#executing-models-using-native-program) and [Executing Models Using Android Application](#executing-models-using-android-application)."
    ],
    "reference": "To execute NNC models on Exynos platforms, users must implement a program with the ENN framework, which provides C++ APIs for utilizing the framework that accelerates graph-based NN applications using NPU/DSP. Since the Exynos AI Studio provides only C++ APIs, users must implement the Java Native Interface (JNI) layer to use the ENN framework on Android applications.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:05:06.933965+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What GPU do in model conversion?",
    "reference_contexts": [
      "In this example, let us consider converting a trained TFLite MobileNet V1 model from TensorFlow Hub.  \nTo get started, you must be a member of the ENN Eco-system [Portal](https://soc-developer.semiconductor.samsung.com/).\n- If you are not a member of the ENN Eco-system Portal, sign up from [here](https://soc-developer.semiconductor.samsung.com/register).\n- If you already have an account, log in to the ENN Eco-system Portal.  \nTo convert MobileNet V1 model:\n1. Download `lite-model_mobilenet_v1_100_224_uint8_1.tflite` from [here](https://tfhub.dev/iree/lite-model/mobilenet_v1_100_224/uint8/1).\n1. Navigate to the Exynos AI Studio Service [page](https://soc-developer.semiconductor.samsung.com/enn-sdk/project) and provide a title for your project.\n1. Then, upload the downloaded TFLite model\n1. Next, select hardware preferences.\n- The **Default** option creates a model that utilizes only the CPU and GPU for conversion.\n- The **Accelerate** option creates a model that utilizes NPU as an accelerator with CPU and GPU.\n1. After confirming your selections, click **Convert** to convert the model.\n1. After the compilation process is successfully completed, the **NNC Download** button is enabled.\n1. Click **NNC Download** to download the converted NNC model.\nYou can now integrate the NNC model into the desired application."
    ],
    "reference": "The GPU is utilized in the conversion process when the Default option is selected, which creates a model that utilizes only the CPU and GPU.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:05:06.933965+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How can I execute a sample NNC model using the enn-sdk-samples-9925 on an ERD board?",
    "reference_contexts": [
      "For this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the [Github](https://github.com/exynos-eco/enn-sdk-samples-9925).  \n1. Download the samples and navigate to the directory that contain the sample program.\n```shell\ngit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\ncd enn-sdk-samples-9925/nnc-model-tester\n```  \n1. Push the necessary files (native program and test files) using adb push.\n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/* /data/local/tmp/\n```  \n1. When nnc_model_tester is built from Windows, execute permission must be provided.\n```shell\nadb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n```  \n1. Execute native binary on ERD board using adb shell.\n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```  \n1. When the command is successful, the following message is displayed:\n```shell\nLoaded Model:\nmodel.nnc(00000B7E01000000)\nModel Execution Time (1): 5413 microseconds\nAvg. Model Execution Time: 5413 microseconds\nOutput Layer(0): Golden Match\n-       snr value:104.802\nENN Framework Execute Model Sucess\n```  \nFor more information on this sample program, refer to [Exynos AI Studio Samples](enn-sdk-samples).\nFor more information on writing native programs using ENN framework, refer to [Getting Started With Native Samples](getting-started-with-native-samples)."
    ],
    "reference": "To execute a sample NNC model using the enn-sdk-samples-9925 on an ERD board, follow these steps: First, download the samples and navigate to the directory containing the sample program by running the commands: `git clone https://github.com/exynos-eco/enn-sdk-samples-9925.git` and `cd enn-sdk-samples-9925/nnc-model-tester`. Next, push the necessary files (native program and test files) using adb push with the commands: `adb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/`, `adb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/`, and `adb push example/* /data/local/tmp/`. If the nnc_model_tester is built from Windows, ensure to provide execute permission with `adb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"`. Finally, execute the native binary on the ERD board using adb shell with the commands: `adb shell`, `cd /data/local/tmp/`, `export LD_LIBRARY_PATH=/data/local/tmp`, and `./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001`. Upon successful execution, you will see a message indicating the model has been loaded and executed successfully.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:05:06.933965+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How can I run an Android application on the ERD board?",
    "reference_contexts": [
      "For this example, we will execute an Android application using Android Studio on the ERD board.\nThis example allows users to execute an image classification Android application.  \nThe source code for the example is available at `enn-sdk-samples-9925/image-classification` within the [Github](https://github.com/exynos-eco/enn-sdk-samples-9925).\n1. Download the samples by cloning the Github repository\n```shell\ngit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n```\n2. Open the downloaded **image-classification** project in Android Studio.\n3. Connect the ERD board and click \"run 'app'\".\nThe application is launched on the ERD board after the build.  \nFor more information, refer to [Getting Started With Android Samples](getting-started-with-android-samples)."
    ],
    "reference": "To run an Android application on the ERD board, you need to execute the following steps: First, download the samples by cloning the Github repository using the command `git clone https://github.com/exynos-eco/enn-sdk-samples-9925.git`. Then, open the downloaded **image-classification** project in Android Studio. After connecting the ERD board, click 'run 'app'' to launch the application on the ERD board after the build.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:05:06.933965+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the steps to execute an NNC model and an Android application on the ERD board, and how do they relate to the integration of hardware and software?",
    "reference_contexts": [
      "<1-hop>\n\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the [Github](https://github.com/exynos-eco/enn-sdk-samples-9925).  \n1. Download the samples and navigate to the directory that contain the sample program.\n```shell\ngit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\ncd enn-sdk-samples-9925/nnc-model-tester\n```  \n1. Push the necessary files (native program and test files) using adb push.\n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/* /data/local/tmp/\n```  \n1. When nnc_model_tester is built from Windows, execute permission must be provided.\n```shell\nadb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n```  \n1. Execute native binary on ERD board using adb shell.\n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```  \n1. When the command is successful, the following message is displayed:\n```shell\nLoaded Model:\nmodel.nnc(00000B7E01000000)\nModel Execution Time (1): 5413 microseconds\nAvg. Model Execution Time: 5413 microseconds\nOutput Layer(0): Golden Match\n-       snr value:104.802\nENN Framework Execute Model Sucess\n```  \nFor more information on this sample program, refer to [Exynos AI Studio Samples](enn-sdk-samples).\nFor more information on writing native programs using ENN framework, refer to [Getting Started With Native Samples](getting-started-with-native-samples).",
      "<2-hop>\n\nFor this example, we will execute an Android application using Android Studio on the ERD board.\nThis example allows users to execute an image classification Android application.  \nThe source code for the example is available at `enn-sdk-samples-9925/image-classification` within the [Github](https://github.com/exynos-eco/enn-sdk-samples-9925).\n1. Download the samples by cloning the Github repository\n```shell\ngit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n```\n2. Open the downloaded **image-classification** project in Android Studio.\n3. Connect the ERD board and click \"run 'app'\".\nThe application is launched on the ERD board after the build.  \nFor more information, refer to [Getting Started With Android Samples](getting-started-with-android-samples)."
    ],
    "reference": "To execute an NNC model on the ERD board, follow these steps: First, download the sample NNC model program from the GitHub repository by cloning it with the command `git clone https://github.com/exynos-eco/enn-sdk-samples-9925.git` and navigate to the `nnc-model-tester` directory. Next, push the necessary files to the device using adb commands. After ensuring the nnc_model_tester has execute permissions, run the binary on the ERD board with the command `./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001`. Upon successful execution, you will see the model execution time and confirmation of the output layer match.\n\nFor executing an Android application, clone the same GitHub repository and open the `image-classification` project in Android Studio. Connect the ERD board and run the application by clicking \"run 'app'\". The application will launch on the ERD board after building.\n\nBoth processes highlight the integration of hardware and software, as they require the use of the ERD board to execute models and applications, demonstrating the execution of NNC models and Android applications in embedded systems.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:05:06.933965+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the role of the ENN framework in executing neural network models on Samsung Exynos hardware?",
    "reference_contexts": [
      "<1-hop>\n\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the [Github](https://github.com/exynos-eco/enn-sdk-samples-9925).  \n1. Download the samples and navigate to the directory that contain the sample program.\n```shell\ngit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\ncd enn-sdk-samples-9925/nnc-model-tester\n```  \n1. Push the necessary files (native program and test files) using adb push.\n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/* /data/local/tmp/\n```  \n1. When nnc_model_tester is built from Windows, execute permission must be provided.\n```shell\nadb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n```  \n1. Execute native binary on ERD board using adb shell.\n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```  \n1. When the command is successful, the following message is displayed:\n```shell\nLoaded Model:\nmodel.nnc(00000B7E01000000)\nModel Execution Time (1): 5413 microseconds\nAvg. Model Execution Time: 5413 microseconds\nOutput Layer(0): Golden Match\n-       snr value:104.802\nENN Framework Execute Model Sucess\n```  \nFor more information on this sample program, refer to [Exynos AI Studio Samples](enn-sdk-samples).\nFor more information on writing native programs using ENN framework, refer to [Getting Started With Native Samples](getting-started-with-native-samples).",
      "<2-hop>\n\n[Exynos AI Studio](https://soc-developer.semiconductor.samsung.com/enn-sdk) allows users to convert the trained [TensorFlow Lite](https://www.tensorflow.org/lite) neural network models to a format that can run efficiently in [Samsung Exynos](https://semiconductor.samsung.com/processor/) hardware.\nExynos AI Studio contains ENNTools to convert trained NN models and ENN framework for executing converted models on Exynos platforms.  \nThis guide covers the basics of using [Exynos AI Studio service](https://soc-developer.semiconductor.samsung.com/enn-sdk/project/) and executing NN models with ENN framework."
    ],
    "reference": "The ENN framework plays a crucial role in executing converted neural network models on Samsung Exynos hardware. It is part of the Exynos AI Studio, which allows users to convert trained TensorFlow Lite models into a format that can run efficiently on Exynos platforms. The framework facilitates the execution of these models, ensuring optimal performance on the hardware.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:05:06.933965+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What does the PoseNet model demonstrate in Android applications?",
    "reference_contexts": [
      "|Sample Name|Description|\n|-------------|-------|\n|[Image Classification In Android](#image-classification-in-android)| Sample Android application to demonstrate the execution of `Inception v4` model with Exynos AI Studio|\n|[Object Detection In Android](#object-detection-in-android)| Sample Android application to demonstrate the execution of `YOLOv5` model with Exynos AI Studio|\n|[Segmentation In Android](#segmentation-in-android)| Sample Android application to demonstrate the execution of `DeeplabV3` model with Exynos AI Studio|\n|[Pose Estimation In Android](#pose-estimation-in-android)| Sample Android application to demonstrate the execution of `PoseNet` model with Exynos AI Studio|\n|[Image Enhance In Android](#image-enhance-in-android)| Sample Android application to demonstrate the execution of `Zero-DCE` model with Exynos AI Studio|\n|[Depth Estimation In Andriod](#depth-estimation-in-andriod)| Sample Android application to demonstrate the execution of `MiDaS v2` model with Exynos AI Studio|\n|[Performance Comparison](#performance-comparison)| Sample Android application to demonstrate the difference between Exynos AI Studio and TFLite |\n|[NNC Model Tester](#nnc-model-tester)|Sample C++ program to demonstrate the execution of NNC model with Exynos AI Studio|"
    ],
    "reference": "The PoseNet model demonstrates pose estimation in Android applications, showcasing its execution with Exynos AI Studio.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:05:38.118258+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are Kotlin sample applications for Android?",
    "reference_contexts": [
      "This section provides an overview of Android (Kotlin) sample applications.\nEach sample application entry provides the details of the functionality of the sample application, its location, and instructions for running it.\nFor more information on implementing the sample applications, refer to [Getting Started With Android Samples](getting-started-with-android-samples) guide.  \n***"
    ],
    "reference": "Kotlin sample applications for Android provide an overview of various functionalities, including details about each application's functionality, its location, and instructions for running it.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:05:38.118258+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does the ENN framework facilitate the execution of AI models in mobile applications?",
    "reference_contexts": [
      "This sample application demonstrates the execution of a converted [Inception v4](https://tfhub.dev/tensorflow/lite-model/inception_v4_quant/1/default/1) model using the ENN framework.\nThe model is converted using Exynos AI Studio service with the **Accelerate** hardware type option."
    ],
    "reference": "The ENN framework facilitates the execution of AI models in mobile applications by allowing the execution of converted models, such as the Inception v4 model, which is converted using the Exynos AI Studio service with the Accelerate hardware type option.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:05:38.118258+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Where can I find the exynos-eco sample for image classification?",
    "reference_contexts": [
      "The sample is available in the `enn-sdk-samples-9925/image-classification` directory within the [Github](https://github.com/exynos-eco/enn-sdk-samples-9925) repository."
    ],
    "reference": "The exynos-eco sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:05:38.118258+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How to use assets in the sample application for AI models?",
    "reference_contexts": [
      "To utilize the sample application:\n1.Download or clone the sample application from the Github repository.\n2.Open the sample application project in Android Studio.\n3.Connect the ERD board to the computer.\n4.Run the application (using Shift + F10).\n5.Select Camera or Image mode and provide the data for inference.  \nTo modify the model used in the sample application:\n1.Copy the desired model file to the `assets` directory of the project.\n2.Copy the corresponding label text file to the `assets` directory.\n3.Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4.If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.  \n***"
    ],
    "reference": "To use assets in the sample application, you need to copy the desired model file to the `assets` directory of the project and also copy the corresponding label text file to the `assets` directory.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:05:38.118258+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How Exynos AI Studio help with model?",
    "reference_contexts": [
      "This sample application demonstrates the execution of a converted [YOLOv5](https://github.com/ultralytics/yolov5) model using the ENN framework.\nThe model is converted using Exynos AI Studio service with the **Default** hardware type option."
    ],
    "reference": "Exynos AI Studio helps by converting the YOLOv5 model using its service with the Default hardware type option.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:05:38.118258+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you provide details on where to find the sample for object-detection in the Exynos AI Studio resources?",
    "reference_contexts": [
      "The sample is available in the `enn-sdk-samples-9925/object-detection` directory within the [Github](https://github.com/exynos-eco/enn-sdk-samples-9925) repository."
    ],
    "reference": "The sample for object-detection can be found in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:05:38.118258+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do you modify the model parameters in ModelConstants.kt?",
    "reference_contexts": [
      "To utilize the sample application:\n1.Download or clone the sample application from the Github repository.\n2.Open the sample application project in Android Studio.\n3.Connect the ERD board to the computer.\n4.Run the application (using Shift + F10).\n5.Select Camera or Image mode and provide the data for inference.  \nTo modify the model used in the sample application:\n1.Copy the desired model file to the `assets` directory of the project.\n2.Copy the corresponding label text file to the `assets` directory.\n3.Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4.If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.  \n***"
    ],
    "reference": "To modify the model used in the sample application, you need to change the parameters in the ModelConstants.kt file to reflect the specifications of the new model.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:05:38.118258+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the relationship between the image classification sample application in Android and the Github repository that provides instructions for executing an NNC model?",
    "reference_contexts": [
      "<1-hop>\n\n|Sample Name|Description|\n|-------------|-------|\n|[Image Classification In Android](#image-classification-in-android)| Sample Android application to demonstrate the execution of `Inception v4` model with Exynos AI Studio|\n|[Object Detection In Android](#object-detection-in-android)| Sample Android application to demonstrate the execution of `YOLOv5` model with Exynos AI Studio|\n|[Segmentation In Android](#segmentation-in-android)| Sample Android application to demonstrate the execution of `DeeplabV3` model with Exynos AI Studio|\n|[Pose Estimation In Android](#pose-estimation-in-android)| Sample Android application to demonstrate the execution of `PoseNet` model with Exynos AI Studio|\n|[Image Enhance In Android](#image-enhance-in-android)| Sample Android application to demonstrate the execution of `Zero-DCE` model with Exynos AI Studio|\n|[Depth Estimation In Andriod](#depth-estimation-in-andriod)| Sample Android application to demonstrate the execution of `MiDaS v2` model with Exynos AI Studio|\n|[Performance Comparison](#performance-comparison)| Sample Android application to demonstrate the difference between Exynos AI Studio and TFLite |\n|[NNC Model Tester](#nnc-model-tester)|Sample C++ program to demonstrate the execution of NNC model with Exynos AI Studio|",
      "<2-hop>\n\nThis program accepts various parameters to execute an NNC model with input binary data.\nIt performs golden matching and displays the inference time, thereby providing a comprehensive view of execution of the model.\nFor more information on specific instructions and parameters, refer to [README](https://github.com/exynos-eco/enn-sdk-samples-9925/tree/main/nnc-model-tester#readme) file in the [Github](https://github.com/exynos-eco/enn-sdk-samples-9925) repository.",
      "<3-hop>\n\nTo utilize the sample application:\n1. The sample is available in the [Github](https://github.com/exynos-eco/enn-sdk-samples-9925) repository.\n1. Build the program by following the instructions described in [Compiling Using NDK](getting-started-with-native-samples/compiling-using-ndk) guide.\n1. Execute the program by following the instructions described in the [Using ADB to Execute Native Program](getting-started-with-native-samples/using-adb) guide."
    ],
    "reference": "The image classification sample application in Android demonstrates the execution of the `Inception v4` model with Exynos AI Studio. For developers looking to utilize this sample application, it is available in the Github repository. The repository contains detailed instructions on how to build and execute various models, including the NNC model, which is executed with input binary data and provides insights such as inference time. Therefore, the Github repository serves as a resource for both the image classification application and the NNC model execution.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:05:38.118258+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do you modify the parameters in ModelConstants.kt to reflect the specifications of a new model in the sample application?",
    "reference_contexts": [
      "<1-hop>\n\nTo utilize the sample application:\n1.Download or clone the sample application from the Github repository.\n2.Open the sample application project in Android Studio.\n3.Connect the ERD board to the computer.\n4.Run the application (using Shift + F10).\n5.Select Camera or Image mode and provide the data for inference.  \nTo modify the model used in the sample application:\n1.Copy the desired model file to the `assets` directory of the project.\n2.Copy the corresponding label text file to the `assets` directory.\n3.Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4.If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.  \n***",
      "<2-hop>\n\nTo utilize the sample application:\n1.Download or clone the sample application from the Github repository.\n2.Open the sample application project in Android Studio.\n3.Connect the ERD board to the computer.\n4.Run the application (using Shift + F10).\n5.Select Camera or Image mode and provide the data for inference.  \nTo modify the model used in the sample application:\n1.Copy the desired model file to the `assets` directory of the project.\n2.Copy the corresponding label text file to the `assets` directory.\n3.Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4.If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.  \n***"
    ],
    "reference": "To modify the parameters in ModelConstants.kt to reflect the specifications of a new model in the sample application, you need to copy the desired model file and the corresponding label text file to the `assets` directory of the project. After that, you can modify the parameters in the ModelConstants.kt file accordingly. If the inputs and outputs of the new model differ from those of the pre-designed sample application, you will also need to adjust the `preProcess()` and `postProcess()` functions.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:05:38.118258+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain the function and purpose of EnnDeinitialize in the Enn Framework?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnInitialize](#function-enninitialize)**(void )<br>Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair.  |\n| EnnReturn | **[EnnDeinitialize](#function-enndeinitialize)**(void )<br>Deinitialize Enn Framework. Framework degenerates context in a caller's process.  |"
    ],
    "reference": "EnnDeinitialize is a function that deinitializes the Enn Framework. It is responsible for degenerating the context in a caller's process.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:53:45.381790+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What Enn Framework do?",
    "reference_contexts": [
      "```cpp\nEnnReturn EnnInitialize(\nvoid\n)\n```  \nInitialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair.  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "Enn Framework initializes and generates context in a caller's process, counting the initialize/deinitialize pair, and returns an EnnReturn result where 0 indicates success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:53:45.381790+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What does the EnnDeinitialize function do?",
    "reference_contexts": [
      "```cpp\nEnnReturn EnnDeinitialize(\nvoid\n)\n```  \nDeinitialize Enn Framework. Framework degenerates context in a caller's process.  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "The EnnDeinitialize function deinitializes the Enn Framework, causing the framework to degenerate context in a caller's process.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:53:45.381790+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What does the function EnnOpenModelFromMemory do?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnOpenModel](#function-ennopenmodel)**(const char * model_file, EnnModelId * model_id)<br>OpenModel with model file.  |\n| EnnReturn | **[EnnOpenModelFromMemory](#function-ennopenmodelfrommemory)**(const char * va, const uint32_t size, EnnModelId * model_id)<br>OpenModel from memory buffer.  |\n| EnnReturn | **[EnnCloseModel](#function-ennclosemodel)**(const EnnModelId model_id);<br>Close model and free all resources in OpenModel()|"
    ],
    "reference": "EnnOpenModelFromMemory opens a model from a memory buffer.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:53:45.381790+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the purpose of the model_file parameter in the EnnOpenModel function?",
    "reference_contexts": [
      "```cpp\nEnnReturn EnnOpenModel(\nconst char * model_file,\nEnnModelId * model_id\n)\n```  \nOpenModel with model file.  \n**Parameters**:  \n* **model_file** [IN] model_file, output from graph-gen. A caller should access the file.\n* **model_id** [OUT] model_id, 64 bit unsigned int  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "The model_file parameter in the EnnOpenModel function is an input parameter that represents the model file, which is the output from graph-gen. A caller should access this file to open the model.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:53:45.381790+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is model_id in the context of EnnOpenModelFromMemory function?",
    "reference_contexts": [
      "```cpp\nEnnReturn EnnOpenModelFromMemory(\nconst char * va,\nconst uint32_t size,\nEnnModelId * model_id\n)\n```  \nOpenModel from memory buffer.  \n**Parameters**:  \n* **va** [IN] address which a model loaded from\n* **size** [IN] size of the buffer\n* **model_id** [OUT] model_id, 64 bit unsigned int  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "model_id is a parameter that is an output of the EnnOpenModelFromMemory function, representing a 64 bit unsigned int.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:53:45.381790+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What EnnModelId do in EnnCloseModel function?",
    "reference_contexts": [
      "```cpp\nEnnReturn EnnCloseModel(\nconst EnnModelId model_id\n);\n```  \nClose model and free all resources in OpenModel().  \n**Parameters**:  \n* **model_id** [IN] model_id from OpenModel()  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "EnnModelId is a parameter in the EnnCloseModel function, which is used to close the model and free all resources allocated in OpenModel().",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:53:45.381790+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the function of EnnCreateBuffer in memory management?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnCreateBuffer](#function-enncreatebuffer)**([EnnBufferPtr](api-reference/enn-framework-data-type-references/#_ennbuffer) * out, const uint32_t req_size, const bool is_cached =true)<br>Create Buffer with request size support ION or dmabufheap which can be used in a device(DSP/CPU/NPU/GPU)  |\n| EnnReturn | **[EnnAllocateAllBuffers](#function-ennallocateallbuffers)**(const EnnModelId model_id, [EnnBufferPtr](api-reference/enn-framework-data-type-references/#_ennbuffer) ** out_buffers, [NumberOfBuffersInfo](api-reference/enn-framework-data-type-references/#_numberofbuffersinfo) * buf_info, const int session_id =0, const bool do_commit =true)<br>Allocate all buffers which a caller should allocate.  |\n| EnnReturn | **[EnnReleaseBuffers](#function-ennreleasebuffers)**([EnnBufferPtr](api-reference/enn-framework-data-type-references/#_ennbuffer) * buffers, const int32_t numOfBuffers)<br>Release buffer array from EnnAllocatedAllBuffers() This API includes releasing all elements in the array.  |\n| EnnReturn | **[EnnReleaseBuffer](#function-ennreleasebuffer)**([EnnBufferPtr](api-reference/enn-framework-data-type-references/#_ennbuffer) buffer)<br>release buffer from [EnnCreateBuffer()](#function-enncreatebuffer) |"
    ],
    "reference": "EnnCreateBuffer creates a buffer with a requested size that supports ION or dmabufheap, which can be used in a device such as DSP, CPU, NPU, or GPU.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:53:45.381790+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What EnnBufferPtr do in EnnCreateBuffer function?",
    "reference_contexts": [
      "```cpp\nEnnReturn EnnCreateBuffer(\nEnnBufferPtr * out,\nconst uint32_t req_size,\nconst bool is_cached =true\n)\n```  \nCreate Buffer with request size support ION or dmabufheap which can be used in a device(DSP/CPU/NPU/GPU)  \n**Parameters**:  \n* **out** [OUT] output buffer pointer. User can get va, size, offset through *out\n* **req_size** [IN] request size\n* **is_cached** [IN] flag, the buffer uses cache or not  \n**Return**: EnnReturn"
    ],
    "reference": "EnnBufferPtr is the output buffer pointer in the EnnCreateBuffer function, allowing the user to get the virtual address, size, and offset through it.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:53:45.381790+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the parameters required for the EnnAllocateAllBuffers function, specifically regarding the EnnModelId?",
    "reference_contexts": [
      "```cpp\nEnnReturn EnnAllocateAllBuffers(\nconst EnnModelId model_id,\nEnnBufferPtr ** out_buffers,\nNumberOfBuffersInfo * buf_info,\nconst int session_id =0,\nconst bool do_commit =true\n)\n```  \nAllocate all buffers which a caller should allocate.  \n**Parameters**:  \n* **model_id** model_id from OpenModel()\n* **out_buffers** [OUT] pointer of EnnBuffer array\n* **buf_info** [OUT] size of the array\n* **session_id** [IN] after generate buffer space, user can set this field if session_id > 0\n* **do_commit** [IN] if true, the framework tries to commit after buffer allocation  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "The EnnAllocateAllBuffers function requires the EnnModelId as the model_id parameter, which is obtained from the OpenModel() function. Other parameters include out_buffers, which is a pointer to an EnnBuffer array, buf_info, which indicates the size of the array, session_id, which can be set by the user if greater than 0, and do_commit, which determines if the framework should attempt to commit after buffer allocation.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:53:45.381790+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What EnnExecuteModelAsync do?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnExecuteModel](Modules/group__api__execute.md#function-ennexecutemodel)**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model with commited buffers.  |\n| EnnReturn | **[EnnExecuteModelAsync](Modules/group__api__execute.md#function-ennexecutemodelasync)**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model in background asynchronously.  |\n| EnnReturn | **[EnnExecuteModelWait](Modules/group__api__execute.md#function-ennexecutemodelwait)**(const EnnModelId model_id, const int session_id =0)<br>Wait result of calling [EnnExecuteModelAsync()](Modules/group__api__execute.md#function-ennexecutemodelasync) If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished.  |"
    ],
    "reference": "EnnExecuteModelAsync request to service core to execute model in background asynchronously.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:54:16.859667+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain the functionality of EnnExecuteModelAsync in the context of executing machine learning models?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnExecuteModel](Modules/group__api__execute.md#function-ennexecutemodel)**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model with commited buffers.  |\n| EnnReturn | **[EnnExecuteModelAsync](Modules/group__api__execute.md#function-ennexecutemodelasync)**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model in background asynchronously.  |\n| EnnReturn | **[EnnExecuteModelWait](Modules/group__api__execute.md#function-ennexecutemodelwait)**(const EnnModelId model_id, const int session_id =0)<br>Wait result of calling [EnnExecuteModelAsync()](Modules/group__api__execute.md#function-ennexecutemodelasync) If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished.  |"
    ],
    "reference": "EnnExecuteModelAsync is a function that requests the service core to execute a model in the background asynchronously. This allows for non-blocking execution of machine learning models, enabling other processes to continue running while the model is being executed.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:54:16.859667+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What session_id do in EnnExecuteModel?",
    "reference_contexts": [
      "```\nEnnReturn EnnExecuteModel(\nconst EnnModelId model_id,\nconst int session_id =0\n)\n```  \nRequest to service core to execute model with commited buffers.  \n**Parameters**:  \n* **model_id** [IN] model ID from load_model\n* **session_id** [IN] session ID  \n**Return**: EnnReturn result, 0 is success  \n**Note**: this function runs in block mode"
    ],
    "reference": "The session_id is an input parameter for the EnnExecuteModel function, which is used to identify the session for executing the model.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:54:16.859667+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the purpose of the EnnExecuteModel function in the context of machine learning services?",
    "reference_contexts": [
      "```\nEnnReturn EnnExecuteModel(\nconst EnnModelId model_id,\nconst int session_id =0\n)\n```  \nRequest to service core to execute model with commited buffers.  \n**Parameters**:  \n* **model_id** [IN] model ID from load_model\n* **session_id** [IN] session ID  \n**Return**: EnnReturn result, 0 is success  \n**Note**: this function runs in block mode"
    ],
    "reference": "The EnnExecuteModel function is a request to the service core to execute a model with committed buffers. It takes parameters such as model_id, which is the model ID from load_model, and session_id, which is the session ID. The function returns an EnnReturn result, where 0 indicates success, and it operates in block mode.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:54:16.859667+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain what EnnExecuteModelAsync does and what parameters it needs?",
    "reference_contexts": [
      "```\nEnnReturn EnnExecuteModelAsync(\nconst EnnModelId model_id,\nconst int session_id =0\n)\n```  \nRequest to service core to execute model in background asynchronously.  \n**Parameters**:  \n* **model_id** [IN] model ID from load_model\n* **session_id** [IN] session ID  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "EnnExecuteModelAsync is a request to the service core to execute a model in the background asynchronously. It requires two parameters: model_id, which is the model ID from load_model, and session_id, which is an optional session ID. The function returns an EnnReturn result, where 0 indicates success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:54:16.859667+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain how EnnExecuteModelAsync works and what happens when you call EnnExecuteModelWait?",
    "reference_contexts": [
      "```\nEnnReturn EnnExecuteModelWait(\nconst EnnModelId model_id,\nconst int session_id =0\n)\n```  \nWait result of calling [EnnExecuteModelAsync()](Modules/group__api__execute.md#function-ennexecutemodelasync) If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished.  \n**Parameters**:  \n* **model_id** [IN] model ID from load_model\n* **session_id** [IN] session ID  \n**Return**: EnnReturn result, 0 is success  \n-------------------------------  \nUpdated on 2023-08-11 at 16:24:05 +0900"
    ],
    "reference": "EnnExecuteModelAsync is a function that allows for the execution of a model asynchronously. When you call EnnExecuteModelWait, it waits for the result of the EnnExecuteModelAsync call. If the execution is finished, EnnExecuteModelWait returns immediately. If not, it blocks until the execution is completed. The parameters for this function include model_id, which is the model ID from load_model, and session_id, which is the session ID. The function returns an EnnReturn result, where 0 indicates success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:54:16.859667+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What EnnReturn do when call EnnExecuteModelWait?",
    "reference_contexts": [
      "```\nEnnReturn EnnExecuteModelWait(\nconst EnnModelId model_id,\nconst int session_id =0\n)\n```  \nWait result of calling [EnnExecuteModelAsync()](Modules/group__api__execute.md#function-ennexecutemodelasync) If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished.  \n**Parameters**:  \n* **model_id** [IN] model ID from load_model\n* **session_id** [IN] session ID  \n**Return**: EnnReturn result, 0 is success  \n-------------------------------  \nUpdated on 2023-08-11 at 16:24:05 +0900"
    ],
    "reference": "EnnReturn is the result returned by the function EnnExecuteModelWait, which waits for the execution of EnnExecuteModelAsync to finish. If execution is finished, it returns immediately; if not, it blocks until execution is complete. A return value of 0 indicates success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:54:16.859667+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain the functionality of EnnAllocateAllBuffers in the context of memory management for software applications?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnCreateBuffer](Modules/group__api__memory.md#function-enncreatebuffer)**([EnnBufferPtr](Classes/struct__enn_buffer.md) * out, const uint32_t req_size, const bool is_cached =true)<br>Create Buffer with request size support ION or dmabufheap which can be used in a device(DSP/CPU/NPU/GPU)  |\n| EnnReturn | **[EnnAllocateAllBuffers](Modules/group__api__memory.md#function-ennallocateallbuffers)**(const EnnModelId model_id, [EnnBufferPtr](Classes/struct__enn_buffer.md) ** out_buffers, [NumberOfBuffersInfo](Classes/struct___number_of_buffers_info.md) * buf_info, const int session_id =0, const bool do_commit =true)<br>Allocate all buffers which a caller should allocate.  |\n| EnnReturn | **[EnnReleaseBuffers](Modules/group__api__memory.md#function-ennreleasebuffers)**([EnnBufferPtr](Classes/struct__enn_buffer.md) * buffers, const int32_t numOfBuffers)<br>Release buffer array from EnnAllocatedAllBuffers() This API includes releasing all elements in the array.  |\n| EnnReturn | **[EnnReleaseBuffer](Modules/group__api__memory.md#function-ennreleasebuffer)**([EnnBufferPtr](Classes/struct__enn_buffer.md) buffer)<br>release buffer from [EnnCreateBuffer()](Modules/group__api__memory.md#function-enncreatebuffer) |"
    ],
    "reference": "EnnAllocateAllBuffers is a function that allocates all buffers that a caller should allocate. It takes parameters such as the model ID, output buffers, buffer information, session ID, and a commit flag. This function is essential for managing memory efficiently in systems involving DSP, CPU, NPU, or GPU.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__memory.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:54:38.742024+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the function of EnnReleaseBuffers in memory management?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnCreateBuffer](Modules/group__api__memory.md#function-enncreatebuffer)**([EnnBufferPtr](Classes/struct__enn_buffer.md) * out, const uint32_t req_size, const bool is_cached =true)<br>Create Buffer with request size support ION or dmabufheap which can be used in a device(DSP/CPU/NPU/GPU)  |\n| EnnReturn | **[EnnAllocateAllBuffers](Modules/group__api__memory.md#function-ennallocateallbuffers)**(const EnnModelId model_id, [EnnBufferPtr](Classes/struct__enn_buffer.md) ** out_buffers, [NumberOfBuffersInfo](Classes/struct___number_of_buffers_info.md) * buf_info, const int session_id =0, const bool do_commit =true)<br>Allocate all buffers which a caller should allocate.  |\n| EnnReturn | **[EnnReleaseBuffers](Modules/group__api__memory.md#function-ennreleasebuffers)**([EnnBufferPtr](Classes/struct__enn_buffer.md) * buffers, const int32_t numOfBuffers)<br>Release buffer array from EnnAllocatedAllBuffers() This API includes releasing all elements in the array.  |\n| EnnReturn | **[EnnReleaseBuffer](Modules/group__api__memory.md#function-ennreleasebuffer)**([EnnBufferPtr](Classes/struct__enn_buffer.md) buffer)<br>release buffer from [EnnCreateBuffer()](Modules/group__api__memory.md#function-enncreatebuffer) |"
    ],
    "reference": "EnnReleaseBuffers is used to release a buffer array that was allocated by EnnAllocateAllBuffers(). This API includes releasing all elements in the array.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__memory.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:54:38.742024+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What does EnnAllocateAllBuffers do?",
    "reference_contexts": [
      "```\nEnnReturn EnnAllocateAllBuffers(\nconst EnnModelId model_id,\nEnnBufferPtr ** out_buffers,\nNumberOfBuffersInfo * buf_info,\nconst int session_id =0,\nconst bool do_commit =true\n)\n```  \nAllocate all buffers which a caller should allocate.  \n**Parameters**:  \n* **model_id** model_id from OpenModel()\n* **out_buffers** [OUT] pointer of EnnBuffer array\n* **buf_info** [OUT] size of the array\n* **session_id** [IN] after generate buffer space, user can set this field if session_id > 0\n* **do_commit** [IN] if true, the framework tries to commit after buffer allocation  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "EnnAllocateAllBuffers allocates all buffers that a caller should allocate, with parameters including model_id, out_buffers, buf_info, session_id, and do_commit.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__memory.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:54:38.742024+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the role of NumberOfBuffersInfo in the EnnAllocateAllBuffers function?",
    "reference_contexts": [
      "```\nEnnReturn EnnAllocateAllBuffers(\nconst EnnModelId model_id,\nEnnBufferPtr ** out_buffers,\nNumberOfBuffersInfo * buf_info,\nconst int session_id =0,\nconst bool do_commit =true\n)\n```  \nAllocate all buffers which a caller should allocate.  \n**Parameters**:  \n* **model_id** model_id from OpenModel()\n* **out_buffers** [OUT] pointer of EnnBuffer array\n* **buf_info** [OUT] size of the array\n* **session_id** [IN] after generate buffer space, user can set this field if session_id > 0\n* **do_commit** [IN] if true, the framework tries to commit after buffer allocation  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "In the EnnAllocateAllBuffers function, NumberOfBuffersInfo is used as an output parameter that provides the size of the array of buffers that the caller should allocate.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__memory.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:54:38.742024+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the role of numOfBuffers in the EnnReleaseBuffers API?",
    "reference_contexts": [
      "```\nEnnReturn EnnReleaseBuffers(\nEnnBufferPtr * buffers,\nconst int32_t numOfBuffers\n)\n```  \nRelease buffer array from EnnAllocatedAllBuffers() This API includes releasing all elements in the array.  \n**Parameters**:  \n* **buffers** [IN] pointer of buffer array\n* **numOfBuffers** [IN] size of bufefr array  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "In the EnnReleaseBuffers API, numOfBuffers is a parameter that specifies the size of the buffer array being released. It is an input parameter that indicates how many elements in the array should be processed.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__memory.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:54:38.742024+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What EnnAllocatedAllBuffers do with buffers?",
    "reference_contexts": [
      "```\nEnnReturn EnnReleaseBuffers(\nEnnBufferPtr * buffers,\nconst int32_t numOfBuffers\n)\n```  \nRelease buffer array from EnnAllocatedAllBuffers() This API includes releasing all elements in the array.  \n**Parameters**:  \n* **buffers** [IN] pointer of buffer array\n* **numOfBuffers** [IN] size of bufefr array  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "EnnAllocatedAllBuffers releases buffer array, this API includes releasing all elements in the array.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__memory.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:54:38.742024+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the function of EnnReleaseBuffer in memory API management, and what parameters does it require?",
    "reference_contexts": [
      "```\nEnnReturn EnnReleaseBuffer(\nEnnBufferPtr buffer\n)\n```  \nrelease buffer from [EnnCreateBuffer()](Modules/group__api__memory.md#function-enncreatebuffer)  \n**Parameters**:  \n* **buffer** [IN] buffer object from [EnnCreateBuffer()](Modules/group__api__memory.md#function-enncreatebuffer)  \n**Return**: EnnReturn result, 0 is success  \n-------------------------------  \nUpdated on 2023-08-11 at 16:24:05 +0900"
    ],
    "reference": "EnnReleaseBuffer is a function that releases a buffer created by EnnCreateBuffer. It requires a single parameter, which is the buffer object from EnnCreateBuffer. The function returns an EnnReturn result, where 0 indicates success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__memory.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:54:38.742024+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What EnnBufferPtr used for in EnnReleaseBuffer function?",
    "reference_contexts": [
      "```\nEnnReturn EnnReleaseBuffer(\nEnnBufferPtr buffer\n)\n```  \nrelease buffer from [EnnCreateBuffer()](Modules/group__api__memory.md#function-enncreatebuffer)  \n**Parameters**:  \n* **buffer** [IN] buffer object from [EnnCreateBuffer()](Modules/group__api__memory.md#function-enncreatebuffer)  \n**Return**: EnnReturn result, 0 is success  \n-------------------------------  \nUpdated on 2023-08-11 at 16:24:05 +0900"
    ],
    "reference": "EnnBufferPtr is used as a parameter in the EnnReleaseBuffer function, which releases a buffer that was created by the EnnCreateBuffer function.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__memory.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:54:38.742024+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain what EnnSetBufferByIndex does in the context of model optimization?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnGetBuffersInfo](Modules/group__api__model.md#function-enngetbuffersinfo)**([NumberOfBuffersInfo](Classes/struct___number_of_buffers_info.md) * buffers_info, const EnnModelId model_id)<br>Get buffers information from loaded model.  |\n| EnnReturn | **[EnnGetBufferInfoByIndex](Modules/group__api__model.md#function-enngetbufferinfobyindex)**([EnnBufferInfo](Classes/struct__enn_buffer_info.md) * out_buf_info, const EnnModelId model_id, const enn_buf_dir_e direction, const uint32_t index)<br>Get one buffer information from loaded model.  |\n| EnnReturn | **[EnnGetBufferInfoByLabel](Modules/group__api__model.md#function-enngetbufferinfobylabel)**([EnnBufferInfo](Classes/struct__enn_buffer_info.md) * out_buf_info, const EnnModelId model_id, const char * label)<br>Get one buffer information from loaded model.  |\n| EnnReturn | **[EnnSetBufferByIndex](Modules/group__api__model.md#function-ennsetbufferbyindex)**(const EnnModelId model_id, const enn_buf_dir_e direction, const uint32_t index, [EnnBufferPtr](Classes/struct__enn_buffer.md) buf, const int session_id =0)<br>Set memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core.  |\n| EnnReturn | **[EnnSetBufferByLabel](Modules/group__api__model.md#function-ennsetbufferbylabel)**(const EnnModelId model_id, const char * label, [EnnBufferPtr](Classes/struct__enn_buffer.md) buf, const int session_id =0)<br>Set memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core.  |"
    ],
    "reference": "EnnSetBufferByIndex sets a memory object to commit-space, allowing a user to generate buffer space to commit. The framework generates 16 spaces, and 'Set Buffer' means a caller can put its memory object into its space. 'Commit' means sending the memory-buffer set, which can run the opened model completely to the service core.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:55:13.584672+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the function of EnnGetBuffersInfo in the context of model execution?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnGetBuffersInfo](Modules/group__api__model.md#function-enngetbuffersinfo)**([NumberOfBuffersInfo](Classes/struct___number_of_buffers_info.md) * buffers_info, const EnnModelId model_id)<br>Get buffers information from loaded model.  |\n| EnnReturn | **[EnnGetBufferInfoByIndex](Modules/group__api__model.md#function-enngetbufferinfobyindex)**([EnnBufferInfo](Classes/struct__enn_buffer_info.md) * out_buf_info, const EnnModelId model_id, const enn_buf_dir_e direction, const uint32_t index)<br>Get one buffer information from loaded model.  |\n| EnnReturn | **[EnnGetBufferInfoByLabel](Modules/group__api__model.md#function-enngetbufferinfobylabel)**([EnnBufferInfo](Classes/struct__enn_buffer_info.md) * out_buf_info, const EnnModelId model_id, const char * label)<br>Get one buffer information from loaded model.  |\n| EnnReturn | **[EnnSetBufferByIndex](Modules/group__api__model.md#function-ennsetbufferbyindex)**(const EnnModelId model_id, const enn_buf_dir_e direction, const uint32_t index, [EnnBufferPtr](Classes/struct__enn_buffer.md) buf, const int session_id =0)<br>Set memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core.  |\n| EnnReturn | **[EnnSetBufferByLabel](Modules/group__api__model.md#function-ennsetbufferbylabel)**(const EnnModelId model_id, const char * label, [EnnBufferPtr](Classes/struct__enn_buffer.md) buf, const int session_id =0)<br>Set memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core.  |"
    ],
    "reference": "EnnGetBuffersInfo is a function that retrieves buffers information from a loaded model. It takes as parameters a pointer to a NumberOfBuffersInfo structure and an EnnModelId to specify which model's buffer information is being requested.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:55:13.584672+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain the function and parameters of EnnGetBuffersInfo in the context of model optimization?",
    "reference_contexts": [
      "```\nEnnReturn EnnGetBuffersInfo(\nNumberOfBuffersInfo * buffers_info,\nconst EnnModelId model_id\n)\n```  \nGet buffers information from loaded model.  \n**Parameters**:  \n* **buffers_info** [OUT] number of in / out buffer which caller should commit.\n* **model_id** [IN] model id from OpenModel()  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "The function EnnGetBuffersInfo is designed to retrieve information about the buffers from a loaded model. It takes two parameters: 'buffers_info', which is an output parameter that indicates the number of input and output buffers that the caller should commit, and 'model_id', which is an input parameter representing the model ID obtained from the OpenModel() function. The function returns an EnnReturn result, where a return value of 0 indicates success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:55:13.584672+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What does the EnnReturn function EnnGetBuffersInfo do in the context of model execution?",
    "reference_contexts": [
      "```\nEnnReturn EnnGetBuffersInfo(\nNumberOfBuffersInfo * buffers_info,\nconst EnnModelId model_id\n)\n```  \nGet buffers information from loaded model.  \n**Parameters**:  \n* **buffers_info** [OUT] number of in / out buffer which caller should commit.\n* **model_id** [IN] model id from OpenModel()  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "The EnnReturn function EnnGetBuffersInfo retrieves buffers information from a loaded model. It has parameters for outputting the number of input/output buffers that the caller should commit and requires a model ID from OpenModel(). The function returns an EnnReturn result, where 0 indicates success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:55:13.584672+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain the function of EnnReturn in the context of retrieving buffer information from a loaded model?",
    "reference_contexts": [
      "```\nEnnReturn EnnGetBufferInfoByIndex(\nEnnBufferInfo * out_buf_info,\nconst EnnModelId model_id,\nconst enn_buf_dir_e direction,\nconst uint32_t index\n)\n```  \nGet one buffer information from loaded model.  \n**Parameters**:  \n* **out_buf_info** [OUT] output buffer information\n* **model_id** [IN] model ID from load_model\n* **direction** [IN] direction (IN, OUT)\n* **index** [IN] buffer's index number in model  \n**Return**: EnnReturn result, 0 is success  \n```cpp\ntypedef struct _ennBufferInfo {\nbool     is_able_to_update;  // this is not used\nuint32_t n;\nuint32_t width;\nuint32_t height;\nuint32_t channel;\nuint32_t size;\nconst char *label;\n} EnnBufferInfo;\n```  \na caller can identify a buffer as {DIR, Index} such as {IN, 0}"
    ],
    "reference": "EnnReturn is the return type of the function EnnGetBufferInfoByIndex, which retrieves information about a specific buffer from a loaded model. The function takes parameters such as out_buf_info for outputting the buffer information, model_id to specify the model, direction to indicate whether the buffer is for input or output, and index to identify the specific buffer within the model. A return value of 0 indicates success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:55:13.584672+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain the purpose and parameters of the EnnGetBufferInfoByIndex function in relation to EnnBufferInfo?",
    "reference_contexts": [
      "```\nEnnReturn EnnGetBufferInfoByIndex(\nEnnBufferInfo * out_buf_info,\nconst EnnModelId model_id,\nconst enn_buf_dir_e direction,\nconst uint32_t index\n)\n```  \nGet one buffer information from loaded model.  \n**Parameters**:  \n* **out_buf_info** [OUT] output buffer information\n* **model_id** [IN] model ID from load_model\n* **direction** [IN] direction (IN, OUT)\n* **index** [IN] buffer's index number in model  \n**Return**: EnnReturn result, 0 is success  \n```cpp\ntypedef struct _ennBufferInfo {\nbool     is_able_to_update;  // this is not used\nuint32_t n;\nuint32_t width;\nuint32_t height;\nuint32_t channel;\nuint32_t size;\nconst char *label;\n} EnnBufferInfo;\n```  \na caller can identify a buffer as {DIR, Index} such as {IN, 0}"
    ],
    "reference": "The EnnGetBufferInfoByIndex function is designed to retrieve information about a specific buffer from a loaded model. It takes several parameters: 'out_buf_info' is an output parameter that will hold the buffer information, 'model_id' is the identifier for the model from which the buffer information is being requested, 'direction' specifies whether the buffer is for input or output, and 'index' indicates the specific buffer's index number within the model. The function returns an EnnReturn result, where a return value of 0 indicates success. The structure EnnBufferInfo contains various attributes such as whether the buffer can be updated, its dimensions (width, height, channel), size, and a label for identification.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:55:13.584672+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What does the function EnnGetBufferInfoByLabel do in a model framework?",
    "reference_contexts": [
      "```\nEnnReturn EnnGetBufferInfoByLabel(\nEnnBufferInfo * out_buf_info,\nconst EnnModelId model_id,\nconst char * label\n)\n```  \nGet one buffer information from loaded model.  \n**Parameters**:  \n* **out_buf_info** [OUT] output buffer information\n* **model_id** [IN] model ID from load_model\n* **label** [IN] label. if .nnc includes redundent label, the framework returns information of the first founded tensor. C-style string type.  \n**Return**: EnnReturn result, 0 is success  \n```cpp\ntypedef struct _ennBufferInfo {\nbool     is_able_to_update;  // this is not used\nuint32_t n;\nuint32_t width;\nuint32_t height;\nuint32_t channel;\nuint32_t size;\nconst char *label;\n} EnnBufferInfo;\n```  \na caller can identify a buffer as {label} or {tensor name}"
    ],
    "reference": "The function EnnGetBufferInfoByLabel retrieves one buffer information from a loaded model, using parameters such as the model ID and a label. It outputs the buffer information through the out_buf_info parameter and returns a result indicating success or failure.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:55:13.584672+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the role of model_id in the EnnGetBufferInfoByLabel function?",
    "reference_contexts": [
      "```\nEnnReturn EnnGetBufferInfoByLabel(\nEnnBufferInfo * out_buf_info,\nconst EnnModelId model_id,\nconst char * label\n)\n```  \nGet one buffer information from loaded model.  \n**Parameters**:  \n* **out_buf_info** [OUT] output buffer information\n* **model_id** [IN] model ID from load_model\n* **label** [IN] label. if .nnc includes redundent label, the framework returns information of the first founded tensor. C-style string type.  \n**Return**: EnnReturn result, 0 is success  \n```cpp\ntypedef struct _ennBufferInfo {\nbool     is_able_to_update;  // this is not used\nuint32_t n;\nuint32_t width;\nuint32_t height;\nuint32_t channel;\nuint32_t size;\nconst char *label;\n} EnnBufferInfo;\n```  \na caller can identify a buffer as {label} or {tensor name}"
    ],
    "reference": "The model_id is an input parameter that represents the model ID from load_model, which is used to get one buffer information from the loaded model.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:55:13.584672+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does the load_model function relate to the EnnSetBufferByIndex operation in managing memory for model execution?",
    "reference_contexts": [
      "```\nEnnReturn EnnSetBufferByIndex(\nconst EnnModelId model_id,\nconst enn_buf_dir_e direction,\nconst uint32_t index,\nEnnBufferPtr buf,\nconst int session_id =0\n)\n```  \nSet memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core.  \n**Parameters**:  \n* **model_id** [IN] model ID from load_model\n* **direction** [IN] Direction (IN/OUT)\n* **index** [IN] index number of buffer\n* **buf** [IN] memory object from EnnCreateBufferXXX()\n* **session_id** [IN] If a caller generates 2 or more buffer space, session_id can be an identifier  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "The load_model function is related to the EnnSetBufferByIndex operation as it provides the model ID, which is a required parameter for the EnnSetBufferByIndex function. This function allows a user to set a memory object to commit-space, enabling the framework to generate buffer space necessary for executing the model completely.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:55:13.584672+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain how session_id is used in the context of buffer management within the EnnSetBufferByIndex function?",
    "reference_contexts": [
      "```\nEnnReturn EnnSetBufferByIndex(\nconst EnnModelId model_id,\nconst enn_buf_dir_e direction,\nconst uint32_t index,\nEnnBufferPtr buf,\nconst int session_id =0\n)\n```  \nSet memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core.  \n**Parameters**:  \n* **model_id** [IN] model ID from load_model\n* **direction** [IN] Direction (IN/OUT)\n* **index** [IN] index number of buffer\n* **buf** [IN] memory object from EnnCreateBufferXXX()\n* **session_id** [IN] If a caller generates 2 or more buffer space, session_id can be an identifier  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "In the context of the EnnSetBufferByIndex function, the session_id serves as an identifier for a caller that generates two or more buffer spaces. This allows the caller to manage and differentiate between multiple buffer allocations effectively.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:55:13.584672+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the significance of OpenModel in software development?",
    "reference_contexts": [
      "---\ntitle: OpenModel / CloseModel related  \n---"
    ],
    "reference": "The context does not provide specific details about the significance of OpenModel in software development.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:55:27.370452+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the relationship between OpenModel and CloseModel?",
    "reference_contexts": [
      "---\ntitle: OpenModel / CloseModel related  \n---"
    ],
    "reference": "The context does not provide specific details about the relationship between OpenModel and CloseModel.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:55:27.370452+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What OpenModel do?",
    "reference_contexts": [
      "---\ntitle: OpenModel / CloseModel related  \n---"
    ],
    "reference": "OpenModel is related to CloseModel.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:55:27.370452+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the function of EnnOpenModelFromMemory in software development?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnOpenModel](Modules/group__api__openmodel.md#function-ennopenmodel)**(const char * model_file, EnnModelId * model_id)<br>OpenModel with model file.  |\n| EnnReturn | **[EnnOpenModelFromMemory](Modules/group__api__openmodel.md#function-ennopenmodelfrommemory)**(const char * va, const uint32_t size, EnnModelId * model_id)<br>OpenModel from memory buffer.  |"
    ],
    "reference": "EnnOpenModelFromMemory is a function that opens a model from a memory buffer, allowing developers to utilize models directly from memory rather than from a file.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:55:27.370452+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you tell me what EnnOpenModel is and how it works in relation to model files?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnOpenModel](Modules/group__api__openmodel.md#function-ennopenmodel)**(const char * model_file, EnnModelId * model_id)<br>OpenModel with model file.  |\n| EnnReturn | **[EnnOpenModelFromMemory](Modules/group__api__openmodel.md#function-ennopenmodelfrommemory)**(const char * va, const uint32_t size, EnnModelId * model_id)<br>OpenModel from memory buffer.  |"
    ],
    "reference": "EnnOpenModel is a function that allows you to open a model using a model file. It takes a model file as input and returns an EnnModelId, which is an identifier for the model.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:55:27.370452+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain what EnnOpenModelFromMemory does in the context of software development?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnOpenModel](Modules/group__api__openmodel.md#function-ennopenmodel)**(const char * model_file, EnnModelId * model_id)<br>OpenModel with model file.  |\n| EnnReturn | **[EnnOpenModelFromMemory](Modules/group__api__openmodel.md#function-ennopenmodelfrommemory)**(const char * va, const uint32_t size, EnnModelId * model_id)<br>OpenModel from memory buffer.  |"
    ],
    "reference": "EnnOpenModelFromMemory is a function that allows for opening a model from a memory buffer, taking in a character pointer for the memory address and a size parameter, along with a model ID pointer.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:55:27.370452+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the parameters and return value of the EnnOpenModel function, particularly regarding the model_id?",
    "reference_contexts": [
      "```\nEnnReturn EnnOpenModel(\nconst char * model_file,\nEnnModelId * model_id\n)\n```  \nOpenModel with model file.  \n**Parameters**:  \n* **model_file** [IN] model_file, output from graph-gen. A caller should access the file.\n* **model_id** [OUT] model_id, 64 bit unsigned int  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "The EnnOpenModel function takes two parameters: model_file, which is an input parameter representing the model file output from graph-gen, and model_id, which is an output parameter that is a 64-bit unsigned int. The function returns an EnnReturn result, where 0 indicates success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:55:27.370452+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What model_file do?",
    "reference_contexts": [
      "```\nEnnReturn EnnOpenModel(\nconst char * model_file,\nEnnModelId * model_id\n)\n```  \nOpenModel with model file.  \n**Parameters**:  \n* **model_file** [IN] model_file, output from graph-gen. A caller should access the file.\n* **model_id** [OUT] model_id, 64 bit unsigned int  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "model_file is input for OpenModel, it is output from graph-gen and caller should access the file.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:55:27.370452+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain what model_id is in the context of loading models from memory?",
    "reference_contexts": [
      "```\nEnnReturn EnnOpenModelFromMemory(\nconst char * va,\nconst uint32_t size,\nEnnModelId * model_id\n)\n```  \nOpenModel from memory buffer.  \n**Parameters**:  \n* **va** [IN] address which a model loaded from\n* **size** [IN] size of the buffer\n* **model_id** [OUT] model_id, 64 bit unsigned int  \n**Return**: EnnReturn result, 0 is success  \n-------------------------------  \nUpdated on 2023-08-11 at 16:24:05 +0900"
    ],
    "reference": "In the context of loading models from memory, model_id is an output parameter that is a 64-bit unsigned integer, which is used to identify the model that has been loaded from the memory buffer.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:55:27.370452+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Wht does va represent in the EnnOpenModelFromMemory function?",
    "reference_contexts": [
      "```\nEnnReturn EnnOpenModelFromMemory(\nconst char * va,\nconst uint32_t size,\nEnnModelId * model_id\n)\n```  \nOpenModel from memory buffer.  \n**Parameters**:  \n* **va** [IN] address which a model loaded from\n* **size** [IN] size of the buffer\n* **model_id** [OUT] model_id, 64 bit unsigned int  \n**Return**: EnnReturn result, 0 is success  \n-------------------------------  \nUpdated on 2023-08-11 at 16:24:05 +0900"
    ],
    "reference": "In the EnnOpenModelFromMemory function, 'va' represents the address from which a model is loaded.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:55:27.370452+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the significance of the size parameter in the EnnOpenModelFromMemory function?",
    "reference_contexts": [
      "```\nEnnReturn EnnOpenModelFromMemory(\nconst char * va,\nconst uint32_t size,\nEnnModelId * model_id\n)\n```  \nOpenModel from memory buffer.  \n**Parameters**:  \n* **va** [IN] address which a model loaded from\n* **size** [IN] size of the buffer\n* **model_id** [OUT] model_id, 64 bit unsigned int  \n**Return**: EnnReturn result, 0 is success  \n-------------------------------  \nUpdated on 2023-08-11 at 16:24:05 +0900"
    ],
    "reference": "The size parameter in the EnnOpenModelFromMemory function indicates the size of the buffer from which the model is loaded.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:55:27.370452+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What AI Studio Farm do?",
    "reference_contexts": [
      "---  \nAI Studio Farm is a service that provides a customized environment for easily and efficiently testing applications on real mobile devices.  \nTo use the AI Studio Farm remote service, you can instantly access it or schedule a connection for a desired date and time."
    ],
    "reference": "AI Studio Farm is a service that gives a customized environment for testing applications on real mobile devices easily and efficiently.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:09.456716+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do I reserv the AI Studio Farm service for a specific date and time?",
    "reference_contexts": [
      "If you want to use the AI Studio Farm service on a specific date and time, you can make a reservation using the **Reservation** feature.  \n![](images/Reserve_Here_1.png)  \n1. Log in to use the AI Studio Farm service,\nthen click the Start Now button.  \n2. Set your desired date and time, then click\nthe Reserve button. Check the required tickets\nfor your reservation and your available ticket balance.  \n![](images/Reserve_Here_2.png)  \n3. After completing the reservation,\nyou can check your reservation details in Reservation Status.  \n**Tip\\!**  \n* Click the Cancel button to cancel your reservation.  \n[**Go to Reservation List**](https://prd.ai-studio-farm.com/global/remotelab/reservation-status?deviceTypeId=000d4a92-5b7c-4d17-83e3-b2015630a566)"
    ],
    "reference": "To reserve the AI Studio Farm service for a specific date and time, log in and click the Start Now button. Then, set your desired date and time and click the Reserve button. Make sure to check the required tickets for your reservation and your available ticket balance. After completing the reservation, you can check your reservation details in Reservation Status.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:09.456716+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the validity period for free credits?",
    "reference_contexts": [
      "To use **the Device Remote service** and make **reservations** on the **AI Studio Farm** page, you need **Tickets**.  \n**How to Earn Tickets:**  \n* **Log in**: Get **10 Tickets** (Max **10 Tickets per day**)  \n* **Write a forum post**: Get **1 Ticket** per post (Max **5 Tickets per day**)  \n* **Create a project post**: Get **10 Tickets** per post (Max **20 Tickets per day**)  \n* **Convert an NNC file**: Get **2 Tickets** per conversion (Max **10 Tickets per day**)  \n**Reservation Ticket Policies**  \n* **Reservation Credit**: Credits used for reservations will be refunded if you do not connect within 30 minutes. If you cancel the reservation before the scheduled time, all credits will be refunded.  \n* **Service Interruption**: 1 credit will be deducted every 30 minutes, and the remaining credits will be refunded.  \n* **Validity**: Free credits will expire after 90 days and will be forfeited without prior notice.  \n* **Credit Balance Check**: Your credit balance will be updated immediately each time you use a credit.  \n* **Management**: You can check the expiration date, acquisition, and usage history of your credits in the \"My Credits\" menu.  \n![](images/Ticket_Management.png)  \n**Tip\\!**  \n* Click the Ticket icon in the top right corner of the page to check your Ticket usage history."
    ],
    "reference": "Free credits will expire after 90 days and will be forfeited without prior notice.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:09.456716+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the role of the Device in the File Upload feature of the Remote Streaming Service?",
    "reference_contexts": [
      "The **File Upload** feature is a useful tool in the **Remote Streaming Service**. It allows you to transfer files from your local storage to the **Device** for various purposes. **Supports most file formats**, **APK files** are **automatically installed and executed** on the device  \n( If an **APK file is not installed**, it may not be supported by the device)  \n![](images/Uploading_File.App.png)  \n**Tip\\!**  \n* All files except APK are uploaded to the /sdcard/download/ directory.\nYou can upload files up to 200MB each, with a total upload limit of 2GB."
    ],
    "reference": "The Device plays a crucial role in the File Upload feature of the Remote Streaming Service by allowing the transfer of files from local storage to the Device for various purposes. It supports most file formats, and APK files are automatically installed and executed on the device, provided they are supported.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:09.456716+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How can users utilize the Remote Streaming Service for documentation purposes?",
    "reference_contexts": [
      "**[Screenshot]**  \nWhile using the **Remote Streaming Service**, you can capture the **Device screen** for documentation or review.\nScreenshots are saved in the **/sdcard/Download/** directory.  \n![](images/Screenshot.Recording_1.png)  \n**[Recording]**  \nYou can record the **Device screen** on the **Remote Streaming** page and save it as a video file.The recorded video file is saved in the **/sdcard/Download/** directory.  \n![](images/Screenshot_Recording_2.png)"
    ],
    "reference": "Users can utilize the Remote Streaming Service to capture the device screen for documentation or review. The screenshots taken are saved in the /sdcard/Download/ directory. Additionally, users can record the device screen on the Remote Streaming page and save the recorded video file in the same /sdcard/Download/ directory.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:09.456716+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How can I manage files on the Device using the Remote Streaming Service?",
    "reference_contexts": [
      "The **File Directory** feature in the **Remote Streaming Service** allows you to access and manage files on the **Device**.  \n![](images/File_Directory.Management.png)  \nDouble-click the **folder icon** to view the desired files. Double-click the **folder icon** to view the desired files.  \n* After uploading a file, access the **/sdcard/download/** directory to check the uploaded file.  \n* Double-click the file to **download it to your local PC**."
    ],
    "reference": "The File Directory feature in the Remote Streaming Service allows you to access and manage files on the Device. You can double-click the folder icon to view the desired files. After uploading a file, you can access the /sdcard/download/ directory to check the uploaded file and double-click the file to download it to your local PC.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:09.456716+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain how the 'cp' command is utilized within the Command Shell feature of the Remote Streaming Service for Android app development?",
    "reference_contexts": [
      "The **Command Shell** feature in the **Remote Streaming Service** allows you to control the device by entering commands.  \n![](images/Command_Shell_Access.png)  \n**Command Shell Rules:**  \n* Enter commands in the **Command Input** field to control the device.  \n* To view in a larger window, click the **Command Shell Window** button to continue entering commands in a modal window.  \nTry This Command to Control the Device.  \n1. View File  \nUse the **cat** command to display the contents of a file:\n```sh\ncat/sdcard/path/to/file.txt\n```  \n2. Move or Rename File  \nUse the **mv** command to move or rename a file.\n```sh\nmv /sdcard/path/to/file.txt /sdcard/path/to/new_name.txt\n```  \n3. Copy File  \nUse the **cp** command to copy a file:\n```sh\ncp /sdcard/path/to/file.txt /sdcard/path/to/backup_file.txt\n```"
    ],
    "reference": "The 'cp' command is used in the Command Shell feature of the Remote Streaming Service to copy a file. For example, you can execute the command 'cp /sdcard/path/to/file.txt /sdcard/path/to/backup_file.txt' to create a backup of the specified file.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:09.456716+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How can an Android App Developer access and download system logs during a remote session?",
    "reference_contexts": [
      "You can review **INFO logs**, which record usage history in real-time, in the **Remote Streaming Service**.  \nClick the **Log** tab to review **system logs and debugging information**.\nTo download system logs for the current session, click the **Download** button.  \n![](images/Logcat_Window.png)  \n**Tip\\!**  \n* Logs can only be downloaded while the remote service is connected.  \n* To view logs more comfortably, click the expand icon to enlarge the window."
    ],
    "reference": "An Android App Developer can access system logs by clicking the Log tab in the Remote Streaming Service, where they can review INFO logs that record usage history in real-time. To download system logs for the current session, they should click the Download button, ensuring that the remote service is connected, as logs can only be downloaded during this time.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:09.456716+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How can the ADB Client Proxy be utilized in the Remote Lab for Android app development?",
    "reference_contexts": [
      "The **ADB Client Proxy** is a tool that enables ADB connections in the **Remote Lab**, allowing you to develop Android applications remotely using ADB.  \n![](images/Adb_proxy_guide.png)  \n1\\. Check your platform and download the ADB Client Proxy.  \n2\\. Go to the Streaming Page , Click menu icon,and copy the token.  \n[**Go to Resource page**](https://prd.ai-studio-farm.com/global/development/enn-sdk)  \n![](images/Adb_proxy_guide_2.png)  \n3\\. Launch the ADB Client Proxy and enter the copied token.  \n4\\. Ensure that the ADB connection is successfully established.  \n**Tip\\!**  \n* Make sure the ADB server is turned off before proceeding.  \n* The ADB Client Proxy connects to ADB on a remote machine, so the ADB server port must be available.  \n* Do not open Android Studio before connecting to the remote ADB server, as it may keep the ADB server running."
    ],
    "reference": "The ADB Client Proxy is a tool that enables ADB connections in the Remote Lab, allowing you to develop Android applications remotely using ADB. To utilize it, you need to check your platform and download the ADB Client Proxy, go to the Streaming Page to copy the token, launch the ADB Client Proxy, and enter the copied token. It is important to ensure that the ADB connection is successfully established, and you should make sure the ADB server is turned off before proceeding. Additionally, the ADB Client Proxy connects to ADB on a remote machine, so the ADB server port must be available, and you should not open Android Studio before connecting to the remote ADB server.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:09.456716+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do I stop the Remot Streaming Servce?",
    "reference_contexts": [
      "To stop the **Remote Streaming Service**, click the **[Stop]** button. This will terminate the service.  \n![](images/Streaming_Service_Terminatio_1.png)  \n**TIP\\!**  \n* Extension Service: You can check for service extension options 10 minutes before the service ends.  \n* Extensions are available for up to 2 hours. If you need more time, extend your session. (Note: Extensions are not possible if another user has already made a reservation.)  \n* If you still have remaining usage time but leave the Streaming Page, click the 'Connect' button in the reservation list to reconnect.\n![](images/Streaming_Service_Terminatio_2.png)"
    ],
    "reference": "To stop the Remote Streaming Service, click the [Stop] button. This will terminate the service.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:09.456716+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the optimization scenarios for CV models in the context of machine learning?",
    "reference_contexts": [
      "This section details the optimization flow according to model type. Currently, optimization scenarios are predetermined for specific model types, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). We plan to update the features to allow flexible scheduling of optimization functions."
    ],
    "reference": "The optimization scenarios for CV models are predetermined according to model type, as detailed in the optimization flow. This includes specific strategies tailored for computer vision models, alongside other types such as large language models and large vision models.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:34.855835+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What model types are included in the optimization scenarios for CV?",
    "reference_contexts": [
      "This section details the optimization flow according to model type. Currently, optimization scenarios are predetermined for specific model types, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). We plan to update the features to allow flexible scheduling of optimization functions."
    ],
    "reference": "The optimization scenarios currently include computer vision (CV) models, large language models (LLM), and large vision models (LVM).",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:34.855835+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Wht are the optimization scenarios for computer vision models?",
    "reference_contexts": [
      "This section details the optimization flow according to model type. Currently, optimization scenarios are predetermined for specific model types, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). We plan to update the features to allow flexible scheduling of optimization functions."
    ],
    "reference": "The optimization scenarios are predetermined for specific model types, including computer vision (CV) models, large language models (LLM), and large vision models (LVM).",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:34.855835+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the process of converting a CNNX model to SNC format?",
    "reference_contexts": [
      "![cv_flow.png](images/cv.png)  \n1. **ONNX-to-CNNX Conversion**  \nConverts an opset16 ONNX model to CNNX format.  \noutput_path : `{result_dir}/cnnx`  \n2. **Optimization**  \nPasses the CNNX model through Simplifier and applies an Optimization template.  \noutput_path : `{result_dir}/optimized`  \n3. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \n4. **Quantization**  \nApplies Fixed Precision Quantization to the optimized CNNX model.  \noutput_path : `{result_dir}/quantized`  \n5. **Performance Evaluation of Quantization**  \nCompares the inference results of the model before and after quantization.  \n6. **CNNX-to-SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`"
    ],
    "reference": "The process of converting a CNNX model to SNC format involves the step labeled 'CNNX-to-SNC Conversion', which converts the CNNX model to SNC format and outputs the result to the specified path: `{result_dir}/snc`.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:34.855835+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the purpose of CNNX in model conversion?",
    "reference_contexts": [
      "![cv_flow.png](images/cv.png)  \n1. **ONNX-to-CNNX Conversion**  \nConverts an opset16 ONNX model to CNNX format.  \noutput_path : `{result_dir}/cnnx`  \n2. **Optimization**  \nPasses the CNNX model through Simplifier and applies an Optimization template.  \noutput_path : `{result_dir}/optimized`  \n3. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \n4. **Quantization**  \nApplies Fixed Precision Quantization to the optimized CNNX model.  \noutput_path : `{result_dir}/quantized`  \n5. **Performance Evaluation of Quantization**  \nCompares the inference results of the model before and after quantization.  \n6. **CNNX-to-SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`"
    ],
    "reference": "CNNX is used to convert an opset16 ONNX model to CNNX format, allowing for further optimization and performance evaluation.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:34.855835+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What CNNX do?",
    "reference_contexts": [
      "![cv_flow.png](images/cv.png)  \n1. **ONNX-to-CNNX Conversion**  \nConverts an opset16 ONNX model to CNNX format.  \noutput_path : `{result_dir}/cnnx`  \n2. **Optimization**  \nPasses the CNNX model through Simplifier and applies an Optimization template.  \noutput_path : `{result_dir}/optimized`  \n3. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \n4. **Quantization**  \nApplies Fixed Precision Quantization to the optimized CNNX model.  \noutput_path : `{result_dir}/quantized`  \n5. **Performance Evaluation of Quantization**  \nCompares the inference results of the model before and after quantization.  \n6. **CNNX-to-SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`"
    ],
    "reference": "CNNX converts an opset16 ONNX model to CNNX format and passes the CNNX model through Simplifier and applies an Optimization template.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:34.855835+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain what CNNX is and how it is used in the process of converting and optimizing machine learning models?",
    "reference_contexts": [
      "![lvm_flow.png](images/lvm.png)  \n1. **ONNX-to-CNNX Conversion**  \nConverts an opset16 ONNX model to CNNX format.  \noutput_path : `{result_dir}/cnnx`  \n2. **Optimization**  \nPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.  \noutput_path : `{result_dir}/optimized`  \n3. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \n4. **Quantization**  \nApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.  \noutput_path :  \n`{result_dir}/smoothquant`  \n`{result_dir}/mixed_precision_quant`  \n`{result_dir}/softmax_bias_correction`  \n5. **Performance Evaluation of Quantization**  \nCompares the inference results of the model before and after quantization.  \n6. **CNNX-to-SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`"
    ],
    "reference": "CNNX is a format used for converting an opset16 ONNX model to CNNX format, which is part of a series of steps that include optimization, performance evaluation, and quantization. The process involves passing the CNNX model through a Simplifier and 4-Dimensional Conversion, followed by applying an Optimization template. After optimization, the performance of the model is evaluated by comparing inference results before and after the optimization. Additionally, CNNX supports quantization methods such as Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction, with performance evaluations conducted similarly before and after quantization. Finally, the CNNX model can be converted to SNC format.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:34.855835+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does the Simplifier contribute to the optimization process in machine learning model conversion?",
    "reference_contexts": [
      "![lvm_flow.png](images/lvm.png)  \n1. **ONNX-to-CNNX Conversion**  \nConverts an opset16 ONNX model to CNNX format.  \noutput_path : `{result_dir}/cnnx`  \n2. **Optimization**  \nPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.  \noutput_path : `{result_dir}/optimized`  \n3. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \n4. **Quantization**  \nApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.  \noutput_path :  \n`{result_dir}/smoothquant`  \n`{result_dir}/mixed_precision_quant`  \n`{result_dir}/softmax_bias_correction`  \n5. **Performance Evaluation of Quantization**  \nCompares the inference results of the model before and after quantization.  \n6. **CNNX-to-SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`"
    ],
    "reference": "The Simplifier is used to pass the CNNX model through optimization and 4-Dimensional Conversion, followed by the application of an Optimization template.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:34.855835+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How is Softmax Bias Correction applied in the quantization process of machine learning models?",
    "reference_contexts": [
      "![lvm_flow.png](images/lvm.png)  \n1. **ONNX-to-CNNX Conversion**  \nConverts an opset16 ONNX model to CNNX format.  \noutput_path : `{result_dir}/cnnx`  \n2. **Optimization**  \nPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.  \noutput_path : `{result_dir}/optimized`  \n3. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \n4. **Quantization**  \nApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.  \noutput_path :  \n`{result_dir}/smoothquant`  \n`{result_dir}/mixed_precision_quant`  \n`{result_dir}/softmax_bias_correction`  \n5. **Performance Evaluation of Quantization**  \nCompares the inference results of the model before and after quantization.  \n6. **CNNX-to-SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`"
    ],
    "reference": "Softmax Bias Correction is applied sequentially to the CNNX FP32 model during the quantization process, which also includes Smooth Quantization and Mixed Precision Quantization methods.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:34.855835+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the steps involved in optimizing a CNNX model for improved performance and deployment?",
    "reference_contexts": [
      "![llm_flow.png](images/llm.png)  \n1. **Optimization**  \nPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.  \noutput_path : `{result_dir}/cnnx`  \n2. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \noutput_path : `{result_dir}/optimized`  \n3. **CNNX-to SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`"
    ],
    "reference": "The steps involved in optimizing a CNNX model include passing the model through Simplifier and 4-Dimensional Conversion, applying an Optimization template, evaluating the performance of the optimization by comparing inference results before and after optimization, and converting the CNNX model to SNC format.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:34.855835+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you elaborate on the process involved in optimizing the CNNX model and its subsequent evaluation?",
    "reference_contexts": [
      "![llm_flow.png](images/llm.png)  \n1. **Optimization**  \nPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.  \noutput_path : `{result_dir}/cnnx`  \n2. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \noutput_path : `{result_dir}/optimized`  \n3. **CNNX-to SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`"
    ],
    "reference": "The optimization process for the CNNX model involves several steps. First, the model is passed through a Simplifier and undergoes 4-Dimensional Conversion. Following this, an Optimization template is applied. The output of this step is saved to the specified path `{result_dir}/cnnx`. After optimization, the performance evaluation compares the inference results of the model before and after the optimization process, with the results being saved to `{result_dir}/optimized`. Additionally, the CNNX model is converted to SNC format, with the output stored at `{result_dir}/snc`.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:34.855835+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the purpse of CNNX in model optimization?",
    "reference_contexts": [
      "![llm_flow.png](images/llm.png)  \n1. **Optimization**  \nPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.  \noutput_path : `{result_dir}/cnnx`  \n2. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \noutput_path : `{result_dir}/optimized`  \n3. **CNNX-to SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`"
    ],
    "reference": "The CNNX model is passed through Simplifier and 4-Dimensional Conversion, followed by the application of an Optimization template to enhance its performance.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:34.855835+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What GPUs do for models training?",
    "reference_contexts": [
      "Models are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.  \n- shape_inference\nIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\n- 4dim conversion\nTo deploy a model with a .nnc extension on a device, all operators must have input and output shapes with four dimensions. To facilitate this, the 4D conversion process identifies and converts all channel axes of the operators to 4D while maintaining their original structure and channel axes. The channel axes can be determined by certain layers, such as Convolutional and Linear layers, which have fixed channel axes at positions 1 and -1, respectively. However, they can be modified by operators like MatMul and Reshape, which can expand or reduce them. To account for these changes, the conversion process tracks channel axes in both the forward and backward pass. As a result, the 4D conversion process may require additional time to complete.\n- support optimization method.\nOptimizer supports many optimization features so that the model works efficiently on the device.\n- Fold\n- GeGLU\n- GeLU\n- GroupNorm\n- LayerNorm\n- PReLU\n- RMSNorm\n- SiLU\n- Fuse\n- SiLU (to GroupNorm)\n- BatchNorm into Convolution\n- Cast\n- Deconvolution bias\n- Math\n- multiple reshape and transpose in a row (when possible)\n- multiple concat in a row\n- Insert\n- Depthwise Convolution for activation\n- Remove\n- unecesary slices\n- Replace\n- Average Pooling to Depthwise convolution\n- Eltwise concat convolution\n- expand with concat (by concatenating the same input multiple times)\n- Convolution kernel 1 to 3\n- Matrix multiplication to dynamic convolution\n- ReduceMean to Global Average Pool\n- ReduceSum to Convolution\n- Slice to Split\n- Global Average Pool to 2 Average Pool\n- Change attribute\n- axis of softmax  \nIn addition, Optimizer Template, one of the Optimizer's powerful features, supports custom optimization methods made by users."
    ],
    "reference": "Models are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\optimizer\\optimizer.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:54.424261+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does the Optimizer enhance model performance on Exynos chips?",
    "reference_contexts": [
      "Models are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.  \n- shape_inference\nIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\n- 4dim conversion\nTo deploy a model with a .nnc extension on a device, all operators must have input and output shapes with four dimensions. To facilitate this, the 4D conversion process identifies and converts all channel axes of the operators to 4D while maintaining their original structure and channel axes. The channel axes can be determined by certain layers, such as Convolutional and Linear layers, which have fixed channel axes at positions 1 and -1, respectively. However, they can be modified by operators like MatMul and Reshape, which can expand or reduce them. To account for these changes, the conversion process tracks channel axes in both the forward and backward pass. As a result, the 4D conversion process may require additional time to complete.\n- support optimization method.\nOptimizer supports many optimization features so that the model works efficiently on the device.\n- Fold\n- GeGLU\n- GeLU\n- GroupNorm\n- LayerNorm\n- PReLU\n- RMSNorm\n- SiLU\n- Fuse\n- SiLU (to GroupNorm)\n- BatchNorm into Convolution\n- Cast\n- Deconvolution bias\n- Math\n- multiple reshape and transpose in a row (when possible)\n- multiple concat in a row\n- Insert\n- Depthwise Convolution for activation\n- Remove\n- unecesary slices\n- Replace\n- Average Pooling to Depthwise convolution\n- Eltwise concat convolution\n- expand with concat (by concatenating the same input multiple times)\n- Convolution kernel 1 to 3\n- Matrix multiplication to dynamic convolution\n- ReduceMean to Global Average Pool\n- ReduceSum to Convolution\n- Slice to Split\n- Global Average Pool to 2 Average Pool\n- Change attribute\n- axis of softmax  \nIn addition, Optimizer Template, one of the Optimizer's powerful features, supports custom optimization methods made by users."
    ],
    "reference": "The Optimizer enhances model performance on Exynos chips by providing optimization methods that ensure the models work efficiently on the device. It supports features like shape inference, 4D conversion, and various optimization techniques such as Fold, GeGLU, and BatchNorm into Convolution, among others.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\optimizer\\optimizer.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:54.424261+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How can I use the Optimizr Templat to optimize my model effectively?",
    "reference_contexts": [
      "Optimizer is a tool that allows users to easily optimize their models. Users can add their own optimization methods using the \"Optimizer Template\" of Optimizer. The following process is required to apply the user-defined template to the user model.  \n- Create custom templates.\n- Prepare model to be optimized\n- Validate optimized model"
    ],
    "reference": "To use the \"Optimizer Template\" to optimize your model effectively, you need to follow these steps: first, create custom templates; second, prepare the model to be optimized; and finally, validate the optimized model.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\optimizer\\optimizer.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:54.424261+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How can the Optimizer Template be utilized to enhance model optimization?",
    "reference_contexts": [
      "Optimizer is a tool that allows users to easily optimize their models. Users can add their own optimization methods using the \"Optimizer Template\" of Optimizer. The following process is required to apply the user-defined template to the user model.  \n- Create custom templates.\n- Prepare model to be optimized\n- Validate optimized model"
    ],
    "reference": "The Optimizer Template can be utilized to enhance model optimization by allowing users to create custom templates, prepare their model to be optimized, and validate the optimized model.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\optimizer\\optimizer.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:54.424261+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is CnnxAttribute used for in the context of model editing?",
    "reference_contexts": [
      "In this part, we introduce how to write custom templates step by step. As an example, we use a template that fuses basic arithmetic operations layers into a convolution layer, when possible. We provide `TemplateStepInternal`, `TemplateCaseInternal`, `OptimizerTemplateInternal` to prevent abnormal behavior of the code.  \n```python\nclass TemplateStepInternal(TemplateStep):\n@property\ndef name(self) -> AnyStr:\nreturn self._name\n\n@name.setter\ndef name(self, value: AnyStr):\nself._name = value\n\n@property\ndef origin(self) -> Dict:\nreturn self._origin\n\n@origin.setter\ndef origin(self, value: Dict):\nself._origin = value\n\ndef optimization(self, model: CnnxModel, node: CnnxNode) -> bool:\nreturn NotImplementedError\n\n\nclass TemplateCaseInternal(TemplateCase):\n@property\ndef name(self) -> AnyStr:\nreturn self._name\n\n@name.setter\ndef name(self, value: AnyStr):\nself._name = value\n\n@property\ndef origin(self) -> Dict:\nreturn self._origin\n\n@origin.setter\ndef origin(self, value: Dict):\nself._originn = value\n\n@property\ndef step_list(self) -> Dict:\nreturn self._step_list\n\n@step_list.setter\ndef step_list(self, value: Dict):\nself._step_list = value\n\ndef origin_condition(self, model: CnnxModel, node: CnnxNode) -> bool:\nraise NotImplementedError\n\ndef launch(self, model: CnnxModel, node: CnnxNode) -> bool:\nif self.origin_condition(model, node):\nfor st in self.step_list:\nif not st.optimization(model, node):\nreturn False\nreturn True\n\n\n\nclass OptimizerTemplateInternal(OptimizerTemplate):\ndef __repr__(self):\nreturn '{}(name={})'.format(\nself.__class__.__name__, self.name)\n\n@property\ndef name(self) -> AnyStr:\nreturn self._name\n\n@name.setter\ndef name(self, value: AnyStr):\nself._name = value\n\n@property\ndef case_list(self) -> List[TemplateCaseInternal]:\nreturn self._case_list\n\n@case_list.setter\ndef case_list(self, value: List[TemplateCaseInternal]):\nself._case_list = value\n\ndef trigger_op(self, node: CnnxNode) -> bool:\nraise NotImplementedError\n\n\ndef launch(self, model: CnnxModel, node: CnnxNode) -> bool:\nif self.trigger_op(node):\nfor case in self.case_list:\nif case.launch(model, node):\nreturn True\nreturn False\n```  \nInherit the above classes and declare each class corresponding to Step, Case, and Template. Import package classes of model_editor for updating the model. If you need more information about model_editor, please see the guide about model_editor.  \nWe **activate the trigger_op method** when the node is one of the four basic arithmetic operators and the input feature map has 4 dimension. In the **origin_condition method of the case class**, more detailled conditions are checked: the previous node of the arithmetic operator must be a convolution layer, the arithmetic operator must be the only node following the convolution node, and it must not be folded yet.\nIn the **optimization method of Step class**, update the value of the convolution node using the value of the arithmetic node. Once the value has been updated, you no longer need the original node. Create a port to clear the node from the graph. update the model using the port you created. First, remove the node from the model. Then, connect the next node of the removed node to the convolution node.  \n```python\n# That classes prevent the abnormal behavior of the code\nfrom optimizer.core.templates.base.template_internal import (\nTemplateStepInternal,\nTemplateCaseInternal,\nOptimizerTemplateInternal\n)\n# These packages have the ability to add, delete, and update nodes of the model.\n# Please see the guide about model_editor for more information.\nfrom model_editor import (\nCnnxNode,\nCnnxModel,\nCnnxInOut,\nCnnxAttribute,\n)\n\n# declare Step class\nclass StepFuseMath(TemplateStepInternal):\ndef __init__(self, device_info):\nself.name = 'Step Fusing Math'\nself.origin = 'Origin Operator, and Parameters'\nself.device_info = device_info\n\n# create bias\ndef make_bias_input(self, node: CnnxNode):\nbias_data = np.zeros(shape=node.outputs[0].inout.shape[1]).astype(np.float32)\nreturn CnnxInOut(\nname=f\"{node.name}_bias\",\ndtype=\"float\",\nshape=[node.outputs[0].inout.shape[1]],\ndata=bias_data\n)\n\ndef optimization(self, model: CnnxModel, node: CnnxNode) -> bool:\n# Set constant Value\n# Apply constant Value to weight, and bias\nparent_conv = node.prev_nodes[0]\nif not node.inputs[1].inout.shape or len(node.inputs[0].inout.shape) > 2 and len(node.inputs[1].inout.shape) > 2:\nshape_0 = node.inputs[0].inout.shape if node.inputs[0].inout.shape is not None else []\nshape_1 = node.inputs[1].inout.shape if node.inputs[1].inout.shape is not None else []\nif shape_0[-2:] == shape_1[-2:]:\nkernel_data = node.inputs[1].inout.data if node.inputs[1].inout.data is not None else node.inputs[0].inout.data\nbias_data = node.inputs[1].inout.data if node.inputs[1].inout.data is not None else node.inputs[0].inout.data\nelse:\nkernel_data = node.inputs[1].inout.data.reshape((-1, 1, 1, 1)) if node.inputs[1].inout.data is not None else node.inputs[0].inout.data.reshape((-1, 1, 1, 1))\nbias_data = node.inputs[1].inout.data.reshape((-1)) if node.inputs[1].inout.data is not None else node.inputs[0].inout.data.reshape((-1))\nif node.op_type == 'Add':\nif len(parent_conv.inputs) != 3:\nparent_conv.inputs.append(self.make_bias_input(parent_conv))\nnew_data = parent_conv.inputs.data[2].inout.data + bias_data\nparent_conv.inputs.data[2].inout.data = new_data\nparent_conv.inputs.data[2].inout.shape = new_data.shape\n\nelif node.op_type == 'Sub':\nif len(parent_conv.inputs) != 3:\nparent_conv.inputs.append(self.make_bias_input(parent_conv))\nnew_data = parent_conv.inputs.data[2].inout.data - bias_data\nparent_conv.inputs.data[2].inout.data = new_data\nparent_conv.inputs.data[2].inout.shape = new_data.shape\nelif node.op_type == 'Mul':\nparent_conv.inputs.data[1].inout.data = parent_conv.inputs.data[1].inout.data * kernel_data\nif len(parent_conv.inputs) == 3:\nparent_conv.inputs.data[2].inout.data = parent_conv.inputs.data[2].inout.data * bias_data\n\nelif node.op_type == 'Div':\nparent_conv.inputs.data[1].inout.data = parent_conv.inputs.data[1].inout.data / kernel_data\nif len(parent_conv.inputs) == 3:\nparent_conv.inputs.data[2].inout.data = parent_conv.inputs.data[2].inout.data / bias_data\n\nconnections = []\nfor next_node in node.next_nodes:\nfor output_tensor in node.outputs:\nport_from = node.prev_nodes[0].outputs[0]\nfor i in next_node.inputs:\nif i.name == output_tensor.name:\nconnections.append((port_from, i))\n\noptimized_nodes = [parent_conv]\nprevious_nodes = [node]\n\ntry:\nmodel.graph.nodes.remove(node)\nfor connection in connections:\nmodel.graph.nodes.connect(connection[0], connection[1])\nlogger.info(f'Fuse {node.op_type}({node.name}) into Deconv({parent_conv.name})')\nreturn True\nexcept Exception as e:\nlogger.error(e)\nreturn False\n\n\nclass CaseFuseMath(TemplateCaseInternal):\ndef __init__(self, device_info):\nself.name = 'Fuse Math to DWCONV'\nself.step_list = [StepFuseMath(device_info)]\n\n# Check the number of Child of [CONV, DWCONV]\ndef origin_condition(self, model: CnnxModel, node: CnnxNode) -> bool:\nif node.prev_nodes == []:\nreturn False\nif node.inputs[0].inout.data is None and node.inputs[1].inout.data is None:\nreturn False\nfm_shape = node.inputs[0].inout.shape if node.inputs[0].inout.data is None else node.inputs[1].inout.shape\nbias_data = node.inputs[0].inout.data if node.inputs[0].inout.data is not None else node.inputs[1].inout.data\n\n# if bias data just have one value\n# if length of bias data same with output_channel of prev conv node\nif isinstance(bias_data.tolist(), list) and fm_shape[1] != bias_data.reshape((-1)).shape[0]:\nreturn False\nif len(node.prev_nodes[0].next_nodes) == 1 and \\\nlen(node.prev_nodes) == 1 and \\\nnode.inputs[0].inout.shape != node.inputs[1].inout.shape:\nif node.prev_nodes[0].op_type in ['Conv', 'ConvTranspose'] and \\\nnode.module is None:\nreturn True\nreturn False\n\n\nclass TemplateFuseMath(OptimizerTemplateInternal):\ndef __init__(self):\nself.name = 'FuseMath'\n# declare Case\nself.case_list = [\nCaseFuseMath()\n]\n'''\naccording to Flow, The first thing you encounter when navigating the nodes in the graph is trigger_op.\nIn this case, it was written to operate only when the input shape of the four arithmetic operations operator\nand a input feature map of node has 4 dimension.\n'''\ndef trigger_op(self, node: CnnxNode):\noptypes = ['Add', 'Sub', 'Mul', 'Div']\nif node.op_type in optypes:\nif len(node.inputs[0].shape) == 4 or len(node.inputs[1].shape) == 4:\nreturn True\nreturn False\n```"
    ],
    "reference": "CnnxAttribute is part of the model_editor package, which has the ability to add, delete, and update nodes of the model.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\optimizer\\optimizer.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:54.424261+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does CaseFuseMath optimize arithmetic operations in convolutional neural networks?",
    "reference_contexts": [
      "In this part, we introduce how to write custom templates step by step. As an example, we use a template that fuses basic arithmetic operations layers into a convolution layer, when possible. We provide `TemplateStepInternal`, `TemplateCaseInternal`, `OptimizerTemplateInternal` to prevent abnormal behavior of the code.  \n```python\nclass TemplateStepInternal(TemplateStep):\n@property\ndef name(self) -> AnyStr:\nreturn self._name\n\n@name.setter\ndef name(self, value: AnyStr):\nself._name = value\n\n@property\ndef origin(self) -> Dict:\nreturn self._origin\n\n@origin.setter\ndef origin(self, value: Dict):\nself._origin = value\n\ndef optimization(self, model: CnnxModel, node: CnnxNode) -> bool:\nreturn NotImplementedError\n\n\nclass TemplateCaseInternal(TemplateCase):\n@property\ndef name(self) -> AnyStr:\nreturn self._name\n\n@name.setter\ndef name(self, value: AnyStr):\nself._name = value\n\n@property\ndef origin(self) -> Dict:\nreturn self._origin\n\n@origin.setter\ndef origin(self, value: Dict):\nself._originn = value\n\n@property\ndef step_list(self) -> Dict:\nreturn self._step_list\n\n@step_list.setter\ndef step_list(self, value: Dict):\nself._step_list = value\n\ndef origin_condition(self, model: CnnxModel, node: CnnxNode) -> bool:\nraise NotImplementedError\n\ndef launch(self, model: CnnxModel, node: CnnxNode) -> bool:\nif self.origin_condition(model, node):\nfor st in self.step_list:\nif not st.optimization(model, node):\nreturn False\nreturn True\n\n\n\nclass OptimizerTemplateInternal(OptimizerTemplate):\ndef __repr__(self):\nreturn '{}(name={})'.format(\nself.__class__.__name__, self.name)\n\n@property\ndef name(self) -> AnyStr:\nreturn self._name\n\n@name.setter\ndef name(self, value: AnyStr):\nself._name = value\n\n@property\ndef case_list(self) -> List[TemplateCaseInternal]:\nreturn self._case_list\n\n@case_list.setter\ndef case_list(self, value: List[TemplateCaseInternal]):\nself._case_list = value\n\ndef trigger_op(self, node: CnnxNode) -> bool:\nraise NotImplementedError\n\n\ndef launch(self, model: CnnxModel, node: CnnxNode) -> bool:\nif self.trigger_op(node):\nfor case in self.case_list:\nif case.launch(model, node):\nreturn True\nreturn False\n```  \nInherit the above classes and declare each class corresponding to Step, Case, and Template. Import package classes of model_editor for updating the model. If you need more information about model_editor, please see the guide about model_editor.  \nWe **activate the trigger_op method** when the node is one of the four basic arithmetic operators and the input feature map has 4 dimension. In the **origin_condition method of the case class**, more detailled conditions are checked: the previous node of the arithmetic operator must be a convolution layer, the arithmetic operator must be the only node following the convolution node, and it must not be folded yet.\nIn the **optimization method of Step class**, update the value of the convolution node using the value of the arithmetic node. Once the value has been updated, you no longer need the original node. Create a port to clear the node from the graph. update the model using the port you created. First, remove the node from the model. Then, connect the next node of the removed node to the convolution node.  \n```python\n# That classes prevent the abnormal behavior of the code\nfrom optimizer.core.templates.base.template_internal import (\nTemplateStepInternal,\nTemplateCaseInternal,\nOptimizerTemplateInternal\n)\n# These packages have the ability to add, delete, and update nodes of the model.\n# Please see the guide about model_editor for more information.\nfrom model_editor import (\nCnnxNode,\nCnnxModel,\nCnnxInOut,\nCnnxAttribute,\n)\n\n# declare Step class\nclass StepFuseMath(TemplateStepInternal):\ndef __init__(self, device_info):\nself.name = 'Step Fusing Math'\nself.origin = 'Origin Operator, and Parameters'\nself.device_info = device_info\n\n# create bias\ndef make_bias_input(self, node: CnnxNode):\nbias_data = np.zeros(shape=node.outputs[0].inout.shape[1]).astype(np.float32)\nreturn CnnxInOut(\nname=f\"{node.name}_bias\",\ndtype=\"float\",\nshape=[node.outputs[0].inout.shape[1]],\ndata=bias_data\n)\n\ndef optimization(self, model: CnnxModel, node: CnnxNode) -> bool:\n# Set constant Value\n# Apply constant Value to weight, and bias\nparent_conv = node.prev_nodes[0]\nif not node.inputs[1].inout.shape or len(node.inputs[0].inout.shape) > 2 and len(node.inputs[1].inout.shape) > 2:\nshape_0 = node.inputs[0].inout.shape if node.inputs[0].inout.shape is not None else []\nshape_1 = node.inputs[1].inout.shape if node.inputs[1].inout.shape is not None else []\nif shape_0[-2:] == shape_1[-2:]:\nkernel_data = node.inputs[1].inout.data if node.inputs[1].inout.data is not None else node.inputs[0].inout.data\nbias_data = node.inputs[1].inout.data if node.inputs[1].inout.data is not None else node.inputs[0].inout.data\nelse:\nkernel_data = node.inputs[1].inout.data.reshape((-1, 1, 1, 1)) if node.inputs[1].inout.data is not None else node.inputs[0].inout.data.reshape((-1, 1, 1, 1))\nbias_data = node.inputs[1].inout.data.reshape((-1)) if node.inputs[1].inout.data is not None else node.inputs[0].inout.data.reshape((-1))\nif node.op_type == 'Add':\nif len(parent_conv.inputs) != 3:\nparent_conv.inputs.append(self.make_bias_input(parent_conv))\nnew_data = parent_conv.inputs.data[2].inout.data + bias_data\nparent_conv.inputs.data[2].inout.data = new_data\nparent_conv.inputs.data[2].inout.shape = new_data.shape\n\nelif node.op_type == 'Sub':\nif len(parent_conv.inputs) != 3:\nparent_conv.inputs.append(self.make_bias_input(parent_conv))\nnew_data = parent_conv.inputs.data[2].inout.data - bias_data\nparent_conv.inputs.data[2].inout.data = new_data\nparent_conv.inputs.data[2].inout.shape = new_data.shape\nelif node.op_type == 'Mul':\nparent_conv.inputs.data[1].inout.data = parent_conv.inputs.data[1].inout.data * kernel_data\nif len(parent_conv.inputs) == 3:\nparent_conv.inputs.data[2].inout.data = parent_conv.inputs.data[2].inout.data * bias_data\n\nelif node.op_type == 'Div':\nparent_conv.inputs.data[1].inout.data = parent_conv.inputs.data[1].inout.data / kernel_data\nif len(parent_conv.inputs) == 3:\nparent_conv.inputs.data[2].inout.data = parent_conv.inputs.data[2].inout.data / bias_data\n\nconnections = []\nfor next_node in node.next_nodes:\nfor output_tensor in node.outputs:\nport_from = node.prev_nodes[0].outputs[0]\nfor i in next_node.inputs:\nif i.name == output_tensor.name:\nconnections.append((port_from, i))\n\noptimized_nodes = [parent_conv]\nprevious_nodes = [node]\n\ntry:\nmodel.graph.nodes.remove(node)\nfor connection in connections:\nmodel.graph.nodes.connect(connection[0], connection[1])\nlogger.info(f'Fuse {node.op_type}({node.name}) into Deconv({parent_conv.name})')\nreturn True\nexcept Exception as e:\nlogger.error(e)\nreturn False\n\n\nclass CaseFuseMath(TemplateCaseInternal):\ndef __init__(self, device_info):\nself.name = 'Fuse Math to DWCONV'\nself.step_list = [StepFuseMath(device_info)]\n\n# Check the number of Child of [CONV, DWCONV]\ndef origin_condition(self, model: CnnxModel, node: CnnxNode) -> bool:\nif node.prev_nodes == []:\nreturn False\nif node.inputs[0].inout.data is None and node.inputs[1].inout.data is None:\nreturn False\nfm_shape = node.inputs[0].inout.shape if node.inputs[0].inout.data is None else node.inputs[1].inout.shape\nbias_data = node.inputs[0].inout.data if node.inputs[0].inout.data is not None else node.inputs[1].inout.data\n\n# if bias data just have one value\n# if length of bias data same with output_channel of prev conv node\nif isinstance(bias_data.tolist(), list) and fm_shape[1] != bias_data.reshape((-1)).shape[0]:\nreturn False\nif len(node.prev_nodes[0].next_nodes) == 1 and \\\nlen(node.prev_nodes) == 1 and \\\nnode.inputs[0].inout.shape != node.inputs[1].inout.shape:\nif node.prev_nodes[0].op_type in ['Conv', 'ConvTranspose'] and \\\nnode.module is None:\nreturn True\nreturn False\n\n\nclass TemplateFuseMath(OptimizerTemplateInternal):\ndef __init__(self):\nself.name = 'FuseMath'\n# declare Case\nself.case_list = [\nCaseFuseMath()\n]\n'''\naccording to Flow, The first thing you encounter when navigating the nodes in the graph is trigger_op.\nIn this case, it was written to operate only when the input shape of the four arithmetic operations operator\nand a input feature map of node has 4 dimension.\n'''\ndef trigger_op(self, node: CnnxNode):\noptypes = ['Add', 'Sub', 'Mul', 'Div']\nif node.op_type in optypes:\nif len(node.inputs[0].shape) == 4 or len(node.inputs[1].shape) == 4:\nreturn True\nreturn False\n```"
    ],
    "reference": "CaseFuseMath optimizes arithmetic operations by checking specific conditions in the origin_condition method, ensuring that the previous node is a convolution layer and that the arithmetic operator is the only node following it. It utilizes the StepFuseMath class to update the convolution node's value based on the arithmetic node's value, effectively fusing operations when possible.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\optimizer\\optimizer.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:56:54.424261+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the key steps involved in developing an image classification Android application using the Exynos Neural Network Software Development Kit?",
    "reference_contexts": [
      "This guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (Exynos AI Studio).\nIt outlines the steps for creating an application that utilizes the Exynos AI Studio to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the Exynos AI Studio in their Android applications."
    ],
    "reference": "The key steps involved in developing an image classification Android application using the Exynos Neural Network Software Development Kit include Android project setup, function implementation, and NNC model conversion. The guide aims to equip developers with the necessary knowledge to use the Exynos AI Studio to run neural network models on Samsung Exynos hardware.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:57:15.774139+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How can Exynos AI Studio be utilized in developing an image classification Android application?",
    "reference_contexts": [
      "This guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (Exynos AI Studio).\nIt outlines the steps for creating an application that utilizes the Exynos AI Studio to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the Exynos AI Studio in their Android applications."
    ],
    "reference": "Exynos AI Studio can be utilized in developing an image classification Android application by following a comprehensive guide that outlines the steps for creating an application that runs neural network models on Samsung Exynos hardware. This includes Android project setup, function implementation, and NNC model conversion, equipping developers with the necessary knowledge to effectively use the Exynos AI Studio.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:57:15.774139+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does Samsung Exynos enhance the performance of neural network models?",
    "reference_contexts": [
      "The [Exynos AI Studio](https://soc-developer.semiconductor.samsung.com/enn-sdk) provides a tool for converting [TFLite](https://www.tensorflow.org/lite) neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the [Samsung Exynos](https://semiconductor.samsung.com/processor/) hardware, to ensure optimal performance.\nThe Exynos AI Studio provides a framework that facilitates the execution of NNC models on the Exynos platform."
    ],
    "reference": "Samsung Exynos enhances the performance of neural network models by allowing them to operate efficiently on its hardware through the conversion of TFLite neural network models into Neural Network Container (NNC) format using the Exynos AI Studio.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:57:15.774139+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the role of NNC in the Exynos AI Studio?",
    "reference_contexts": [
      "The [Exynos AI Studio](https://soc-developer.semiconductor.samsung.com/enn-sdk) provides a tool for converting [TFLite](https://www.tensorflow.org/lite) neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the [Samsung Exynos](https://semiconductor.samsung.com/processor/) hardware, to ensure optimal performance.\nThe Exynos AI Studio provides a framework that facilitates the execution of NNC models on the Exynos platform."
    ],
    "reference": "NNC, or Neural Network Container, is a format that allows neural network models to operate efficiently on Samsung Exynos hardware, ensuring optimal performance. The Exynos AI Studio provides a tool for converting TFLite neural network models into NNC format.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:57:15.774139+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What Exynos AI Studio do?",
    "reference_contexts": [
      "The sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the Exynos AI Studio to efficiently execute the NN model on the Exynos platform."
    ],
    "reference": "Exynos AI Studio helps to efficiently execute the NN model on the Exynos platform.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:57:15.774139+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does the Exynos platform enhance the performance of applications that utilize AI models, particularly in the context of object classification from camera feeds?",
    "reference_contexts": [
      "The sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the Exynos AI Studio to efficiently execute the NN model on the Exynos platform."
    ],
    "reference": "The Exynos platform enhances the performance of applications by leveraging the Exynos AI Studio, which efficiently executes the NN model, thereby improving the classification of objects in input from a camera feed or an image.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:57:15.774139+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What model is used in the TFLite example for image classification?",
    "reference_contexts": [
      "This example uses the quantized `Inception v4` TFLite model from [TensorFlow Hub](https://tfhub.dev/tensorflow/lite-model/inception_v4_quant/1/default/1) for image classification."
    ],
    "reference": "The example uses the quantized `Inception v4` TFLite model from TensorFlow Hub for image classification.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:57:15.774139+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain how TensorFlow Hub is utilized in the context of image classifcation using the Inception v4 model?",
    "reference_contexts": [
      "This example uses the quantized `Inception v4` TFLite model from [TensorFlow Hub](https://tfhub.dev/tensorflow/lite-model/inception_v4_quant/1/default/1) for image classification."
    ],
    "reference": "TensorFlow Hub is utilized in this example by providing the quantized Inception v4 TFLite model for image classification, which can be accessed through the specified link.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:57:15.774139+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do you convert a TFLite model to NNC model in the process of implementing an Android application?",
    "reference_contexts": [
      "Implementing sample application involves the following five steps:  \n1. [Starting Android project](getting-started-with-android-samples/starting-android-project): Covers the basics of starting an Android project, from installing Android studio to connecting the ERD board to Android studio.\n1. [Setting necessary UI](getting-started-with-android-samples/setting-necessary-ui): Provides essential information on Android UI view components.\n1. [Function implementation](getting-started-with-android-samples/function-implementation): Explains the functions available in the sample application.\n1. [Exynos AI Studio service](getting-started-with-android-samples/enn-sdk-service): Provides the step-by-step process for converting the TFLite model to NNC model.\n1. [ENN framework](getting-started-with-android-samples/enn-framework): Explains the implementation of the ENN framework in the sample application.  \nThe general workflow of writing and executing an Android application using the Exynos AI Studio is discribed in the following flowchart.\n```mermaid\ngraph TB\nA[Start]\nsubgraph B[Starting Android Project]\nB1[Install Android Studio] --> B2[Create a New Android Project]\nB2 --> B3[Connect the ERD Board to Android Studio]\nend\nsubgraph C[Setting Necessary UI]\nC1[Set up Android UI View Components]\nend\nsubgraph D[Function Implementation]\nD1[Implement UI Functions]\nD1 --> D2[Implement Listener]\nD2 --> D3[Implement Data Processing Functions]\nend\nsubgraph E[ENNTools]\nE1[Use ENNTools to Convert the TFLite Model to the NNC Model]\nend\nsubgraph F[ENN Framework]\nF1[Implement the ENN Framework in the Sample Application]\nend\nG[End]\nA --> B\nB --> C\nC --> D\nD --> E\nE --> F\nF --> G\n```"
    ],
    "reference": "To convert a TFLite model to NNC model, you use ENNTools as described in the function implementation step of the sample application.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:57:15.774139+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you elaborate on the role of ENNTools in the process of implementing a sample Android application?",
    "reference_contexts": [
      "Implementing sample application involves the following five steps:  \n1. [Starting Android project](getting-started-with-android-samples/starting-android-project): Covers the basics of starting an Android project, from installing Android studio to connecting the ERD board to Android studio.\n1. [Setting necessary UI](getting-started-with-android-samples/setting-necessary-ui): Provides essential information on Android UI view components.\n1. [Function implementation](getting-started-with-android-samples/function-implementation): Explains the functions available in the sample application.\n1. [Exynos AI Studio service](getting-started-with-android-samples/enn-sdk-service): Provides the step-by-step process for converting the TFLite model to NNC model.\n1. [ENN framework](getting-started-with-android-samples/enn-framework): Explains the implementation of the ENN framework in the sample application.  \nThe general workflow of writing and executing an Android application using the Exynos AI Studio is discribed in the following flowchart.\n```mermaid\ngraph TB\nA[Start]\nsubgraph B[Starting Android Project]\nB1[Install Android Studio] --> B2[Create a New Android Project]\nB2 --> B3[Connect the ERD Board to Android Studio]\nend\nsubgraph C[Setting Necessary UI]\nC1[Set up Android UI View Components]\nend\nsubgraph D[Function Implementation]\nD1[Implement UI Functions]\nD1 --> D2[Implement Listener]\nD2 --> D3[Implement Data Processing Functions]\nend\nsubgraph E[ENNTools]\nE1[Use ENNTools to Convert the TFLite Model to the NNC Model]\nend\nsubgraph F[ENN Framework]\nF1[Implement the ENN Framework in the Sample Application]\nend\nG[End]\nA --> B\nB --> C\nC --> D\nD --> E\nE --> F\nF --> G\n```"
    ],
    "reference": "ENNTools play a crucial role in the implementation of a sample Android application by facilitating the conversion of the TFLite model to the NNC model. This step is essential in the workflow, which includes starting the Android project, setting up the necessary UI, implementing functions, and integrating the ENN framework.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:57:15.774139+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do I add C++ to a project in Android Studio?",
    "reference_contexts": [
      "1. Right click **Project** panel with **Android** option being selected.  \n<img src=\"./img/cpp1.png\" alt=\"drawing\" width=\"500\"/>  \n1. Select the **Add C++ to Module** option and click OK.  \n<img src=\"./img/cpp2.png\" alt=\"drawing\" width=\"500\"/>"
    ],
    "reference": "To add C++ to a project in Android Studio, right-click the Project panel with the Android option selected, then select the Add C++ to Module option and click OK.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:57:37.900848+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do I add C++ to a module in an Android project?",
    "reference_contexts": [
      "1. Right click **Project** panel with **Android** option being selected.  \n<img src=\"./img/cpp1.png\" alt=\"drawing\" width=\"500\"/>  \n1. Select the **Add C++ to Module** option and click OK.  \n<img src=\"./img/cpp2.png\" alt=\"drawing\" width=\"500\"/>"
    ],
    "reference": "To add C++ to a module in an Android project, right-click on the Project panel with the Android option selected. Then, select the Add C++ to Module option and click OK.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:57:37.900848+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How to use ENN framework for Android development and what steps need to be followed?",
    "reference_contexts": [
      "Download the ENN framework library from [resources](https://soc-developer.semiconductor.samsung.com/support/resource).\nTo load the necessary libraries, perform the folowing steps:  \n1. Modiy Android Manifest to:\n```xml\n<manifest>\n<application>\n...\n<!-- Declare the native library in the Android Manifest -->\n<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />\n...\n</application>\n</manifest>\n```  \n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/app/src/main/jniLibs/arm64-v8a`.\n1. copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/app/src/main/cpp/include`.\n1. Modify `CMakeLists.txt` that is created [here](#adding-c-to-module).  \n```cmake\n# Include the directory where the header files are located\ninclude_directories(include)\n\n# Declare the imported shared library\nadd_library(\nenn_service_so\nSHARED\nIMPORTED\n)\n\n# Set the location of the imported library\nset_target_properties(\nenn_service_so\nPROPERTIES IMPORTED_LOCATION\n${CMAKE_SOURCE_DIR}/../jniLibs/${ANDROID_ABI}/libenn_public_api_ndk_v1.so\n)\n\n# Link the imported library to the target library\ntarget_link_libraries(\n...\nenn_service_so\n...\n)\n```"
    ],
    "reference": "To use the ENN framework for Android development, you need to download the ENN framework library from the provided resources. Then, follow these steps: First, modify the Android Manifest to declare the native library by adding `<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />` within the `<application>` tag. Next, copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/app/src/main/jniLibs/arm64-v8a`. After that, copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/app/src/main/cpp/include`. Finally, modify the `CMakeLists.txt` to include the directory where the header files are located, declare the imported shared library, set the location of the imported library, and link the imported library to the target library.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:57:37.900848+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do you declare the native library libenn_user.samsung_slsi.so in the Android Manifest?",
    "reference_contexts": [
      "Download the ENN framework library from [resources](https://soc-developer.semiconductor.samsung.com/support/resource).\nTo load the necessary libraries, perform the folowing steps:  \n1. Modiy Android Manifest to:\n```xml\n<manifest>\n<application>\n...\n<!-- Declare the native library in the Android Manifest -->\n<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />\n...\n</application>\n</manifest>\n```  \n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/app/src/main/jniLibs/arm64-v8a`.\n1. copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/app/src/main/cpp/include`.\n1. Modify `CMakeLists.txt` that is created [here](#adding-c-to-module).  \n```cmake\n# Include the directory where the header files are located\ninclude_directories(include)\n\n# Declare the imported shared library\nadd_library(\nenn_service_so\nSHARED\nIMPORTED\n)\n\n# Set the location of the imported library\nset_target_properties(\nenn_service_so\nPROPERTIES IMPORTED_LOCATION\n${CMAKE_SOURCE_DIR}/../jniLibs/${ANDROID_ABI}/libenn_public_api_ndk_v1.so\n)\n\n# Link the imported library to the target library\ntarget_link_libraries(\n...\nenn_service_so\n...\n)\n```"
    ],
    "reference": "To declare the native library libenn_user.samsung_slsi.so in the Android Manifest, you need to modify the Android Manifest file to include the following line: `<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />` within the `<application>` tag.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:57:37.900848+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the way to set the ABI in the ENN framework for it to work properly, and what specific ABI does it support?",
    "reference_contexts": [
      "The ENN framework currently supports only the `arm64-v8a` ABI. To set ABI, modify the `build.gradle` file and specify the ABI as `arm64-v8a`.\n```\ndefaultConfig {\n...\nndk {\nabiFilters \"arm64-v8a\"\n}\n...\n}\n```"
    ],
    "reference": "The ENN framework supports only the `arm64-v8a` ABI. To set the ABI, you need to modify the `build.gradle` file and specify the ABI as `arm64-v8a` using the following configuration: `defaultConfig { ... ndk { abiFilters \"arm64-v8a\" } ... }`.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:57:37.900848+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What ABI does the ENN frmaework support?",
    "reference_contexts": [
      "The ENN framework currently supports only the `arm64-v8a` ABI. To set ABI, modify the `build.gradle` file and specify the ABI as `arm64-v8a`.\n```\ndefaultConfig {\n...\nndk {\nabiFilters \"arm64-v8a\"\n}\n...\n}\n```"
    ],
    "reference": "The ENN framework currently supports only the `arm64-v8a` ABI.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:57:37.900848+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does the Java Native Interface facilitate interaction between Java and other programming languages?",
    "reference_contexts": [
      "The Java Native Interface (JNI) is a framework that allows Java code to interact with the code written in other languages such as C or C++. In the sample application, JNI accesses features or libraries such as the ENN framework that are written in C or C++.  \n1. Create `enn_jni.cc` in `cpp` directory.\n1. Modify `CMakeLists.txt` created [here](#adding-c-to-module).  \n```cmake\nadd_library(\nenn_jni\nSHARED\nenn_jni.cc\n)\n\ntarget_link_libraries(\nenn_jni\n)\n```"
    ],
    "reference": "The Java Native Interface (JNI) is a framework that allows Java code to interact with code written in other languages such as C or C++. In a sample application, JNI accesses features or libraries, like the ENN framework, that are written in C or C++.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:57:37.900848+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does the Java Native Interface facilitate interaction between Java and other programming languages, and what is its application in accessing libraries like the ENN framework?",
    "reference_contexts": [
      "The Java Native Interface (JNI) is a framework that allows Java code to interact with the code written in other languages such as C or C++. In the sample application, JNI accesses features or libraries such as the ENN framework that are written in C or C++.  \n1. Create `enn_jni.cc` in `cpp` directory.\n1. Modify `CMakeLists.txt` created [here](#adding-c-to-module).  \n```cmake\nadd_library(\nenn_jni\nSHARED\nenn_jni.cc\n)\n\ntarget_link_libraries(\nenn_jni\n)\n```"
    ],
    "reference": "The Java Native Interface (JNI) is a framework that allows Java code to interact with code written in other languages such as C or C++. In the context of a sample application, JNI is utilized to access features or libraries, like the ENN framework, that are implemented in C or C++. This is achieved by creating a file named `enn_jni.cc` in the `cpp` directory and modifying the `CMakeLists.txt` to include the JNI library, allowing for seamless integration and functionality.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:57:37.900848+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain how JNI is used in the context of the provided code example for model execution?",
    "reference_contexts": [
      "Following function is an example of an implemented JNI wrapper.\nFor more information, refer to the  [Android Developer Documentation](https://developer.android.com/ndk/samples/sample_hellojni).  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/cpp/enn_jni.cc#L57)):\n```cpp\nextern \"C\"\nJNIEXPORT jlong JNICALL\nJava_com_samsung_imageclassification_executor_ModelExecutor_ennOpenModel(\nJNIEnv *env,\njobject thiz,\njstring j_filename\n) {\nEnnModelId model_id;\nconst char *filename = env->GetStringUTFChars(j_filename, 0);\n\nif (enn::api::EnnOpenModel(filename, &model_id)) {\n__android_log_print(ANDROID_LOG_ERROR, LOG_TAG, \"EnnOpenModel of [%s] Failed\", filename);\n}\n\nreturn static_cast<jlong>(model_id);\n}\n```"
    ],
    "reference": "The provided code example demonstrates the implementation of a JNI wrapper, specifically the function 'Java_com_samsung_imageclassification_executor_ModelExecutor_ennOpenModel'. This function uses JNI to interact with Java code, allowing the native C++ function to be called from Java. It takes a filename as a parameter, opens a model using the 'enn::api::EnnOpenModel' function, and returns a model ID. If the model opening fails, it logs an error message.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:57:37.900848+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What GitHub link show JNI wrapper example?",
    "reference_contexts": [
      "Following function is an example of an implemented JNI wrapper.\nFor more information, refer to the  [Android Developer Documentation](https://developer.android.com/ndk/samples/sample_hellojni).  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/cpp/enn_jni.cc#L57)):\n```cpp\nextern \"C\"\nJNIEXPORT jlong JNICALL\nJava_com_samsung_imageclassification_executor_ModelExecutor_ennOpenModel(\nJNIEnv *env,\njobject thiz,\njstring j_filename\n) {\nEnnModelId model_id;\nconst char *filename = env->GetStringUTFChars(j_filename, 0);\n\nif (enn::api::EnnOpenModel(filename, &model_id)) {\n__android_log_print(ANDROID_LOG_ERROR, LOG_TAG, \"EnnOpenModel of [%s] Failed\", filename);\n}\n\nreturn static_cast<jlong>(model_id);\n}\n```"
    ],
    "reference": "The GitHub link that shows the JNI wrapper example is https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/cpp/enn_jni.cc#L57.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:57:37.900848+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you tell me what BufferSetInfo is and what it does in the context of the application?",
    "reference_contexts": [
      "Following classes are required to create the sample application:  \n- `app/java/package/executor`\n- `ModelExecutor.kt`: Includes methods for processing images and return classification results.\n- `app/java/package/fragments`\n- `CameraFragment.kt`: Handles user interactions and updates the UI in Camera mode.\n- `ImageFragment.kt`: Handles user interactions and updates the UI in Image mode.\n- `app/java/package/enn_type`\n- `BufferSetInfo`: Data class that holds information about the buffer set and number of input/output layers"
    ],
    "reference": "BufferSetInfo is a data class that holds information about the buffer set and the number of input/output layers.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\function-implementation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:58:04.612144+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What does the onResults method do in the ExecutorListener interface?",
    "reference_contexts": [
      "The `ExecutorListener` interface in `ModelExecutor.kt` is a custom listener that provides two methods such as `onError` and `onResults`.  \n- `onError`: Triggered when an error occurs during image classification. It logs the error or displays an error message.\n- `onResults`: Triggered when image classification is successfully completed. It updates the UI with the classification results and the time taken for classification.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/executor/ModelExecutor.kt#L258)):\n```kotlin\ninterface ExecutorListener {\nfun onError(error: String)\nfun onResults(\nresult: Map<String, Float>, inferenceTime: Long\n)\n}\n```  \nImplement the interface in the `fragment` class.\nThe `onError` method logs the error and the `onResults` method updates the UI.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L185)):  \n```kotlin\nclass SampleFragment : Fragment(), ImageClassifierHelper.ClassifierListener {\n...\noverride fun onError(error: String) {\nLog.e(TAG, \"ModelExecutor error: $error\")\n}\n\noverride fun onResults(\nresult: Map<String, Float>, inferenceTime: Long\n) {\nactivity?.runOnUiThread {\nbinding.processData.inferenceTime.text = \"$inferenceTime ms\"\nupdateUI(result)\n}\n}\n}\n```"
    ],
    "reference": "The onResults method is triggered when image classification is successfully completed. It updates the UI with the classification results and the time taken for classification.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\function-implementation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:58:04.612144+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does the process method work in the ModelExecutor class?",
    "reference_contexts": [
      "Create the `ModelExecutor` object iin the `fragment` class with the current context and the fragment as `executorListener`. The `process` method of `modelExecutor` is called to start the image classification.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L53)):\n```kotlin\nmodelExecutor = ModelExecutor(\ncontext = requireContext(), executorListener = this\n)\n...\nmodelExecutor.process(bitmapBuffer)\n```  \nIn the `ModelExecutor.kt` class, the `process` method processes the image and calls the `onResults` method of `executorListener` to pass the results back to the fragment.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/executor/ModelExecutor.kt#L76)):\n```kotlin\nfun process(image: Bitmap) {\n...\n\nexecutorListener?.onResults(\nresult, inferenceTime\n)\n}\n```"
    ],
    "reference": "The process method in the ModelExecutor class processes the image and calls the onResults method of executorListener to pass the results back to the fragment.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\function-implementation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:58:04.612144+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does binding.viewFinder relate to the setImageAnalyzer function?",
    "reference_contexts": [
      "The `setImageAnalyzer` function sets an `ImageAnalysis` object to process the camera feed.\nIt creates a bitmap buffer and processes each image frame.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L100)):\n```kotlin\nprivate fun setImageAnalyzer() {\nimageAnalyzer =\nImageAnalysis.Builder()\n.setTargetRotation(binding.viewFinder.display.rotation)\n.setBackpressureStrategy(ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST)\n.setOutputImageFormat(ImageAnalysis.OUTPUT_IMAGE_FORMAT_RGBA_8888)\n.build().also {\nit.setAnalyzer(cameraExecutor) { image ->\nif (!::bitmapBuffer.isInitialized) {\nbitmapBuffer = Bitmap.createBitmap(\nimage.width, image.height, Bitmap.Config.ARGB_8888\n)\n}\nprocess(image)\n}\n}\n}\n```  \nThe `setCamera` function introduced [here](getting-started-with-android-samples/setting-necessary-ui#camera-preview) is updated to include the `setImageAnalyzer` method.\nThis inclusion allows the camera feed to be analyzed and processed.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L65)):\n```kotlin\ncameraProviderFuture.addListener(\n{\n...\nsetPreview()\nsetImageAnalyzer()\n\ntry {\ncameraProvider.unbindAll()\ncamera = cameraProvider.bindToLifecycle(\nthis, cameraSelector, preview, imageAnalyzer\n)\npreview?.setSurfaceProvider(binding.viewFinder.surfaceProvider)\n}\n...\n},\n...\n)\n```"
    ],
    "reference": "The binding.viewFinder is used in the setImageAnalyzer function to set the target rotation for the ImageAnalysis object, which processes the camera feed.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\function-implementation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:58:04.612144+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the role of ImageView in displaying images retrieved from the device media?",
    "reference_contexts": [
      "The `ActivityResult` object `getContent` retrieves an image from the device media.\nThe selected image is displayed in an ImageView and converted to a bitmap.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/ImageFragment.kt#L28)):\n```kotlin\nprivate val getContent =\nregisterForActivityResult(ActivityResultContracts.GetContent()) { uri: Uri? ->\nuri?.let {\nbinding.imageView.setImageURI(it)\nbinding.buttonProcess.isEnabled = true\n\nbitmapBuffer = ImageDecoder.decodeBitmap(\nImageDecoder.createSource(\nrequireContext().contentResolver,\nit\n)\n) { decoder, _, _ ->\ndecoder.setTargetColorSpace(ColorSpace.get(ColorSpace.Named.SRGB))\ndecoder.allocator = ImageDecoder.ALLOCATOR_SOFTWARE\ndecoder.setTargetSampleSize(1)\n}\n}\n}\n```  \nClick **Load** to launch the `ActivityResult`.\nIt allows the user to select an image from their device.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/ImageFragment.kt#L69)):\n```kotlin\nbinding.buttonLoad.setOnClickListener {\ngetContent.launch(\"image/*\")\n}\n```"
    ],
    "reference": "The ImageView is used to display the selected image retrieved from the device media. After an image is selected, it is converted to a bitmap and set to the ImageView using the method `binding.imageView.setImageURI(it)`.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\function-implementation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:58:04.612144+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the significance of GitHub in the context of copying the NNC model file for the ENN Framework?",
    "reference_contexts": [
      "Use `copyNNCFromAssetsToInternalStorage` function to copy the NNC model file from the asset directory of app to its internal storage.\nIt is necessary to copy the NNC model file because the model file needs to be accessed from the internal storage when used by the ENN Framework.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/executor/ModelExecutor.kt#L231)):\n```kotlin\nprivate fun copyNNCFromAssetsToInternalStorage(filename: String) {\ntry {\nval inputStream = context.assets.open(filename)\nval outputFile = File(context.filesDir, filename)\nval outputStream = FileOutputStream(outputFile)\nval buffer = ByteArray(2048)\nvar bytesRead: Int\n\nwhile (inputStream.read(buffer).also { bytesRead = it } != -1) {\noutputStream.write(buffer, 0, bytesRead)\n}\ninputStream.close()\noutputStream.close()\n} catch (e: IOException) {\ne.printStackTrace()\n}\n}\n```"
    ],
    "reference": "The context does not explicitly mention the significance of GitHub in relation to copying the NNC model file for the ENN Framework. However, it provides a link to an example on GitHub that demonstrates the usage of the `copyNNCFromAssetsToInternalStorage` function, which is relevant for understanding how to implement the copying process in the application.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\function-implementation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:58:04.612144+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How is Kotlin used in the context of the BufferSetInfo data class for neural network models?",
    "reference_contexts": [
      "The `BufferSetInfo` data class holds the information about the buffer set used in the neural network model.\nIt includes the memory location of the buffer set (`buffer_set`), the number of input buffers (`n_in_buf`), and the number of output buffers (`n_out_buf`).\nUse the data class to return this information from the JNI library.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/enn_type/BufferSetInfo.kt)):\n```kotlin\nclass BufferSetInfo {\nvar buffer_set: Long = 0\nvar n_in_buf: Int = 0\nvar n_out_buf: Int = 0\n}\n```"
    ],
    "reference": "In the context of the BufferSetInfo data class, Kotlin is used to define a class that holds information about the buffer set used in neural network models. The class includes properties for the memory location of the buffer set (`buffer_set`), the number of input buffers (`n_in_buf`), and the number of output buffers (`n_out_buf`).",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\function-implementation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:58:04.612144+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "In the context of Android application development, what role does the TextView play in the user interface, particularly regarding the display of classification results?",
    "reference_contexts": [
      "- **ImageView**: Displays the image that must be classified.\n- **Image Load Button**: Triggers the loading of an image.\n- **Inference Button**: Initiates the image classification process.\n- **TextView**: Displays the classification result and score."
    ],
    "reference": "The TextView is responsible for displaying the classification result and score within the user interface of the Android application.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\setting-necessary-ui.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:58:27.725436+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What does the TextView do?",
    "reference_contexts": [
      "- **ImageView**: Displays the image that must be classified.\n- **Image Load Button**: Triggers the loading of an image.\n- **Inference Button**: Initiates the image classification process.\n- **TextView**: Displays the classification result and score."
    ],
    "reference": "TextView displays the classification result and score.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\setting-necessary-ui.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:58:27.725436+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How I can use app/res/layout/*.xml for showing classification result and score in my Android app?",
    "reference_contexts": [
      "The TextView displays the classification result and score to the user.\nIn the layout XML file, define a TextView with a unique ID.\nIn the Kotlin file, use this ID to reference the TextView and set its text to the classification result and score.  \n- Layout `app/res/layout/*.xml` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/res/layout/enn_info.xml#L18)):\n```xml\n<TextView\nandroid:id=\"@+id/sample_text\" />\n```  \n- Kotlin `app/java/package/fragments/Fragment.kt` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L194)):\n```kotlin\nbinding.sampleText.text = \"Sample Text\"\n```"
    ],
    "reference": "In the layout XML file located at app/res/layout/*.xml, you need to define a TextView with a unique ID. For example, you can use the following XML code: `<TextView android:id=\"@+id/sample_text\" />`. Then, in your Kotlin file, you can reference this TextView using its ID and set its text to display the classification result and score, like this: `binding.sampleText.text = \"Sample Text\"`.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\setting-necessary-ui.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:58:27.725436+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How is a TextView used in an Android application?",
    "reference_contexts": [
      "The TextView displays the classification result and score to the user.\nIn the layout XML file, define a TextView with a unique ID.\nIn the Kotlin file, use this ID to reference the TextView and set its text to the classification result and score.  \n- Layout `app/res/layout/*.xml` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/res/layout/enn_info.xml#L18)):\n```xml\n<TextView\nandroid:id=\"@+id/sample_text\" />\n```  \n- Kotlin `app/java/package/fragments/Fragment.kt` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L194)):\n```kotlin\nbinding.sampleText.text = \"Sample Text\"\n```"
    ],
    "reference": "A TextView is used to display the classification result and score to the user. In the layout XML file, it is defined with a unique ID, and in the Kotlin file, this ID is used to reference the TextView and set its text to the classification result and score.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\setting-necessary-ui.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:58:27.725436+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do you use an ImigeView in an Android application?",
    "reference_contexts": [
      "The ImageView displays the image that is classified.\nIn the layout XML file, define an ImageView with a unique ID.\nIn the Kotlin file, use this ID to reference the ImageView and set its image to the selected image.  \n- Layout `app/res/layout/*.xml` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/res/layout/fragment_image.xml#L19)):\n```xml\n<ImageView\nandroid:id=\"@+id/sample_image\" />\n```  \n- Kotlin `app/java/package/fragments/Fragment.kt` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/ImageFragment.kt#L41)):\n```kotlin\nbinding.imageView.setImageBitmap(resizedImage)\n```"
    ],
    "reference": "To use an ImageView in an Android application, define it in the layout XML file with a unique ID, such as `<ImageView android:id=\"@+id/sample_image\" />`. In the Kotlin file, reference this ID to set the image using `binding.imageView.setImageBitmap(resizedImage)`.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\setting-necessary-ui.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:58:27.725436+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How to use ImageView for show image in app?",
    "reference_contexts": [
      "The ImageView displays the image that is classified.\nIn the layout XML file, define an ImageView with a unique ID.\nIn the Kotlin file, use this ID to reference the ImageView and set its image to the selected image.  \n- Layout `app/res/layout/*.xml` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/res/layout/fragment_image.xml#L19)):\n```xml\n<ImageView\nandroid:id=\"@+id/sample_image\" />\n```  \n- Kotlin `app/java/package/fragments/Fragment.kt` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/ImageFragment.kt#L41)):\n```kotlin\nbinding.imageView.setImageBitmap(resizedImage)\n```"
    ],
    "reference": "The ImageView displays the image that is classified. In the layout XML file, define an ImageView with a unique ID. In the Kotlin file, use this ID to reference the ImageView and set its image to the selected image.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\setting-necessary-ui.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:58:27.725436+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is a Fragmnt in the context of setting up a camera preview?",
    "reference_contexts": [
      "The PreviewView displays a live camera feed for real-time classification.\nIn the layout XML file, define a PreviewView with a unique ID.\nIn the Kotlin file, use this ID to reference the PreviewView and set up the camera preview.\nThis process involves creating a Preview object, binding it to the lifecycle of the Fragment, and setting its surface provider to the surface provider of PreviewView.  \n- Layout `app/res/layout/*.xml` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/res/layout/fragment_camera.xml#L9)):\n```xml\n<androidx.camera.view.PreviewView\nandroid:id=\"@+id/view_finder\" />\n```  \n- Kotlin `app/java/package/fragments/Fragment.kt` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L61)):\n```kotlin\nprivate fun setCamera() {\ncameraExecutor = Executors.newSingleThreadExecutor()\n\nval cameraProviderFuture = ProcessCameraProvider.getInstance(requireContext())\ncameraProviderFuture.addListener(\n{\nval cameraProvider = cameraProviderFuture.get()\nval cameraSelector = CameraSelector.DEFAULT_BACK_CAMERA\n\nsetPreview()\nsetImageAnalyzer()\n\ntry {\ncameraProvider.unbindAll()\ncamera = cameraProvider.bindToLifecycle(\nthis, cameraSelector, preview, imageAnalyzer\n)\n\npreview?.setSurfaceProvider(binding.viewFinder.surfaceProvider)\n} catch (exc: java.lang.Exception) {\nLog.e(TAG, \"Camera binding failed\", exc)\n}\n}, ContextCompat.getMainExecutor(requireContext())\n)\n}\n\nprivate fun setPreview() {\npreview = Preview.Builder().setTargetRotation(binding.viewFinder.display.rotation).build()\n}\n```"
    ],
    "reference": "In the context of setting up a camera preview, a Fragment is used to bind the camera lifecycle to the application. The camera preview is set up by creating a Preview object and binding it to the lifecycle of the Fragment, allowing for real-time camera feed display.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\setting-necessary-ui.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:58:27.725436+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How is the PreviewView utilized in an Android application for camera functionality?",
    "reference_contexts": [
      "The PreviewView displays a live camera feed for real-time classification.\nIn the layout XML file, define a PreviewView with a unique ID.\nIn the Kotlin file, use this ID to reference the PreviewView and set up the camera preview.\nThis process involves creating a Preview object, binding it to the lifecycle of the Fragment, and setting its surface provider to the surface provider of PreviewView.  \n- Layout `app/res/layout/*.xml` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/res/layout/fragment_camera.xml#L9)):\n```xml\n<androidx.camera.view.PreviewView\nandroid:id=\"@+id/view_finder\" />\n```  \n- Kotlin `app/java/package/fragments/Fragment.kt` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L61)):\n```kotlin\nprivate fun setCamera() {\ncameraExecutor = Executors.newSingleThreadExecutor()\n\nval cameraProviderFuture = ProcessCameraProvider.getInstance(requireContext())\ncameraProviderFuture.addListener(\n{\nval cameraProvider = cameraProviderFuture.get()\nval cameraSelector = CameraSelector.DEFAULT_BACK_CAMERA\n\nsetPreview()\nsetImageAnalyzer()\n\ntry {\ncameraProvider.unbindAll()\ncamera = cameraProvider.bindToLifecycle(\nthis, cameraSelector, preview, imageAnalyzer\n)\n\npreview?.setSurfaceProvider(binding.viewFinder.surfaceProvider)\n} catch (exc: java.lang.Exception) {\nLog.e(TAG, \"Camera binding failed\", exc)\n}\n}, ContextCompat.getMainExecutor(requireContext())\n)\n}\n\nprivate fun setPreview() {\npreview = Preview.Builder().setTargetRotation(binding.viewFinder.display.rotation).build()\n}\n```"
    ],
    "reference": "The PreviewView is utilized to display a live camera feed for real-time classification. In the layout XML file, a PreviewView is defined with a unique ID, which is then referenced in the Kotlin file to set up the camera preview. This involves creating a Preview object, binding it to the lifecycle of the Fragment, and setting its surface provider to the surface provider of the PreviewView.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\setting-necessary-ui.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:58:27.725436+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What role does Kotlin play in Android development?",
    "reference_contexts": [
      "Before you proceed, ensure you have a basic understanding of the following Android development concepts:  \n- **Kotlin**: The primary programming language for Android development.\n- [**View Binding**](https://developer.android.com/topic/libraries/view-binding): A feature that allows you to easily write code that interacts with views.\n- [**Fragments**](https://developer.android.com/guide/fragments): A reusable piece of an user interface or behavior of Android application.\n- [**Navigation Component**](https://developer.android.com/guide/navigation): A component that helps to implement navigation.  \nIf you are new to Android, it is recommended to review Chapters 1 to 3 of the [Android Basics in Kotlin](https://developer.android.com/courses/android-basics-kotlin/course) course.\nThis course describes the basics of developing Android application using Kotlin."
    ],
    "reference": "Kotlin is the primary programming language for Android development.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:58:49.754118+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are some essential concepts to understand before starting Android development?",
    "reference_contexts": [
      "Before you proceed, ensure you have a basic understanding of the following Android development concepts:  \n- **Kotlin**: The primary programming language for Android development.\n- [**View Binding**](https://developer.android.com/topic/libraries/view-binding): A feature that allows you to easily write code that interacts with views.\n- [**Fragments**](https://developer.android.com/guide/fragments): A reusable piece of an user interface or behavior of Android application.\n- [**Navigation Component**](https://developer.android.com/guide/navigation): A component that helps to implement navigation.  \nIf you are new to Android, it is recommended to review Chapters 1 to 3 of the [Android Basics in Kotlin](https://developer.android.com/courses/android-basics-kotlin/course) course.\nThis course describes the basics of developing Android application using Kotlin."
    ],
    "reference": "Before starting Android development, it is essential to have a basic understanding of the following concepts: Kotlin, which is the primary programming language for Android development; View Binding, a feature that allows you to easily write code that interacts with views; Fragments, which are reusable pieces of user interface or behavior in an Android application; and the Navigation Component, which helps implement navigation. Additionally, it is recommended to review Chapters 1 to 3 of the Android Basics in Kotlin course for newcomers.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:58:49.754118+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do I download and install Android Studio?",
    "reference_contexts": [
      "1. **To Download Android studio**, visit the [official website](https://developer.android.com/studio) and click **Download Android Studio**.\n1. **To install and set up Android Studio**, execute the downloaded file and follow the instructions of installation wizard.  \n> For more information, such as requirement for each OS, refer to [Install Android Studio](https://developer.android.com/studio/install) from Android Developers.\n1. Add the path to the Platform Tools directory in environment variables\n- Windows\n1. Navigate to Start > Control Panel > System > Advanced System Settings > Environment Variables.\n1. Select `Path` User variable, then select **Edit**.\n1. Select New and add `%USERPROFILE%\\AppData\\Local\\Android\\sdk\\platform-tools` directory.\n1. Select **Ok** to close all the settings windows.S  \n<img src=\"./img/windows-env.jpg\" alt=\"drawing\" width=\"400\"/>"
    ],
    "reference": "To download Android Studio, visit the official website and click 'Download Android Studio'. To install and set up Android Studio, execute the downloaded file and follow the instructions of the installation wizard.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:58:49.754118+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do I access Advanced System Settings to add the Android SDK path in Windows for my development environment?",
    "reference_contexts": [
      "1. **To Download Android studio**, visit the [official website](https://developer.android.com/studio) and click **Download Android Studio**.\n1. **To install and set up Android Studio**, execute the downloaded file and follow the instructions of installation wizard.  \n> For more information, such as requirement for each OS, refer to [Install Android Studio](https://developer.android.com/studio/install) from Android Developers.\n1. Add the path to the Platform Tools directory in environment variables\n- Windows\n1. Navigate to Start > Control Panel > System > Advanced System Settings > Environment Variables.\n1. Select `Path` User variable, then select **Edit**.\n1. Select New and add `%USERPROFILE%\\AppData\\Local\\Android\\sdk\\platform-tools` directory.\n1. Select **Ok** to close all the settings windows.S  \n<img src=\"./img/windows-env.jpg\" alt=\"drawing\" width=\"400\"/>"
    ],
    "reference": "To access Advanced System Settings in Windows, navigate to Start > Control Panel > System > Advanced System Settings. From there, you can modify the environment variables, including adding the path to the Platform Tools directory.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:58:49.754118+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How can one access the Build number on the ERD board to enable Developer Mode?",
    "reference_contexts": [
      "Enabling Developer Mode in ERD board\n1. Open the **Settings** app.\n2. Scroll down and tap **About phone**.\n3. Find the **Build number** and tap it a few times until the **You are now a developer** message appears.  \nEnabling USB Debugging in ERD board\n1. Navigate to the main **Settings** screen.\n2. Scroll down and tap **System**.\nThe **Developer options** is now displayed.\n3. Tap **Developer options**, then scroll down and turn on **USB debugging**."
    ],
    "reference": "To access the Build number on the ERD board, open the Settings app, scroll down and tap About phone, then find the Build number and tap it a few times until the You are now a developer message appears.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:58:49.754118+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What steps must be taken to enable Developer Mode on the ERD board, and what message confirms this action?",
    "reference_contexts": [
      "Enabling Developer Mode in ERD board\n1. Open the **Settings** app.\n2. Scroll down and tap **About phone**.\n3. Find the **Build number** and tap it a few times until the **You are now a developer** message appears.  \nEnabling USB Debugging in ERD board\n1. Navigate to the main **Settings** screen.\n2. Scroll down and tap **System**.\nThe **Developer options** is now displayed.\n3. Tap **Developer options**, then scroll down and turn on **USB debugging**."
    ],
    "reference": "To enable Developer Mode on the ERD board, you need to open the **Settings** app, scroll down and tap **About phone**, then find the **Build number** and tap it a few times until the **You are now a developer** message appears.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:58:49.754118+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What steps should an aspiring Android developer take to ensure that Android Studio detects their device properly during the setup process?",
    "reference_contexts": [
      "1. Connect the ERD board to your computer using a USB cable.\n1. In the pop-up that appears, select **Allow** to enable the USB debugging.\n1. Android Studio automatically detects the device. If the device is not detected, enable **File transfer** on the device."
    ],
    "reference": "To ensure that Android Studio detects your device properly, first connect the ERD board to your computer using a USB cable. When a pop-up appears, select **Allow** to enable USB debugging. If Android Studio does not automatically detect the device, make sure to enable **File transfer** on the device.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:58:49.754118+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How can I ensure that Android Studio detects my device during setup?",
    "reference_contexts": [
      "1. Connect the ERD board to your computer using a USB cable.\n1. In the pop-up that appears, select **Allow** to enable the USB debugging.\n1. Android Studio automatically detects the device. If the device is not detected, enable **File transfer** on the device."
    ],
    "reference": "To ensure that Android Studio detects your device during setup, connect the ERD board to your computer using a USB cable. When the pop-up appears, select **Allow** to enable USB debugging. If the device is not detected, make sure to enable **File transfer** on the device.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:58:49.754118+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Wht hapens at 2023/11/10 21:43:36 during the ERD board update?",
    "reference_contexts": [
      "***<span style=\"color:red\">WARNING:</span>* Updating ERD board binary results in *erasing all data* on ERD board.**  \nTo update the binary of an ERD board (Windows):\n1. Download ERD board binary from [resources](https:/./soc-developer.semiconductor.samsung.com/support/resource).\n1. Extract the contents of the downloaded `zip` file.\n1. Enable USB debugging mode and connect the device as demonstrated [here](#configuring-the-erd-board).\n1. Boot ERD board to `fastboot` bootloader mode.\n```bash\nadb reboot bootloader\n```\n1. Check if ERD board is ready to flash by executing:\n```bash\nfastboot devices\n```\n1. From the extracted files, find and execute `ff_erd9925_all.exe`.\n1. Press any key to continue.\n1. After `ff_erd9925_all.exe` is executed, the ERD Board reboots automatically and the following message appears.\n```shell\n=======================================\nFINISHED\n2023/11/10 21:43:36\nSUCCESS 1/1 fastboot devices\n0) 00000a8fcf39b308, SUCCESS, elapsed=168s\n=======================================\n```"
    ],
    "reference": "At 2023/11/10 21:43:36, the message 'FINISHED' appears, indicating that the update process for the ERD board has been completed successfully, with a report of 'SUCCESS 1/1 fastboot devices'.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:58:49.754118+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Wut is the siginificance of 2023/11/10 21:43:36 in the ERD board update process?",
    "reference_contexts": [
      "***<span style=\"color:red\">WARNING:</span>* Updating ERD board binary results in *erasing all data* on ERD board.**  \nTo update the binary of an ERD board (Windows):\n1. Download ERD board binary from [resources](https:/./soc-developer.semiconductor.samsung.com/support/resource).\n1. Extract the contents of the downloaded `zip` file.\n1. Enable USB debugging mode and connect the device as demonstrated [here](#configuring-the-erd-board).\n1. Boot ERD board to `fastboot` bootloader mode.\n```bash\nadb reboot bootloader\n```\n1. Check if ERD board is ready to flash by executing:\n```bash\nfastboot devices\n```\n1. From the extracted files, find and execute `ff_erd9925_all.exe`.\n1. Press any key to continue.\n1. After `ff_erd9925_all.exe` is executed, the ERD Board reboots automatically and the following message appears.\n```shell\n=======================================\nFINISHED\n2023/11/10 21:43:36\nSUCCESS 1/1 fastboot devices\n0) 00000a8fcf39b308, SUCCESS, elapsed=168s\n=======================================\n```"
    ],
    "reference": "The timestamp 2023/11/10 21:43:36 indicates the successful completion of the ERD board update process, as shown in the message that appears after executing `ff_erd9925_all.exe`.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:58:49.754118+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the purpose of Exynos AI Stodio in developing native programs?",
    "reference_contexts": [
      "This guide provides a detailed walkthrough for developing a native program using the Exynos Neural Network Software Development Kit (Exynos AI Studio).\nIt describes the steps for developing a native program that leverages the Exynos AI Studio to execute neural network models on Samsung Exynos hardware within the adb shell environment.\nThe guide aims at equipping developers with the necessary knowledge so that they can create native binaries for testing their models without having to create an Android application."
    ],
    "reference": "Exynos AI Studio provides a detailed walkthrough for developing a native program that leverages the Exynos Neural Network Software Development Kit to execute neural network models on Samsung Exynos hardware within the adb shell environment.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:08.841092+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain how to use the Samsung Exynos for developing native programs with the Exynos AI Studio?",
    "reference_contexts": [
      "This guide provides a detailed walkthrough for developing a native program using the Exynos Neural Network Software Development Kit (Exynos AI Studio).\nIt describes the steps for developing a native program that leverages the Exynos AI Studio to execute neural network models on Samsung Exynos hardware within the adb shell environment.\nThe guide aims at equipping developers with the necessary knowledge so that they can create native binaries for testing their models without having to create an Android application."
    ],
    "reference": "The guide provides a detailed walkthrough for developing a native program using the Exynos Neural Network Software Development Kit (Exynos AI Studio). It describes the steps for leveraging the Exynos AI Studio to execute neural network models on Samsung Exynos hardware within the adb shell environment. The aim is to equip developers with the necessary knowledge to create native binaries for testing their models without needing to create an Android application.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:08.841092+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain how NNC format models are made and why they are important for running neural networks on Samsung Exynos hardware?",
    "reference_contexts": [
      "The [Exynos AI Studio](https://soc-developer.semiconductor.samsung.com/enn-sdk) tool facilitates the conversion of [TFLite](https://www.tensorflow.org/lite) neural network models into NNC format models.\nThis conversion enables the NN models to execute efficiently on the [Samsung Exynos](https://semiconductor.samsung.com/processor/) hardware to ensure optimal performance.\nThe Exynos AI Studio provides a framework that facilitates the execution of NNC models on the Exynos platform."
    ],
    "reference": "The NNC format models are created by converting TFLite neural network models using the Exynos AI Studio tool. This conversion is important because it allows the neural network models to execute efficiently on Samsung Exynos hardware, ensuring optimal performance.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:08.841092+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does the Samsung Exynos enhance the performance of neural network models?",
    "reference_contexts": [
      "The [Exynos AI Studio](https://soc-developer.semiconductor.samsung.com/enn-sdk) tool facilitates the conversion of [TFLite](https://www.tensorflow.org/lite) neural network models into NNC format models.\nThis conversion enables the NN models to execute efficiently on the [Samsung Exynos](https://semiconductor.samsung.com/processor/) hardware to ensure optimal performance.\nThe Exynos AI Studio provides a framework that facilitates the execution of NNC models on the Exynos platform."
    ],
    "reference": "The Samsung Exynos enhances the performance of neural network models by enabling the efficient execution of NNC format models, which are converted from TFLite neural network models using the Exynos AI Studio tool. This ensures optimal performance on the Exynos hardware.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:08.841092+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What resources are available in the enn-sdk-samples-9925 repository?",
    "reference_contexts": [
      "In this sample, a converted NNC file and raw input/output file available in the [Github Repository](https://github.com/exynos-eco/enn-sdk-samples-9925) are used."
    ],
    "reference": "In the enn-sdk-samples-9925 repository, a converted NNC file and a raw input/output file are available.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:08.841092+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Where can I find exynos-eco files?",
    "reference_contexts": [
      "In this sample, a converted NNC file and raw input/output file available in the [Github Repository](https://github.com/exynos-eco/enn-sdk-samples-9925) are used."
    ],
    "reference": "You can find the exynos-eco files in the Github Repository at https://github.com/exynos-eco/enn-sdk-samples-9925.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:08.841092+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How can I execute the sample native program using the nnc-model-tester?",
    "reference_contexts": [
      "To execute the sample native program, refer to [README](https://github.com/exynos-eco/enn-sdk-samples-9925/tree/main/nnc-model-tester#readme) file."
    ],
    "reference": "To execute the sample native program, refer to the README file available at the provided GitHub link.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:08.841092+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do I find the README file to execute the sample native program?",
    "reference_contexts": [
      "To execute the sample native program, refer to [README](https://github.com/exynos-eco/enn-sdk-samples-9925/tree/main/nnc-model-tester#readme) file."
    ],
    "reference": "To execute the sample native program, refer to the README file.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:08.841092+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What NDK do in native program?",
    "reference_contexts": [
      "This guide comprises the following sections:\n1. [**Writing Native Program**](getting-started-with-native-samples/writing-native-program): This section provides the process of writing a C++ program to implement the ENN framework.\n1. [**Compiling Using NDK**](getting-started-with-native-samples/compiling-using-ndk): This section provides the step-by-step process to compile the native program using NDK.\n1. [**Using ADB to Execute Native Program**](getting-started-with-native-samples/using-adb): This section explains the method to execute the native program using ADB.  \nThe general workflow of writing and executing a native program using the Exynos AI Studio is described in the following flowchart.  \n```mermaid\ngraph TB\n\nA[Start]\nB[Write Native Program]\nsubgraph C[Android Native Development Kit]\nC1[Create Makefile]-->C2[Compile Native Program]\nend\nsubgraph D[Android Debug Bridge]\nD1[Push Native Program and Data]--> D2[Execute Native Program]\nend\nE[End]\nA-->B-->C-->D-->E\n\n```"
    ],
    "reference": "NDK is used to compile the native program in the process of writing a C++ program to implement the ENN framework.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:08.841092+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the role of NDK in compiling a native program?",
    "reference_contexts": [
      "This guide comprises the following sections:\n1. [**Writing Native Program**](getting-started-with-native-samples/writing-native-program): This section provides the process of writing a C++ program to implement the ENN framework.\n1. [**Compiling Using NDK**](getting-started-with-native-samples/compiling-using-ndk): This section provides the step-by-step process to compile the native program using NDK.\n1. [**Using ADB to Execute Native Program**](getting-started-with-native-samples/using-adb): This section explains the method to execute the native program using ADB.  \nThe general workflow of writing and executing a native program using the Exynos AI Studio is described in the following flowchart.  \n```mermaid\ngraph TB\n\nA[Start]\nB[Write Native Program]\nsubgraph C[Android Native Development Kit]\nC1[Create Makefile]-->C2[Compile Native Program]\nend\nsubgraph D[Android Debug Bridge]\nD1[Push Native Program and Data]--> D2[Execute Native Program]\nend\nE[End]\nA-->B-->C-->D-->E\n\n```"
    ],
    "reference": "NDK is used in the process of compiling the native program, as detailed in the section 'Compiling Using NDK', which provides a step-by-step process for this task.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:08.841092+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the main steps involved in using the ADB with the ERD board?",
    "reference_contexts": [
      "This section provides the detailed information on using the ADB to execute the native program on the ERD board.\nThis process comprises of two main steps such as copying data to the board and executing the native program on the ERD board."
    ],
    "reference": "The main steps involved in using the ADB with the ERD board are copying data to the board and executing the native program on the ERD board.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:22.375430+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Wht is the prpose of ADB in executing programs on the ERD bord?",
    "reference_contexts": [
      "This section provides the detailed information on using the ADB to execute the native program on the ERD board.\nThis process comprises of two main steps such as copying data to the board and executing the native program on the ERD board."
    ],
    "reference": "The ADB is used to execute the native program on the ERD board, which involves two main steps: copying data to the board and executing the native program.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:22.375430+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do you use ADB to run a program on the ERD board?",
    "reference_contexts": [
      "This section provides the detailed information on using the ADB to execute the native program on the ERD board.\nThis process comprises of two main steps such as copying data to the board and executing the native program on the ERD board."
    ],
    "reference": "To use ADB to execute a native program on the ERD board, you need to follow two main steps: first, copy the data to the board, and then execute the native program on the ERD board.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:22.375430+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What role does the input.bin file play in the process of copying files to the ERD board?",
    "reference_contexts": [
      "The following commands copy the necessary files to the ERD board:  \n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/* /data/local/tmp/\n```  \nThese commands perform the following actions:  \n- `adb push` is used to copy the model file (`model.nnc`), input data file (`input.bin`), golden data file (`golden.bin`), library file (`libenn_public_api_ndk_v1.so`), and the native program (`enn_nnc_model_tester`) to the `/data/vendor/enn/` directory on the ERD board."
    ],
    "reference": "The input.bin file is one of the files being copied to the ERD board using the adb push command, along with the model file, golden data file, library file, and the native program.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:22.375430+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain the role of the enn_nnc_model_tester in the context of deploying files to the ERD board?",
    "reference_contexts": [
      "The following commands copy the necessary files to the ERD board:  \n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/* /data/local/tmp/\n```  \nThese commands perform the following actions:  \n- `adb push` is used to copy the model file (`model.nnc`), input data file (`input.bin`), golden data file (`golden.bin`), library file (`libenn_public_api_ndk_v1.so`), and the native program (`enn_nnc_model_tester`) to the `/data/vendor/enn/` directory on the ERD board."
    ],
    "reference": "The enn_nnc_model_tester is a native program that is copied to the ERD board using the `adb push` command, along with other necessary files such as the model file, input data file, golden data file, and library file. This process ensures that all required components are available for testing and executing applications on the hardware platform.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:22.375430+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What does the enn_nnc_model_tester do?",
    "reference_contexts": [
      "The following commands copy the necessary files to the ERD board:  \n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/* /data/local/tmp/\n```  \nThese commands perform the following actions:  \n- `adb push` is used to copy the model file (`model.nnc`), input data file (`input.bin`), golden data file (`golden.bin`), library file (`libenn_public_api_ndk_v1.so`), and the native program (`enn_nnc_model_tester`) to the `/data/vendor/enn/` directory on the ERD board."
    ],
    "reference": "The enn_nnc_model_tester is a native program that is copied to the ERD board using the adb push command, along with other necessary files for execution.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:22.375430+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What files are copied to the /data/vendor/enn/ directory on the ERD board using adb push commands?",
    "reference_contexts": [
      "The following commands copy the necessary files to the ERD board:  \n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/* /data/local/tmp/\n```  \nThese commands perform the following actions:  \n- `adb push` is used to copy the model file (`model.nnc`), input data file (`input.bin`), golden data file (`golden.bin`), library file (`libenn_public_api_ndk_v1.so`), and the native program (`enn_nnc_model_tester`) to the `/data/vendor/enn/` directory on the ERD board."
    ],
    "reference": "The files copied to the /data/vendor/enn/ directory on the ERD board include the model file (`model.nnc`), input data file (`input.bin`), golden data file (`golden.bin`), library file (`libenn_public_api_ndk_v1.so`), and the native program (`enn_nnc_model_tester`).",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:22.375430+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do you execute a program on the ERD board after copying the necessary files?",
    "reference_contexts": [
      "After copying the necessary files to the ERD board, execute the native program using the following commands:  \n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```  \nThese commands perform the following actions:  \n- `adb shell` starts a shell session on the board.\n- `cd /data/local/tmp/` changes the current directory to `/data/local/tmp/`, where the necessary files are available.\n- The `LD_LIBRARY_PATH` environment variable sets the directory that contains `libenn_public_api_ndk_v1.so`.\n- The native program is executed with the `--model`, `--input`, `--golden`, and `--threshold` parameters, which specify the model file, input data file, golden data file, and threshold value, respectively."
    ],
    "reference": "To execute a program on the ERD board after copying the necessary files, you need to use the following commands:  \n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```  \nThese commands start a shell session on the board, change the current directory to `/data/local/tmp/`, set the `LD_LIBRARY_PATH` environment variable, and execute the native program with the specified parameters.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:22.375430+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What steps should be taken to execute a native program on the ERD board?",
    "reference_contexts": [
      "After copying the necessary files to the ERD board, execute the native program using the following commands:  \n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```  \nThese commands perform the following actions:  \n- `adb shell` starts a shell session on the board.\n- `cd /data/local/tmp/` changes the current directory to `/data/local/tmp/`, where the necessary files are available.\n- The `LD_LIBRARY_PATH` environment variable sets the directory that contains `libenn_public_api_ndk_v1.so`.\n- The native program is executed with the `--model`, `--input`, `--golden`, and `--threshold` parameters, which specify the model file, input data file, golden data file, and threshold value, respectively."
    ],
    "reference": "To execute a native program on the ERD board, first copy the necessary files to the board. Then, start a shell session using the command `adb shell`. Next, change the current directory to `/data/local/tmp/` with the command `cd /data/local/tmp/`. After that, set the `LD_LIBRARY_PATH` environment variable to point to the directory containing `libenn_public_api_ndk_v1.so` using the command `export LD_LIBRARY_PATH=/data/local/tmp`. Finally, execute the native program with the command `./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001`, specifying the model file, input data file, golden data file, and threshold value.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:22.375430+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do I set the LD_LIBRARY_PATH for libenn_public_api_ndk_v1.so?",
    "reference_contexts": [
      "After copying the necessary files to the ERD board, execute the native program using the following commands:  \n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```  \nThese commands perform the following actions:  \n- `adb shell` starts a shell session on the board.\n- `cd /data/local/tmp/` changes the current directory to `/data/local/tmp/`, where the necessary files are available.\n- The `LD_LIBRARY_PATH` environment variable sets the directory that contains `libenn_public_api_ndk_v1.so`.\n- The native program is executed with the `--model`, `--input`, `--golden`, and `--threshold` parameters, which specify the model file, input data file, golden data file, and threshold value, respectively."
    ],
    "reference": "To set the LD_LIBRARY_PATH for libenn_public_api_ndk_v1.so, you need to execute the command `export LD_LIBRARY_PATH=/data/local/tmp` after changing the directory to `/data/local/tmp/` where the necessary files are located.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:22.375430+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What commands should I use to execute a native program on the ERD bord?",
    "reference_contexts": [
      "After copying the necessary files to the ERD board, execute the native program using the following commands:  \n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```  \nThese commands perform the following actions:  \n- `adb shell` starts a shell session on the board.\n- `cd /data/local/tmp/` changes the current directory to `/data/local/tmp/`, where the necessary files are available.\n- The `LD_LIBRARY_PATH` environment variable sets the directory that contains `libenn_public_api_ndk_v1.so`.\n- The native program is executed with the `--model`, `--input`, `--golden`, and `--threshold` parameters, which specify the model file, input data file, golden data file, and threshold value, respectively."
    ],
    "reference": "After copying the necessary files to the ERD board, execute the native program using the following commands: `adb shell`, `cd /data/local/tmp/`, and `export LD_LIBRARY_PATH=/data/local/tmp`. Finally, run `./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001`. These commands start a shell session on the board, change the current directory to where the necessary files are available, set the `LD_LIBRARY_PATH`, and execute the native program with the specified parameters.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:22.375430+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How to use enn-sdk-samples-9925?",
    "reference_contexts": [
      "Download the ENN framework library (ENN Public API NDK) from [resources](https://soc-developer.semiconductor.samsung.com/support/resource).\nNext, copy the necessary libraries by perform the following steps:  \n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/jni/lib64``\n2. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/jni/include`\n3. Add following code in `en_nnc_model_tester.cpp` to load ENN framework.\n([Example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L13)):\n```cpp\n#include \"include/enn_api-public_ndk_v1.hpp\"\n```"
    ],
    "reference": "To use enn-sdk-samples-9925, first download the ENN framework library from the provided resources. Then, copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/jni/lib64`, and copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/jni/include`. Finally, add the code `#include \"include/enn_api-public_ndk_v1.hpp\"` in `en_nnc_model_tester.cpp` to load the ENN framework.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\writing-native-program.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:48.310528+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the ENN frmaework used for?",
    "reference_contexts": [
      "This section describes the steps for executing NN models on the ENN framework.\nFor more information on the ENN framework, refer to the [documentation](developer-guide#4-enn-framework-api)."
    ],
    "reference": "The ENN framework is used for executing NN models.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\writing-native-program.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:48.310528+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What need for ML model in ENN?",
    "reference_contexts": [
      "To execute NNC models using the ENN framework, the following parameters are required:  \n|Parameter|Data Type|Explanation|\n|--|--|--|\n|`model_name`|string|Path to the ML model file|\n|`inputs`|vector&lt;string&gt;|List of input file paths|\n|`goldens` (optional)|vector&lt;string&gt;|List of golden file paths for validation|\n|`threshold` (optional)|float|Threshold for golden matching, used to determine the acceptable deviation|"
    ],
    "reference": "To execute NNC models using the ENN framework, the required parameters include `model_name`, which is the path to the ML model file, and `inputs`, which is a list of input file paths.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\writing-native-program.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:48.310528+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the steps involved in executing neural network models using the ENN framework, and how does the lifecycle of these models operate?",
    "reference_contexts": [
      "Executing NN models on the ENN framework comprises of three steps such as initializing framework, inferring the models, and deinitializing the framework.  \nThe following chart describes the lifecycle and process of inferring NN models using the ENN framework.  \n```mermaid\ngraph TB\nsubgraph A[Initialize ENN Framework]\nA1[Initialize]\nA1 --> A2[Open Model]\nA2 --> A3[Allocate/Commit Buffers]\nend\nsubgraph B[Inference]\nB1[Copy Input Layer]\nB1 --> B2[Execute Model]\nB2 --> B3[Copy Output Layer]\nend\nsubgraph C[Deinitialize]\nC1[Release Buffers]\nC1 --> C2[Close Model]\nC2 --> C3[Deinitialize]\nend\nA --> B\nB --> C\n```  \nTo infer multiple data, repeat `Inference`."
    ],
    "reference": "Executing neural network models on the ENN framework comprises three steps: initializing the framework, inferring the models, and deinitializing the framework. The lifecycle begins with the initialization phase, where the framework is set up by opening the model and allocating or committing buffers. Next, during the inference phase, the input layer is copied, the model is executed, and the output layer is copied. Finally, in the deinitialization phase, buffers are released, the model is closed, and the framework is deinitialized. To infer multiple data, the inference step can be repeated.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\writing-native-program.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:48.310528+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the role of EnnBufferPtr in the ENN framework when allocating buffers for ML models?",
    "reference_contexts": [
      "Before executing ML models on the ENN framework, initialize the framework, load the model, and allocate the necessary buffers.  \n1. [EnnInitialize](api-reference/enn-framework-api-functions#function-enninitialize):\nThis function initializes the ENN Framework.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L48)):\n```cpp\nenn::api::EnnInitialize();\n```  \n1. [EnnOpenModel](api-reference/enn-framework-api-functions#function-ennopenmodel):\nThis function opens the specified model and return a model ID.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L56)):\n```cpp\nEnnModelId model_id;\nenn::api::EnnOpenModel(model_name.c_str(), &model_id);\n```  \n1. [EnnAllocateAllBuffers](api-reference/enn-framework-api-functions#function-ennallocateallbuffers):\nThis function allocates all the necessary buffers for the model.\nIt also provides the number of input/output buffers (`buffer_info`), their locations, and sizes (`buffer_set`).  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L68)):\n```cpp\nEnnBufferPtr *buffer_set;\nNumberOfBuffersInfo buffer_info;\nenn::api::EnnAllocateAllBuffers(model_id, &buffer_set, &buffer_info);\n```  \nFollowing is the data structure of `EnnBufferPtr` and `NumberOfBuffersInfo`.  \n```cpp\ntypedef struct _ennBuffer {\nvoid *va;\nuint32_t size;\nuint32_t offset;\n} EnnBuffer;\n\ntypedef EnnBuffer* EnnBufferPtr;\n\ntypedef struct _NumberOfBuffersInfo {\nuint32_t n_in_buf;\nuint32_t n_out_buf;\n} NumberOfBuffersInfo;\n```"
    ],
    "reference": "EnnBufferPtr is a typedef for a pointer to the EnnBuffer structure, which contains information about the allocated buffers, including their virtual address, size, and offset. It is used in the function EnnAllocateAllBuffers to store the necessary buffers for the model, along with the NumberOfBuffersInfo structure that provides the number of input and output buffers.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\writing-native-program.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:48.310528+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the function of EnnCloseModel in the ENN framework?",
    "reference_contexts": [
      "After executing the model, deinitialize the framework to release resources.  \n1. [EnnReleaseBuffers](api-reference/enn-framework-api-functions#function-ennreleasebuffers):\nThis function releases the allocated buffers.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L112)):\n```cpp\nenn::api::EnnReleaseBuffers(buffer_set, buffer_info.n_in_buf + buffer_info.n_out_buf)\n```  \n1. [EnnCloseModel](api-reference/enn-framework-api-functions#function-ennclosemodel):\nThis function closes the specified model.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L118)):\n```cpp\nenn::api::EnnCloseModel(model_id)\n```  \n1. [EnnDeinitialize](api-reference/enn-framework-api-functions#function-enndeinitialize):\nThis function deinitializes the ENN framework.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L124)):\n```cpp\nenn::api::EnnDeinitialize()\n```"
    ],
    "reference": "EnnCloseModel is a function that closes the specified model within the ENN framework.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\writing-native-program.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 16:59:48.310528+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does Android Jetpack, specifically the CameraX library, enhance the development of camera functionalities in Android applications?",
    "reference_contexts": [
      "The CameraX library, an integral component of Android Jetpack, has been designed to simplify camera operations across various Android devices. It offers a consistent and intuitive API, ensuring developers face minimal challenges when incorporating camera functionalities. This documentation delves deeper into the integration process: from initializing the hardware to intricately binding the camera's lifecycle."
    ],
    "reference": "The CameraX library, a key part of Android Jetpack, simplifies camera operations across different Android devices by providing a consistent and intuitive API. This design minimizes challenges for developers when integrating camera functionalities into their applications.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\other\\WorkingWithTheCamera.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:00:15.360267+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What are the prerequisites for using CameraX in Android app development?",
    "reference_contexts": [
      "1. Proficiency with Android application development.\n2. A configured Android Studio environment.\n3. CameraX library integrated into your project's Gradle dependencies."
    ],
    "reference": "The prerequisites for using CameraX in Android app development include proficiency with Android application development, a configured Android Studio environment, and the CameraX library integrated into your project's Gradle dependencies.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\other\\WorkingWithTheCamera.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:00:15.360267+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What role does cameraProviderFuture play in managing camera devices in Android development?",
    "reference_contexts": [
      "```kotlin\nprivate var preview: Preview? = null\nprivate var camera: Camera? = null\n\nprivate fun setCamera() {\ncameraExecutor = Executors.newSingleThreadExecutor()\nval cameraProviderFuture = ProcessCameraProvider.getInstance(requireContext())\n\ncameraProviderFuture.addListener(\n{\nval cameraProvider = cameraProviderFuture.get()\nval cameraSelector = CameraSelector.DEFAULT_BACK_CAMERA\nsetPreview()\n\ntry {\ncameraProvider.unbindAll()\ncamera = cameraProvider.bindToLifecycle(this, cameraSelector, preview)\npreview?.setSurfaceProvider(binding.viewFinder.surfaceProvider)\n} catch (exc: java.lang.Exception) {\nLog.e(TAG, \"Camera binding failed\", exc)\n}\n},\nContextCompat.getMainExecutor(requireContext())\n)\n}\n```\n**Annotations**:\n- `cameraExecutor`: A dedicated executor ensuring that camera operations run on a separate background thread, avoiding any UI slowdowns.\n- `cameraProviderFuture`: An asynchronous task to fetch an instance of `ProcessCameraProvider`. This instance helps control and manage camera devices.\n- `cameraSelector`: Utilized to specify preferences for camera initialization. Here, we select the device's default back camera."
    ],
    "reference": "cameraProviderFuture is an asynchronous task that fetches an instance of ProcessCameraProvider, which is essential for controlling and managing camera devices in Android development.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\other\\WorkingWithTheCamera.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:00:15.360267+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain how PreviewView is used in Android development for camera functionalities and what are the important configurations?",
    "reference_contexts": [
      "```xml\n<androidx.constraintlayout.widget.ConstraintLayout>\n...\n<androidx.camera.view.PreviewView\nandroid:id=\"@+id/viewFinder\"\nandroid:layout_width=\"match_parent\"\nandroid:layout_height=\"match_parent\" />\n...\n</androidx.constraintlayout.widget.ConstraintLayout>\n```  \n```kotlin\nprivate fun setPreview() {\npreview = Preview.Builder()\n.setTargetAspectRatio(AspectRatio.RATIO_4_3)\n.setTargetRotation(binding.viewFinder.display.rotation)\n.build()\n}\n```  \n**Annotations**:\n- Declare the component that will output the preview image, such as \"PreviewView\".\n- The `Preview.Builder` facilitates the configuration of the live feed properties.\n- `setTargetAspectRatio(AspectRatio.RATIO_4_3)`: Configures the aspect ratio of the camera preview. Here, a 4:3 ratio is selected.\n- `setTargetRotation()`: Aligns the camera feed's rotation with the device's current display rotation, ensuring the feed orientation is consistent with user expectations."
    ],
    "reference": "In Android development, PreviewView is declared as the component that outputs the preview image. It is used within a ConstraintLayout and is set to match the parent's width and height. The Preview.Builder is utilized to configure the live feed properties, including setting the target aspect ratio to 4:3 and aligning the camera feed's rotation with the device's current display rotation using setTargetRotation(). This ensures that the preview feed orientation is consistent with user expectations.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\other\\WorkingWithTheCamera.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:00:15.360267+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does the camera's operation affect battery life in Android development?",
    "reference_contexts": [
      "**Objective**: To harmonize the camera's operations with the Fragment's lifecycle. Such synchronization guarantees the camera's optimal performance, ensuring that it doesn't run unnecessarily, conserving both CPU cycles and battery life.  \n**Methods Utilized**:\n- `unbindAll()`: Clears any previous bindings, ensuring a fresh slate for binding. This step is crucial to prevent potential conflicts.\n- `bindToLifecycle()`: Merges the camera's lifecycle with the fragment's lifecycle."
    ],
    "reference": "The camera's operation is synchronized with the Fragment's lifecycle to ensure optimal performance, which helps prevent unnecessary camera usage, thereby conserving battery life.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\other\\WorkingWithTheCamera.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:00:15.360267+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the purpose of the Exynos AI Studio guide as mentioned in the document?",
    "reference_contexts": [
      "This document is a draft for the Exynos AI Studio guide.\nIt is intended solely for internal review and is not meant for external release."
    ],
    "reference": "The Exynos AI Studio guide is intended solely for internal review and is not meant for external release.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\other\\WorkingWithTheCamera.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:00:15.360267+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain how to use the Exynous Neural Network Software Development Kit for converting Neural Network models to Neural Network Container models and executing them on Exynous devices?",
    "reference_contexts": [
      "This guide provides basic instructions for using Exynos Neural Network Software Development Kit (Exynos AI Studio).\nThis guide explains the method to convert Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also describes the execution of NNC models on Exynos devices."
    ],
    "reference": "This guide provides basic instructions for using Exynos Neural Network Software Development Kit (Exynos AI Studio). It explains the method to convert Neural Network (NN) models to Neural Network Container (NNC) models and describes the execution of NNC models on Exynos devices.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:00:44.511462+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do you convrt NN models to NNC models using Exynos AI Studio?",
    "reference_contexts": [
      "This guide provides basic instructions for using Exynos Neural Network Software Development Kit (Exynos AI Studio).\nThis guide explains the method to convert Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also describes the execution of NNC models on Exynos devices."
    ],
    "reference": "This guide explains the method to convert Neural Network (NN) models to Neural Network Container (NNC) models.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:00:44.511462+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What Exynos platforms do for AI models?",
    "reference_contexts": [
      "[Exynos AI Studio](https://soc-developer.semiconductor.samsung.com/enn-sdk) allows users to convert the trained [TensorFlow Lite](https://www.tensorflow.org/lite) neural network models to a format that can run efficiently in [Samsung Exynos](https://semiconductor.samsung.com/processor/) hardware.\nExynos AI Studio contains ENNTools to convert trained NN models and ENN framework for executing converted models on Exynos platforms.  \nThis guide covers the basics of using [Exynos AI Studio service](https://soc-developer.semiconductor.samsung.com/enn-sdk/project/) and executing NN models with ENN framework."
    ],
    "reference": "Exynos platforms allow users to run efficiently converted TensorFlow Lite neural network models using the ENN framework provided by Exynos AI Studio.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:00:44.511462+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How TensorFlow Lite help run models on Exynos hardware?",
    "reference_contexts": [
      "[Exynos AI Studio](https://soc-developer.semiconductor.samsung.com/enn-sdk) allows users to convert the trained [TensorFlow Lite](https://www.tensorflow.org/lite) neural network models to a format that can run efficiently in [Samsung Exynos](https://semiconductor.samsung.com/processor/) hardware.\nExynos AI Studio contains ENNTools to convert trained NN models and ENN framework for executing converted models on Exynos platforms.  \nThis guide covers the basics of using [Exynos AI Studio service](https://soc-developer.semiconductor.samsung.com/enn-sdk/project/) and executing NN models with ENN framework."
    ],
    "reference": "TensorFlow Lite helps by allowing users to convert trained neural network models to a format that can run efficiently in Samsung Exynos hardware.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:00:44.511462+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain how Exynos AI Studio Service is used in the process of converting and executing a neural network model?",
    "reference_contexts": [
      "Following figure illustrates the three steps for converting and executing an NN model:  \n```mermaid\nflowchart LR\nsubgraph \"Exynos AI Studio Service\"\ndirection LR\nconvert(\"Convert The Model\")\nend\nsubgraph \"ENN Framework\"\ndirection LR\nexecute(\"Execute The Model\")\nend\nmodel(\"Prepare Trained Model<br>(TFLite)\")-->convert-->execute\n```"
    ],
    "reference": "Exynos AI Studio Service is involved in the first step of the process, which is converting the model. The flowchart illustrates that after preparing the trained model in TFLite format, the next step is to convert it using the Exynos AI Studio Service, followed by executing the model with the ENN Framework.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:00:44.511462+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain the process of using TFLite for deploying neural network models on embedded devices, as illustrated in the provided context?",
    "reference_contexts": [
      "Following figure illustrates the three steps for converting and executing an NN model:  \n```mermaid\nflowchart LR\nsubgraph \"Exynos AI Studio Service\"\ndirection LR\nconvert(\"Convert The Model\")\nend\nsubgraph \"ENN Framework\"\ndirection LR\nexecute(\"Execute The Model\")\nend\nmodel(\"Prepare Trained Model<br>(TFLite)\")-->convert-->execute\n```"
    ],
    "reference": "The process of using TFLite for deploying neural network models on embedded devices involves three steps: first, the trained model is prepared in TFLite format. Next, the model is converted using the Exynos AI Studio Service, and finally, it is executed using the ENN Framework.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:00:44.511462+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How can I convert TensorFlow Lite models using Exynos AI Studio?",
    "reference_contexts": [
      "To convert TensorFlow Lite models, Exynos AI Studio provides an online conversion tool through the Samsung Exynos Eco-system [Portal](https://soc-developer.semiconductor.samsung.com/enn-sdk).\nThis online conversion tool allows users to upload their TFLite models, convert them to NNC models, and download the NNC models to their devices.  \nFor more information on the process of converting NN models, refer to [Converting NN Models with Exynos AI Studio Service](#converting-nn-models-with-enn-sdk-service)."
    ],
    "reference": "To convert TensorFlow Lite models, Exynos AI Studio provides an online conversion tool through the Samsung Exynos Eco-system. This tool allows users to upload their TFLite models, convert them to NNC models, and download the NNC models to their devices.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:00:44.511462+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does TensorFlow Lite facilitate the conversion of models for use in embedded systems, particularly through the Exynos AI Studio?",
    "reference_contexts": [
      "To convert TensorFlow Lite models, Exynos AI Studio provides an online conversion tool through the Samsung Exynos Eco-system [Portal](https://soc-developer.semiconductor.samsung.com/enn-sdk).\nThis online conversion tool allows users to upload their TFLite models, convert them to NNC models, and download the NNC models to their devices.  \nFor more information on the process of converting NN models, refer to [Converting NN Models with Exynos AI Studio Service](#converting-nn-models-with-enn-sdk-service)."
    ],
    "reference": "TensorFlow Lite models can be converted using the Exynos AI Studio, which provides an online conversion tool through the Samsung Exynos Eco-system. This tool allows users to upload their TFLite models, convert them to NNC models, and subsequently download the NNC models to their devices.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:00:44.511462+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How JNI help with ENN framework?",
    "reference_contexts": [
      "To execute NNC models on Exynos platforms, users must implement a program with ENN framework.\nENN framework provides C++ APIs for utilizing the framework that accelerate graph-based NN applications using NPU/DSP.\nThe Exynos AI Studio provides only C++ APIs.\nTherefore, the user must implement the Java Native Interface (JNI) layer to use the ENN framework on Android applications.  \nFor more information on the process of executing NN models, refer to [Executing Models Using Native Program](#executing-models-using-native-program) and [Executing Models Using Android Application](#executing-models-using-android-application)."
    ],
    "reference": "JNI is needed to implement a layer that allows the ENN framework, which provides C++ APIs, to be used in Android applications.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:00:44.511462+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How does the ENN framework facilitate the execution of neural network models on Exynos platforms?",
    "reference_contexts": [
      "To execute NNC models on Exynos platforms, users must implement a program with ENN framework.\nENN framework provides C++ APIs for utilizing the framework that accelerate graph-based NN applications using NPU/DSP.\nThe Exynos AI Studio provides only C++ APIs.\nTherefore, the user must implement the Java Native Interface (JNI) layer to use the ENN framework on Android applications.  \nFor more information on the process of executing NN models, refer to [Executing Models Using Native Program](#executing-models-using-native-program) and [Executing Models Using Android Application](#executing-models-using-android-application)."
    ],
    "reference": "The ENN framework facilitates the execution of neural network models on Exynos platforms by providing C++ APIs that accelerate graph-based neural network applications using NPU/DSP. Users must implement a program with the ENN framework and create a Java Native Interface (JNI) layer to use it on Android applications.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:00:44.511462+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What is the role of TFLite in performance comparison for Android applications?",
    "reference_contexts": [
      "|Sample Name|Description|\n|-------------|-------|\n|[Image Classification In Android](#image-classification-in-android)| Sample Android application to demonstrate the execution of `Inception v4` model with Exynos AI Studio|\n|[Object Detection In Android](#object-detection-in-android)| Sample Android application to demonstrate the execution of `YOLOv5` model with Exynos AI Studio|\n|[Segmentation In Android](#segmentation-in-android)| Sample Android application to demonstrate the execution of `DeeplabV3` model with Exynos AI Studio|\n|[Pose Estimation In Android](#pose-estimation-in-android)| Sample Android application to demonstrate the execution of `PoseNet` model with Exynos AI Studio|\n|[Image Enhance In Android](#image-enhance-in-android)| Sample Android application to demonstrate the execution of `Zero-DCE` model with Exynos AI Studio|\n|[Depth Estimation In Andriod](#depth-estimation-in-andriod)| Sample Android application to demonstrate the execution of `MiDaS v2` model with Exynos AI Studio|\n|[Performance Comparison](#performance-comparison)| Sample Android application to demonstrate the difference between Exynos AI Studio and TFLite |\n|[NNC Model Tester](#nnc-model-tester)|Sample C++ program to demonstrate the execution of NNC model with Exynos AI Studio|"
    ],
    "reference": "TFLite is involved in the performance comparison, demonstrating the difference between Exynos AI Studio and TFLite in a sample Android application.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:01:17.195539+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What details are provided in the overview of Android (Kotlin) sample applications?",
    "reference_contexts": [
      "This section provides an overview of Android (Kotlin) sample applications.\nEach sample application entry provides the details of the functionality of the sample application, its location, and instructions for running it.\nFor more information on implementing the sample applications, refer to [Getting Started With Android Samples](getting-started-with-android-samples) guide.  \n***"
    ],
    "reference": "The overview of Android (Kotlin) sample applications provides details about the functionality of each sample application, its location, and instructions for running it.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:01:17.195539+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How is the Inception v4 model executed in the sample application, and what tools are used for its conversion?",
    "reference_contexts": [
      "This sample application demonstrates the execution of a converted [Inception v4](https://tfhub.dev/tensorflow/lite-model/inception_v4_quant/1/default/1) model using the ENN framework.\nThe model is converted using Exynos AI Studio service with the **Accelerate** hardware type option."
    ],
    "reference": "The Inception v4 model is executed in the sample application using the ENN framework. The model is converted using the Exynos AI Studio service with the Accelerate hardware type option.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:01:17.195539+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Where can I find the image-classification sample?",
    "reference_contexts": [
      "The sample is available in the `enn-sdk-samples-9925/image-classification` directory within the [Github](https://github.com/exynos-eco/enn-sdk-samples-9925) repository."
    ],
    "reference": "The image-classification sample is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:01:17.195539+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do you modify the ModelConstants.kt file for a new model in the sample application?",
    "reference_contexts": [
      "To utilize the sample application:\n1.Download or clone the sample application from the Github repository.\n2.Open the sample application project in Android Studio.\n3.Connect the ERD board to the computer.\n4.Run the application (using Shift + F10).\n5.Select Camera or Image mode and provide the data for inference.  \nTo modify the model used in the sample application:\n1.Copy the desired model file to the `assets` directory of the project.\n2.Copy the corresponding label text file to the `assets` directory.\n3.Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4.If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.  \n***"
    ],
    "reference": "To modify the ModelConstants.kt file for a new model in the sample application, you need to change the parameters in the file to reflect the specifications of the new model.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:01:17.195539+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How is YOLOv5 utilized in the sample application mentioned?",
    "reference_contexts": [
      "This sample application demonstrates the execution of a converted [YOLOv5](https://github.com/ultralytics/yolov5) model using the ENN framework.\nThe model is converted using Exynos AI Studio service with the **Default** hardware type option."
    ],
    "reference": "In the sample application, YOLOv5 is executed using the ENN framework, and the model is converted using the Exynos AI Studio service with the Default hardware type option.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:01:17.195539+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Where can I find the enn-sdk-samples-9925 for object detection?",
    "reference_contexts": [
      "The sample is available in the `enn-sdk-samples-9925/object-detection` directory within the [Github](https://github.com/exynos-eco/enn-sdk-samples-9925) repository."
    ],
    "reference": "The enn-sdk-samples-9925 for object detection is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:01:17.195539+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "How do you select the Camera mode in the sample application?",
    "reference_contexts": [
      "To utilize the sample application:\n1.Download or clone the sample application from the Github repository.\n2.Open the sample application project in Android Studio.\n3.Connect the ERD board to the computer.\n4.Run the application (using Shift + F10).\n5.Select Camera or Image mode and provide the data for inference.  \nTo modify the model used in the sample application:\n1.Copy the desired model file to the `assets` directory of the project.\n2.Copy the corresponding label text file to the `assets` directory.\n3.Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4.If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.  \n***"
    ],
    "reference": "To select the Camera mode in the sample application, you need to run the application and then choose Camera or Image mode to provide the data for inference.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:01:17.195539+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "What DeeplabV3 model do in this app?",
    "reference_contexts": [
      "This sample application demonstrates the execution of a converted [DeeplabV3](https://tfhub.dev/tensorflow/lite-model/deeplabv3/1/default/1) model using the ENN framework.\nThe model is converted using Exynos AI Studio service with the **Default** hardware type option."
    ],
    "reference": "This sample application shows how to run a converted DeeplabV3 model using the ENN framework.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:01:17.195539+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Where can I find the sample for segmentation on Github?",
    "reference_contexts": [
      "The sample is available in the `enn-sdk-samples-9925/segmentation` directory within the [Github](https://github.com/exynos-eco/enn-sdk-samples-9925) repository."
    ],
    "reference": "The sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:01:17.195539+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot",
    "user_comment": ""
  },
  {
    "user_input": "Can you explain the functionalities and attributes associated with the EnnBuffer in the ENN Framework?",
    "reference_contexts": [
      "API reference ## [ENN Framework Data Type References](api-reference/enn-framework-data-type-references) * **[_NumberOfBuffersInfo](api-reference/enn-framework-data-type-references#_numberofbuffersinfo)** * **[_ennBuffer](api-reference/enn-framework-data-type-references#_ennbuffer)** * **[_ennBufferInfo](api-reference/enn-framework-data-type-references#_ennbufferinfo)** ## [ENN Framwork API Functions](api-reference/enn-framework-api-functions) * **[Context initialize / deinitialize](api-reference/enn-framework-api-functions#context-initialize--deinitialize)** * **[OpenModel / CloseModel related](api-reference/enn-framework-api-functions#openmodel--closemodel-related)** * **[Memory Handling](api-reference/enn-framework-api-functions#memory-handling)** * **[Setters and Getters for model](api-reference/enn-framework-api-functions#setters-and-getters-for-model)** * **[Commit Buffer](api-reference/enn-framework-api-functions#commit-buffer)** * **[Execute Models](api-reference/enn-framework-api-functions#execute-models)** * **[Security, preference, get meta information..](api-reference/enn-framework-api-functions#security-preference-get-meta-information)** -- title: _ennBuffer --- # _ennBuffer ## Public Attributes | | Name | | -------------- | -------------- | | void * | **[va](Classes/struct__enn_buffer.md#variable-va)** | | uint32_t | **[size](Classes/struct__enn_buffer.md#variable-size)** | | uint32_t | **[offset](Classes/struct__enn_buffer.md#variable-offset)** | ## Public Attributes Documentation ### variable va ```cpp void va; ``` ### variable size ```cpp uint32_t size; ``` ### variable offset ```cpp uint32_t offset; ``` ------------------------------- Updated on 2023 08 11 at 16:24:05 +0900 -- title: _ennBufferInfo --- # _ennBufferInfo ## Public Attributes | | Name | | -------------- | -------------- | | bool | **[is_able_to_update](Classes/struct__enn_buffer_info.md#variable-is-able-to-update)** | | uint32_t | **[n](Classes/struct__enn_buffer_info.md#variable-n)** | | uint32_t | **[width](Classes/struct__enn_buffer_info.md#variable-width)** | | uint32_t | **[height](Classes/struct__enn_buffer_info.md#variable-height)** | | uint32_t | **[channel](Classes/struct__enn_buffer_info.md#variable-channel)** | | uint32_t | **[size](Classes/struct__enn_buffer_info.md#variable-size)** | | uint32_t | **[buffer_type](Classes/struct__enn_buffer_info.md#variable-buffer-type)** | | const char * | **[label](Classes/struct__enn_buffer_info.md#variable-label)** | ## Public Attributes Documentation ### variable is_able_to_update ```cpp bool is_able_to_update; ``` ### variable n ```cpp uint32_t n; ``` ### variable width ```cpp uint32_t width; ``` ### variable height ```cpp uint32_t height; ``` ### variable channel ```cpp uint32_t channel; ``` ### variable size ```cpp uint32_t size; ``` ### variable buffer_type ```cpp uint32_t buffer_type; ``` ### variable label ```cpp const char label; ``` ------------------------------- Updated on 2023 08 11 at 16:24:05 +0900 -- title: _NumberOfBuffersInfo --- # _NumberOfBuffersInfo ## Public Attributes | | Name | | -------------- | -------------- | | uint32_t | **[n_in_buf](Classes/struct___number_of_buffers_info.md#variable-n-in-buf)** | | uint32_t | **[n_out_buf](Classes/struct___number_of_buffers_info.md#variable-n-out-buf)** | ## Public Attributes Documentation ### variable n_in_buf ```cpp uint32_t n_in_buf; ``` ### variable n_out_buf ```cpp uint32_t n_out_buf; ``` ------------------------------- Updated on 2023 08 11 at 16:24:05 +0900 -- title: Commit Buffer --- # Commit Buffer ## Functions | | Name | | -------------- | -------------- | | EnnReturn | **[EnnBufferCommit](Modules/group__api__commit.md#function-ennbuffercommit)**(const EnnModelId model_id, const int session_id =0)<br>Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() | ## Functions Documentation ### function EnnBufferCommit ``` EnnReturn EnnBufferCommit( const EnnModelId model_id, const int session_id =0 ) ``` Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Parameters* : **model_id ** [IN] model ID from load_model **Return**: EnnReturn result, 0 is success ------------------------------- Updated on 2023 08 11 at 16:24:05 +0900 --- title: Context initialize / deinitialize --- # Context initialize / deinitialize ## Functions | | Name | | -------------- | -------------- | | EnnReturn | **[EnnInitialize](Modules/group__api__context.md#function-enninitialize)**(void )<br>Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. | | EnnReturn | **[EnnDeinitialize](Modules/group__api__context.md#function-enndeinitialize)**(void )<br>Deinitialize Enn Framework. Framework degenerates context in a caller's process. | ## Functions Documentation ### function EnnInitialize ``` EnnReturn EnnInitialize( void ) ``` Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. **Return**: EnnReturn result, 0 is success ### function EnnDeinitialize ``` EnnReturn EnnDeinitialize( void ) ``` Deinitialize Enn Framework. Framework degenerates context in a caller's process. **Return**: EnnReturn result, 0 is success ------------------------------- Updated on 2023 08 11 at 16:24:05 +0900 --- title: Security, preference, get meta information.. --- # Security, preference, get meta information.. ## Functions | | Name | | -------------- | -------------- | | EnnReturn | **[EnnSetPreferencePresetId](Modules/group__api__miscellaneous.md#function-ennsetpreferencepresetid)**(const uint32_t val)<br>Setting Preset ID for operation performance. | | EnnReturn | **[EnnSetPreferencePerfConfigId](Modules/group__api__miscellaneous.md#function-ennsetpreferenceperfconfigid)**(const uint32_t val)<br>Setting PerfConfig ID for operation performance. | | EnnReturn | **[EnnSetPreferencePerfMode](Modules/group__api__miscellaneous.md#function-ennsetpreferenceperfmode)**(const uint32_t val)<br>Setting Performance Mode. | | EnnReturn | **[EnnSetPreferenceTimeOut](Modules/group__api__miscellaneous.md#function-ennsetpreferencetimeout)**(const uint32_t val)<br>Setting Preset ID for time out. | | EnnReturn | **[EnnSetPreferencePriority](Modules/group__api__miscellaneous.md#function-ennsetpreferencepriority)**(const uint32_t val)<br>Setting priority value for NPU. | | EnnReturn | **[EnnSetPreferenceCoreAffinity](Modules/group__api__miscellaneous.md#function-ennsetpreferencecoreaffinity)**(const uint32_t val)<br>Setting affinity to set NPU core operation. | | EnnReturn | **[EnnGetPreferencePresetId](Modules/group__api__miscellaneous.md#function-enngetpreferencepresetid)**(uint32_t * val_ptr)<br>Get current information for Preset ID. | | EnnReturn | **[EnnGetPreferencePerfConfigId](Modules/group__api__miscellaneous.md#function-enngetpreferenceperfconfigid)**(uint32_t * val_ptr)<br>Get current information for PerfConfig ID. | | EnnReturn | **[EnnGetPreferencePerfMode](Modules/group__api__miscellaneous.md#function-enngetpreferenceperfmode)**(uint32_t * val_ptr)<br>Get current information for Performance Mode. | | EnnReturn | **[EnnGetPreferenceTimeOut](Modules/group__api__miscellaneous.md#function-enngetpreferencetimeout)**(uint32_t * val_ptr)<br>Get current information for Time Out. | | EnnReturn | **[EnnGetPreferencePriority](Modules/group__api__miscellaneous.md#function-enngetpreferencepriority)**(uint32_t * val_ptr)<br>Get current information for NPU Priority. | | EnnReturn | **[EnnGetPreferenceCoreAffinity](Modules/group__api__miscellaneous.md#function-enngetpreferencecoreaffinity)**(uint32_t * val_ptr)<br>Get current information for NPU Core affinity. | | EnnReturn | **[EnnGetMetaInfo](Modules/group__api__miscellaneous.md#function-enngetmetainfo)**(const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX])<br>Get Meta Information. | | EnnReturn | **[EnnSetExecMsgAlwaysOn](Modules/group__api__miscellaneous.md#function-ennsetexecmsgalwayson)**()<br>Set frequency of execution message print. | ## Functions Documentation ### function EnnSetPreferencePresetId ``` EnnReturn EnnSetPreferencePresetId( const uint32_t val ) ``` Setting Preset ID for operation performance. Parameters* : **val ** [IN] value to set preset ID **Return**: EnnReturn result, 0 is success ### function EnnSetPreferencePerfConfigId ``` EnnReturn EnnSetPreferencePerfConfigId( const uint32_t val ) ``` Setting PerfConfig ID for operation performance. Parameters* : **val ** [IN] value to set PerfConfig ID **Return**: EnnReturn result, 0 is success ### function EnnSetPreferencePerfMode ``` EnnReturn EnnSetPreferencePerfMode( const uint32_t val ) ``` Setting Performance Mode. Parameters* : **val ** [IN] value to set Performance Mode **Return**: EnnReturn result, 0 is success ### function EnnSetPreferenceTimeOut ``` EnnReturn EnnSetPreferenceTimeOut( const uint32_t val ) ``` Setting Preset ID for time out. Parameters* : **val ** [IN] value to set time out **Return**: EnnReturn result, 0 is success Note* : in second ### function EnnSetPreferencePriority ``` EnnReturn EnnSetPreferencePriority( const uint32_t val ) ``` Setting priority value for NPU. Parameters* : **val ** [IN] value to set NPU job priority **Return**: EnnReturn result, 0 is success ### function EnnSetPreferenceCoreAffinity ``` EnnReturn EnnSetPreferenceCoreAffinity( const uint32_t val ) ``` Setting affinity to set NPU core operation. Parameters* : **val ** [IN] value to set affinity **Return**: EnnReturn result, 0 is success Note* : in second ### function EnnGetPreferencePresetId ``` EnnReturn EnnGetPreferencePresetId( uint32_t val_ptr ) ``` Get current information for Preset ID. Parameters* : **val ** [OUT] current value of Preset ID **Return**: EnnReturn result, 0 is success ### function EnnGetPreferencePerfConfigId ``` EnnReturn EnnGetPreferencePerfConfigId( uint32_t val_ptr ) ``` Get current information for PerfConfig ID. Parameters* : **val ** [OUT] current value of PerfConfig ID **Return**: EnnReturn"
    ],
    "reference": "The _ennBuffer in the ENN Framework has several public attributes, including a void pointer named va, a uint32_t named size, and another uint32_t named offset. The va attribute is used to hold a pointer to the buffer data, while size indicates the size of the buffer, and offset specifies the offset within the buffer. These attributes are essential for managing memory and data flow within the framework.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:33:18.750554+09:00",
    "chatbot_server": "",
    "user_comment": ""
  },
  {
    "user_input": "What is the significance of NPU Priority in the context of Exynos AI Studio?",
    "reference_contexts": [
      "result, 0 is success ### function EnnGetPreferencePerfMode ``` EnnReturn EnnGetPreferencePerfMode( uint32_t val_ptr ) ``` Get current information for Performance Mode. Parameters* : **val ** [OUT] current value of Performance Mode **Return**: EnnReturn result, 0 is success ### function EnnGetPreferenceTimeOut ``` EnnReturn EnnGetPreferenceTimeOut( uint32_t val_ptr ) ``` Get current information for Time Out. Parameters* : **val ** [OUT] current value of Time Out **Return**: EnnReturn result, 0 is success ### function EnnGetPreferencePriority ``` EnnReturn EnnGetPreferencePriority( uint32_t val_ptr ) ``` Get current information for NPU Priority. Parameters* : **val ** [OUT] current value of NPU Priority **Return**: EnnReturn result, 0 is success ### function EnnGetPreferenceCoreAffinity ``` EnnReturn EnnGetPreferenceCoreAffinity( uint32_t val_ptr ) ``` Get current information for NPU Core affinity. Parameters* : **val ** [OUT] current value of NPU Core affinity **Return**: EnnReturn result, 0 is success ### function EnnGetMetaInfo ``` EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) ``` Get Meta Information. Parameters* : **info_id ** info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. ```cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW ``` model_id* output_str** **Return**: EnnReturn result, 0 is success This API includes loaded model information as well as framework information ### function EnnSetExecMsgAlwaysOn ``` EnnReturn EnnSetExecMsgAlwaysOn() ``` Set frequency of execution message print. Parameters* : **rate ** if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return* : EnnReturn ------------------------------- Updated on 2023 08 11 at 16:24:05 +0900 # * Dataset preparation** This is a guideline for preparing the input dataset. ### * Dataset format** The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. # Model Requirements and Constraints ### model format Model should be prepared in ONNX format to start optimization. ### opset version EHT currently support ONNX opset version 13 ~ 17. # Exynos AI Studio Developer Guide ## Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. ## 1. Introduction [Exynos AI Studio](https://soc-developer.semiconductor.samsung.com/enn-sdk) allows users to convert trained [TensorFlow Lite](https://www.tensorflow.org/lite) neural network models to a format that can run efficiently in [Samsung Exynos](https://semiconductor.samsung.com/processor/) hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. ### Structure of Documentation - [Chapter 1](#1-introduction) introduces Exynos AI Studio and its eco-system. - [Chapter 2](#2-enn-sdks-capabilities) provides information on the features of Exynos AI Studio. - [Chapter 3](#3-tools) provides information on tools provided with Exynos AI Studio. - [Chapter 4](#4-enn-framework-api) provides information on ENN framework API. - The subsequent chapters provide additional information on Exynos AI Studio. ### Samples The list of samples for Exynos AI Studio is available in [Exynos AI Studio Samples](enn-sdk-samples). ### Support Support materials including forums, FAQs, and others are available at the [Exynos Eco-system web page](https://soc-developer.semiconductor.samsung.com/). ### Reporting Bugs To report a bug or issue, follow the instructions described in the [Reporting Exynos AI Studio Issues](#reporting-enn-sdk-issues). ## 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. ### Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. #### Model Conversion Use one of the [tools](#3-tools) that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. #### Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. > To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. ### Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the [Support Matrix](support-matrix). ## 3. Tools ### Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: 1. **Access the ENN Eco-system**: - If you are a new user, [sign up](https://soc-developer.semiconductor.samsung.com/register) to create an account. - If you are an existing user, log in to ENN eco-system. 1. **Navigate to the Service**: - Visit the [Exynos AI Studio service](https://soc-developer.semiconductor.samsung.com/enn-sdk/project/) page. 1. **Provide Project Information**: 1. Enter a descriptive title for your project. 1. Use the provided interface to upload your TFLite model. 1. **Choose Hardware Preferences**: - **Default**: Utilizes only the CPU and GPU. - **Accelerate**: Engages the NPU as an additional accelerator. > ***Warning***: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to [Support Matix](support-matrix). 1. **Initiate the Conversion**: 1. Click **Confirm** to verify your selections. 1. Click **Convert** to start the model conversion process."
    ],
    "reference": "NPU Priority is important as it determines the current value of priority assigned to the NPU, which can affect how resources are allocated and managed during model execution.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:33:18.750554+09:00",
    "chatbot_server": "",
    "user_comment": ""
  },
  {
    "user_input": "What functionalities does the Exynos AI Studio provide for neural network model optimization?",
    "reference_contexts": [
      "1. **Download the Converted Model**: 1. If the conversion is successful, the **NNC Download** button is enabled. 1. Click NNC Download to download the NNC model. 1. Integrate the downloaded NNC model into your application required. ## 4. ENN tensor Framework APIs ### Data Type References For more information on the list of data types, refer to [API Reference](api-reference/#data-type-references). ### API Functions For more information on the list of API functions, refer to [API Reference](api-reference/#api-functions). ## 5. Advanced Topics ### Model Design Tips #### Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. | Architecture | Channel Alignment | | -- | -- | | Gen-4 | 32 | #### Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: **Option A **: Aligned corner: `False` Half pixel centers: `False` Performance: High speed **Option B **: Aligned corner: `True` Half pixel centers: `False` Compatibility: Gen 4 and later NPUs Performance: Medium speed **Option C **: Aligned corner: `False` Half pixel centers: `True` Note: Requires workaround Performance: Reduced speed #### Data Processing Procedures - **Pre-processing** and **Post-processing**: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. - **Memory Alignment**: Includes data transformation operations such as `split` and `reshape` during the pre-processing phase to ensure proper data alignment. #### Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. #### PReLU Use the `PReLU` activation function for optimal performance. Although `LeakyReLU` is functional, it may not provide the same level of efficiency. #### Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). ## 6. Troubleshooting ### FAQs Following are the responses to some of the most frequently asked questions: #### 1. How do I use Exynos AI Studio Service? The [Exynos AI Studio service](#enn-sdk-service) section provides detailed information on using the Exynos AI Studio service. #### 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. #### 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. <!-- ### Error Messages Following are some of the typical error messages that users may encounter, along with potential solutions: `To Be Updated` --> ## Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our [forums](https://soc-developer.semiconductor.samsung.com/user-lab/forum) for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our [1:1 Support](https://soc-developer.semiconductor.samsung.com/support/support11). # Introduction to Exynos AI High-Level Toolchain (EHT) **EHT** is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. ## System overview diagram ![overview_diagram](images/overview_diagram.png) ## [Converter](converter/converter.md) ## [Optimizer](optimizer/optimizer.md) ## [Quantizer](quantizer/quantizer.md) ## [Simulator](simulator/simulator.md) ## [Model Optimization Flow](model_optimization_flow/model_optimization_flow.md) # Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` # Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. ## Basic Quantization Methods ### Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. ### Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 ## Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. ## Debug API ### Layer-wise mixed precision quantiztion debug API The quantizer provides a"
    ],
    "reference": "The Exynos AI Studio provides optimization techniques for neural networks, including functionalities such as quantization and model optimization to generate SNC models. It includes a converter that enables model conversion between various intermediate representations, allowing users to apply optimization features to their PyTorch and TensorFlow models. Additionally, it offers a quantizer module that applies basic and advanced quantization methods to input models.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:33:18.750554+09:00",
    "chatbot_server": "",
    "user_comment": ""
  },
  {
    "user_input": "What steps are involved in using the Exynos AI Studio Service to convert trained TFLite models to NNC models?",
    "reference_contexts": [
      "mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. # Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using *simulated quantization*. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - `get_quantization_sim`: Attain a CNNX quantization simulation session for manual run. ```python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) ``` `load_input_data`: Load the input data from the given dataset path. `run_inference`: Conduct inference on a CNNX model. `compare_model_by_inference`: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. `compare_model_by_layer` : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` # Exynos AI Studio ## Exynos AI Studio Service Use the [Exynos AI Studio Service](https://soc-developer.semiconductor.samsung.com/enn-sdk/project/) to convert trained TFLite models to NNC models by performing the following steps. ### Preparation 1. Access the ENN Ecosystem Portal - If you are a new user, sign up to create an account. - If you are a returning user, log in to ENN eco-system 1. Download Inception v4 `.tflite` model from [here](https://tfhub.dev/tensorflow/lite-model/inception_v4_quant/1/default/1). ### Project Creation 1. Navigate to the Exynos AI Studio Service [page](https://soc-developer.semiconductor.samsung.com/enn-sdk/project/). <img src=\"./img/ai ml_enn sdk_project.png\" alt=\"drawing\" width=\"400\"/> 1. Enter a descriptive title for your model. 1. For this guide, choose **Accelerate** hardware type. - **Default**: Utilizes only the CPU and GPU. - **Accelerate**: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the ***Accelerate*** option may lead to complications. 1. After confirming the selections, the subsequent screen appears: <img src=\"./img/ai ml_enn sdk_project_convert.png\" alt=\"drawing\" width=\"400\"/> ### Conversion 1. Select **Convert** to initiate the conversion process. 1. After the completion of conversion process, the **NNC Download** button is enabled. ### Download Model 1. Click **NNC Download** to obtain the converted NNC model file. 1. To view the logs for the conversion that has failed, click **Log Download**. You can download and examine the log files. 1. Copy the downloaded model to `${APP_ROOT}/app/src/main/assets`. # Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. ## Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as `Android.mk` and `Application.mk`. ### Android.mk The `Android.mk` file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := llog LOCAL_CFLAGS += Wall std=c++14 O3 LOCAL_CPPFLAGS += fexceptions frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` ### Application.mk: The `Application.mk` file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): ```cmake APP_ABI := arm64 v8a APP_STL := c++_static ``` ## Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. ```shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk build C . ``` This command instructs NDK to start the build process in the current directory. The `NDK_PROJECT_PATH` environment variable is set to the current directory. ## Verifying the Build After the build process is complete, the compiled program can be verified by checking the `libs` directory: ```shell ls libs/arm64-v8a/ ``` The compiled program is visible in the output. ## Troubleshooting If you encounter any issues during the build process, ensure the following: The `NDK_PROJECT_PATH` environment variable is correctly set. The `ANDROID_NDK_HOME` environment variable points to the correct location of the NDK installation. The paths in the `Android.mk` file are correct. # * How to use** ### `enntools initialization` To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. ```bash enntools init ``` ### * elt yaml file** These are the configuration values used in various modules of elt. ### **Detailed explanation for elt yaml file** --- **model_analyzer : dict** - **check : bool** : Check the op support status - **device : str** : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - **level : int** : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - **snc_input : bool** : analyze the snc model, false: analyze the original model --- **database_gen : dict** - **database_spec : str** : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) --- **converter : dict** - **device : str** : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - **do_quantize : bool** : Enable quantization - **eltsum_cvt : bool** : enable eltwise conversion process; eltsum -> concat + conv - **graph_opt : str** : Graph Optimizer"
    ],
    "reference": "To use the Exynos AI Studio Service for converting trained TFLite models to NNC models, follow these steps: 1. Access the ENN Ecosystem Portal and create an account if you are a new user, or log in if you are a returning user. 2. Download the Inception v4 `.tflite` model from the specified link. 3. Navigate to the Exynos AI Studio Service page. 4. Enter a descriptive title for your model and choose the hardware type, either 'Default' for CPU and GPU or 'Accelerate' to engage the NPU as an additional accelerator, noting that not all models are supported by the NPU. 5. After confirming your selections, select 'Convert' to initiate the conversion process. 6. Once the conversion is complete, the 'NNC Download' button will be enabled, allowing you to download the converted NNC model file. 7. If the conversion fails, you can click 'Log Download' to view and examine the log files.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:33:18.750554+09:00",
    "chatbot_server": "",
    "user_comment": ""
  },
  {
    "user_input": "What does the SSD parameter indicate in the context of model optimization?",
    "reference_contexts": [
      "Model Type, Model Name for Graph Optimizer. - **mean : str** : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - **onnx_simplify : bool** : enable onnx_simplify process - **optimize : bool** : Use graph optimization - **quantize_type : str** : Select quantization type, quantized model (include caffeQAT) is \"qat\" - **scale : str** : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - **bw_ofm : int** : Bitwidth of intermediate feature map(A). - **data_format : str** : [channel_first, channel_last] - **debug : bool** : dump layerwise sqnr between new snc and hw quantized snc. - **gpu_enable : bool** : enable infer model on gpu if gpu is available - **gpu_id : int** : gpu id for quantization profiling - **input_dtype : str** : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - **mode : str** : [elt, eht_cnnx, eht_snc] - **output_dtype : str** : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - **profile_batchsize : int** : Batchsize for profile (value 100 is recommened). - **skip_old_snc_optimizer : bool** : true, skip old snc optimizer in old2new - **snc_converter : bool** : True, convert old snc to new snc, set it to false when input is new snc - **test_vector_gen : bool** : Enable testvector geneartion after quantization. - **tv_input : str** : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - **use_randomdb : bool** : Use randomdb for profiling data set - **userdb : str** : Profling data set path (default path is {workspace}/DATA/database.txt) --- **compiler : dict** - **assign_cpu : str** : Assign specific layer to cpu device - **assign_dsp : str** : Assign specific layer to dsp device - **assign_gpu : str** : Assign specific layer to gpu device - **best_fit_generalized : bool** : Control whether generalized best fit allocation is to be used. - **cast_in : str** : Type casting fp32 to fp16 for nnc input data - **cast_out : str** : Type casting fp16 to fp32 for nnc output data - **cfs : bool** : Enable cfifo sync - **compiler : str** : Compiler option - **datalayout_conversion_in : str** : Data layout(NHWC) conversion for nnc input data - **datalayout_conversion_out : str** : Data layout(NHWC) conversion for nnc output data - **debug_str : str** : debug str for compiler - **dequant_type : str** : dequantiztion type - **device : str** : System on Chip type [Gen-3, Gen-3b, Gen-4, Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - **enable_ofm_reuse : bool** : Enable the reuse of OFM region for IMFM. - **enable_stm : bool** : Generate compile log including L1 tiling information - **flc : bool** : Enable featuremap lossless compression - **fp16_swwa : bool** : Enable NPU fp16 workaround with psum_init - **input_conversion : str** : Add a Tensor2Cell format converter node at start of network - **mi : bool** : multiple input compile - **mo : bool** : multiple output compile - **multi_ncp : bool** : generate multi-ncp(ucgo) custom op - **multi_vc : bool** : Introduce Multi-VC for OFM, IFM, and weight transfer - **multicore : bool** : Enable NPU multicore - **optimization : str** : Optimization choice [O1, O2, O3] - **output_conversion : str** : Add a Tensor2Cell format converter node at end of network - **packed_ucgo : bool** : true: generate one ucgo in nnc, false: generate multiple ucgos in nnc if there are multiple independent dsp OPs - **preemption : bool** : Setting priority of NNC while compiling - **quant_type : str** : quantiztion type - **sync_npu_dsp : bool** : --- **simulator : dict** - **data_format : str** : Indicate the position of channel of input [channel_first, channel_last] - **use_randomdb : bool** : Use randomdb to forward, just support single input - **userdb : str** : Simulation data set path (default path is {workspace}/DATA/data.txt) --- **perf_estimator : dict** - **O2_enable : bool** : O2 optimization (true or false) - **O2_fm_forwarding : bool** : feature-map forwarding (true or false) - **SEG : bool** : Set true if input model is Deeplab V3+ - **SSD : bool** : Set true if input model is SSD detection - **bit_width_factor_FM : int** : Select feature map bit width factor (1 or 2) - **bit_width_factor_FP16 : bool** : Set bit width factor as floating point (true or false) - **bit_width_factor_weight : int** : Select weight bit width factor (1 or 2) - **core_num : int** : 1 for single core, 2 for instance-1 - **device : str** : Select device type [Gen-3, Gen-4, Gen-5, Gen-5a] - **json_report : bool** : Enable report json format - **nq_fold : bool** : Speed-up by folding normalization parameters for input data (true: model input dtype is uint8, false: model input dtype is float32) --- **profiler : dict** - **iter : int** : This decides how many time the model inference will be processed. - **mode : str** : 1. Low power mode : Mode for low power consumption and low performance. 2. Balanced mode : Mode for medium power consumption and medium performance. 3. Performance mode : Mode for high power consumption and high performance. 4. Boost mode : Mode for very high power consumption and very high performance. [lowpower, balanced, performance, boost] - **target : str** : profiling target. [model, system] - **test_type : str** : ENN running mode [lib, service] - **tv_threshold : float** : The value is used for tolerance threshold of output match verification. - **bitmatch_test : bool** : if set true, visual profiler will compile nnc first - **core_num : str** : The number of NPU core. [single, multiple] - **device : str** : Target device. [Gen-3, Gen-4, Gen-5, Gen-5a, Gen-6, Gen-6npu, Gen-7] - **device_id : str** : id of the device connected to the server or PC running the enntools docker - **remote_ssh_config_path : str** : remote testing ssh config path (default path is {workspace}/DATA/remote_ssh_config.yaml) - **ssh_bool : str** : Connect to the device through ssh."
    ],
    "reference": "The SSD parameter is set to true if the input model is SSD detection.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:33:18.750554+09:00",
    "chatbot_server": "",
    "user_comment": ""
  },
  {
    "user_input": "What are the key differences in model support between TensorFlow and TensorFlow Lite when using Exynos AI Studio for optimization?",
    "reference_contexts": [
      "<1-hop>\n\n[SSH_FALSE, SSH_TRUE] --- ### * eht yaml file** The optimization file contains optimization information. It allows you to set the locations of input and output models and detailed configurations for each module. Optimization is performed differently depending on the model type. ### **Detailed explanation for eht yaml file** --- model_type : string** Specify the type of the input model. Optimization is performed differently depending on the model type, with details as follows CV LVM LLM --- quantizer : dict** **precision_weight : str ** : precision of weight(ex. int8, int16, fp16) **precision_activation : str ** : precision of activation(ex. int8, int16, fp16) **mpq_operator_dict : dict ** : When performing mixed precision quantization, input the operators and precision to be quantized with a precision different from the values specified in precision_weight and precision_activation above. **alpha : float ** : smoothquant migration strength **calibration_data_path : string ** : The path to the representative data to be used for calibration **calibration_args : dict : ** Arguments for processing calibration **samples : int ** : How many calibration data samples to use **seed : int ** : A value set as a seed for random selection **add_dummy_conv: bool ** : Whether apply the dummy conv algorithm **input_dtype: dict ** : Input data type for the quantized model **output_dtype: dict ** : Output data type for the quantized model --- simulator : dict** **metric : string **: The metric to be used for measurement **threshold : float ** : The threshold value of the metric that determines agreement / disagreement **input_data_path : string **: The path to the dataset for model inference --- optimizer : dict** **skip_4_dim_conversion : bool ** : If true, it does not convert to 4 dimensions before optimization; if false, it proceeds with the conversion. **overwrite_input_shapes : dict ** : Enter the input shape for models with undefined input shapes. **custom_template_path : dict **: Enter the templates to apply for optimization. Use the path of the Python file containing the template as the key, and the template within it as the value. ### `enntools conversion` When performing conversion, users can choose between using eht and elt depending on the mode. The detailed workflow is illustrated in the figure below. ![conversion_workflow](images/conversion_workflow.png) ```bash enntools conversion ``` # Introduction to Exynos AI Studio **Exynos AI Studio** is a software that provides optimization and conversion techniques for neural networks models. This software takes models as input and offers functionalities such as quantization, conversion, optimization, compilation to generate NNC models. ## System overview diagram ![overview_diagram](images/overview_diagram.png) # Exynos AI Studio Documentation ## Exynos AI Studio References ### [Exynos AI Studio Developer Guide](developer-guide) This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. ### [API References](api-reference) This API reference documentation provides a list of data types and functions of Exynos AI Studio. ### [Support Matrix](support-matrix) These support matrices provide information on the supported platforms and operators of Exynos AI Studio. *** ## Exynos AI Studio Usage Guide ### [Quick Start Guide](quick-start-guide) This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. ### [Exynos AI Studio Samples](enn-sdk-samples) This guide provides a list of samples of Exynos AI Studio and their explanation. #### [Getting Started With Android Samples](getting-started-with-android-samples) This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. #### [Getting Started with Native Samples](getting-started-with-native-samples) This guide provides a walkthrough for developing a native program using Exynos AI Studio. -- Last Updated: 2023 11 21 0321 UTC # Quick Start ## Run docker container ```bash docker run it - gpus all name exynos_ai_studio_container \\ v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 ``` ## RUN tutorial ```bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init ``` ```bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ ``` modify config file and set mode ```bash enntools conversion ``` # Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. ## Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for **NPU**, the models must be quantized. ## Supported Operators Exynos AI Studio supports the following operators | **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O |",
      "<2-hop>\n\n1. **Download the Converted Model**: 1. If the conversion is successful, the **NNC Download** button is enabled. 1. Click NNC Download to download the NNC model. 1. Integrate the downloaded NNC model into your application required. ## 4. ENN tensor Framework APIs ### Data Type References For more information on the list of data types, refer to [API Reference](api-reference/#data-type-references). ### API Functions For more information on the list of API functions, refer to [API Reference](api-reference/#api-functions). ## 5. Advanced Topics ### Model Design Tips #### Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. | Architecture | Channel Alignment | | -- | -- | | Gen-4 | 32 | #### Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: **Option A **: Aligned corner: `False` Half pixel centers: `False` Performance: High speed **Option B **: Aligned corner: `True` Half pixel centers: `False` Compatibility: Gen 4 and later NPUs Performance: Medium speed **Option C **: Aligned corner: `False` Half pixel centers: `True` Note: Requires workaround Performance: Reduced speed #### Data Processing Procedures - **Pre-processing** and **Post-processing**: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. - **Memory Alignment**: Includes data transformation operations such as `split` and `reshape` during the pre-processing phase to ensure proper data alignment. #### Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. #### PReLU Use the `PReLU` activation function for optimal performance. Although `LeakyReLU` is functional, it may not provide the same level of efficiency. #### Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). ## 6. Troubleshooting ### FAQs Following are the responses to some of the most frequently asked questions: #### 1. How do I use Exynos AI Studio Service? The [Exynos AI Studio service](#enn-sdk-service) section provides detailed information on using the Exynos AI Studio service. #### 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. #### 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. <!-- ### Error Messages Following are some of the typical error messages that users may encounter, along with potential solutions: `To Be Updated` --> ## Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our [forums](https://soc-developer.semiconductor.samsung.com/user-lab/forum) for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our [1:1 Support](https://soc-developer.semiconductor.samsung.com/support/support11). # Introduction to Exynos AI High-Level Toolchain (EHT) **EHT** is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. ## System overview diagram ![overview_diagram](images/overview_diagram.png) ## [Converter](converter/converter.md) ## [Optimizer](optimizer/optimizer.md) ## [Quantizer](quantizer/quantizer.md) ## [Simulator](simulator/simulator.md) ## [Model Optimization Flow](model_optimization_flow/model_optimization_flow.md) # Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` # Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. ## Basic Quantization Methods ### Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. ### Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 ## Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. ## Debug API ### Layer-wise mixed precision quantiztion debug API The quantizer provides a"
    ],
    "reference": "Exynos AI Studio supports TensorFlow Lite models, which are recommended to be converted using MLIR version 1.14 or higher. Additionally, TensorFlow Lite models can have tensors up to four dimensions and a maximum size of 1 GB. In contrast, the context does not specify any particular limitations or requirements for standard TensorFlow models, indicating that TensorFlow Lite has specific guidelines for compatibility and optimization within the Exynos AI Studio framework.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:33:18.750554+09:00",
    "chatbot_server": "",
    "user_comment": ""
  },
  {
    "user_input": "What are the steps to convert a TensorFlow Lite model using Exynos AI Studio, and how does the optimization process differ for TensorFlow models?",
    "reference_contexts": [
      "<1-hop>\n\n[SSH_FALSE, SSH_TRUE] --- ### * eht yaml file** The optimization file contains optimization information. It allows you to set the locations of input and output models and detailed configurations for each module. Optimization is performed differently depending on the model type. ### **Detailed explanation for eht yaml file** --- model_type : string** Specify the type of the input model. Optimization is performed differently depending on the model type, with details as follows CV LVM LLM --- quantizer : dict** **precision_weight : str ** : precision of weight(ex. int8, int16, fp16) **precision_activation : str ** : precision of activation(ex. int8, int16, fp16) **mpq_operator_dict : dict ** : When performing mixed precision quantization, input the operators and precision to be quantized with a precision different from the values specified in precision_weight and precision_activation above. **alpha : float ** : smoothquant migration strength **calibration_data_path : string ** : The path to the representative data to be used for calibration **calibration_args : dict : ** Arguments for processing calibration **samples : int ** : How many calibration data samples to use **seed : int ** : A value set as a seed for random selection **add_dummy_conv: bool ** : Whether apply the dummy conv algorithm **input_dtype: dict ** : Input data type for the quantized model **output_dtype: dict ** : Output data type for the quantized model --- simulator : dict** **metric : string **: The metric to be used for measurement **threshold : float ** : The threshold value of the metric that determines agreement / disagreement **input_data_path : string **: The path to the dataset for model inference --- optimizer : dict** **skip_4_dim_conversion : bool ** : If true, it does not convert to 4 dimensions before optimization; if false, it proceeds with the conversion. **overwrite_input_shapes : dict ** : Enter the input shape for models with undefined input shapes. **custom_template_path : dict **: Enter the templates to apply for optimization. Use the path of the Python file containing the template as the key, and the template within it as the value. ### `enntools conversion` When performing conversion, users can choose between using eht and elt depending on the mode. The detailed workflow is illustrated in the figure below. ![conversion_workflow](images/conversion_workflow.png) ```bash enntools conversion ``` # Introduction to Exynos AI Studio **Exynos AI Studio** is a software that provides optimization and conversion techniques for neural networks models. This software takes models as input and offers functionalities such as quantization, conversion, optimization, compilation to generate NNC models. ## System overview diagram ![overview_diagram](images/overview_diagram.png) # Exynos AI Studio Documentation ## Exynos AI Studio References ### [Exynos AI Studio Developer Guide](developer-guide) This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. ### [API References](api-reference) This API reference documentation provides a list of data types and functions of Exynos AI Studio. ### [Support Matrix](support-matrix) These support matrices provide information on the supported platforms and operators of Exynos AI Studio. *** ## Exynos AI Studio Usage Guide ### [Quick Start Guide](quick-start-guide) This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. ### [Exynos AI Studio Samples](enn-sdk-samples) This guide provides a list of samples of Exynos AI Studio and their explanation. #### [Getting Started With Android Samples](getting-started-with-android-samples) This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. #### [Getting Started with Native Samples](getting-started-with-native-samples) This guide provides a walkthrough for developing a native program using Exynos AI Studio. -- Last Updated: 2023 11 21 0321 UTC # Quick Start ## Run docker container ```bash docker run it - gpus all name exynos_ai_studio_container \\ v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 ``` ## RUN tutorial ```bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init ``` ```bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ ``` modify config file and set mode ```bash enntools conversion ``` # Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. ## Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for **NPU**, the models must be quantized. ## Supported Operators Exynos AI Studio supports the following operators | **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O |",
      "<2-hop>\n\n1. **Download the Converted Model**: 1. If the conversion is successful, the **NNC Download** button is enabled. 1. Click NNC Download to download the NNC model. 1. Integrate the downloaded NNC model into your application required. ## 4. ENN tensor Framework APIs ### Data Type References For more information on the list of data types, refer to [API Reference](api-reference/#data-type-references). ### API Functions For more information on the list of API functions, refer to [API Reference](api-reference/#api-functions). ## 5. Advanced Topics ### Model Design Tips #### Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. | Architecture | Channel Alignment | | -- | -- | | Gen-4 | 32 | #### Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: **Option A **: Aligned corner: `False` Half pixel centers: `False` Performance: High speed **Option B **: Aligned corner: `True` Half pixel centers: `False` Compatibility: Gen 4 and later NPUs Performance: Medium speed **Option C **: Aligned corner: `False` Half pixel centers: `True` Note: Requires workaround Performance: Reduced speed #### Data Processing Procedures - **Pre-processing** and **Post-processing**: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. - **Memory Alignment**: Includes data transformation operations such as `split` and `reshape` during the pre-processing phase to ensure proper data alignment. #### Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. #### PReLU Use the `PReLU` activation function for optimal performance. Although `LeakyReLU` is functional, it may not provide the same level of efficiency. #### Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). ## 6. Troubleshooting ### FAQs Following are the responses to some of the most frequently asked questions: #### 1. How do I use Exynos AI Studio Service? The [Exynos AI Studio service](#enn-sdk-service) section provides detailed information on using the Exynos AI Studio service. #### 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. #### 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. <!-- ### Error Messages Following are some of the typical error messages that users may encounter, along with potential solutions: `To Be Updated` --> ## Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our [forums](https://soc-developer.semiconductor.samsung.com/user-lab/forum) for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our [1:1 Support](https://soc-developer.semiconductor.samsung.com/support/support11). # Introduction to Exynos AI High-Level Toolchain (EHT) **EHT** is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. ## System overview diagram ![overview_diagram](images/overview_diagram.png) ## [Converter](converter/converter.md) ## [Optimizer](optimizer/optimizer.md) ## [Quantizer](quantizer/quantizer.md) ## [Simulator](simulator/simulator.md) ## [Model Optimization Flow](model_optimization_flow/model_optimization_flow.md) # Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` # Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. ## Basic Quantization Methods ### Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. ### Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 ## Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. ## Debug API ### Layer-wise mixed precision quantiztion debug API The quantizer provides a"
    ],
    "reference": "To convert a TensorFlow Lite model using Exynos AI Studio, you first need to ensure that the model is compatible with the software. The recommended approach is to use MLIR version 1.14 or higher for the conversion. Once the model is ready, you can initiate the conversion process by using the command `enntools conversion`. If the conversion is successful, the NNC Download button will be enabled, allowing you to download the NNC model for integration into your application. The optimization process for TensorFlow models differs in that it involves specifying the model type in the optimization file, which allows for tailored optimization techniques based on whether the model is a computer vision model, a large language model, or a large vision model. This ensures that the optimization is performed differently depending on the model type, enhancing the overall performance.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:33:18.750554+09:00",
    "chatbot_server": "",
    "user_comment": ""
  },
  {
    "user_input": "What steps are involved in downloading a converted CNNX model after using the Exynos AI Studio Service?",
    "reference_contexts": [
      "<1-hop>\n\nmixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. # Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using *simulated quantization*. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - `get_quantization_sim`: Attain a CNNX quantization simulation session for manual run. ```python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) ``` `load_input_data`: Load the input data from the given dataset path. `run_inference`: Conduct inference on a CNNX model. `compare_model_by_inference`: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. `compare_model_by_layer` : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` # Exynos AI Studio ## Exynos AI Studio Service Use the [Exynos AI Studio Service](https://soc-developer.semiconductor.samsung.com/enn-sdk/project/) to convert trained TFLite models to NNC models by performing the following steps. ### Preparation 1. Access the ENN Ecosystem Portal - If you are a new user, sign up to create an account. - If you are a returning user, log in to ENN eco-system 1. Download Inception v4 `.tflite` model from [here](https://tfhub.dev/tensorflow/lite-model/inception_v4_quant/1/default/1). ### Project Creation 1. Navigate to the Exynos AI Studio Service [page](https://soc-developer.semiconductor.samsung.com/enn-sdk/project/). <img src=\"./img/ai ml_enn sdk_project.png\" alt=\"drawing\" width=\"400\"/> 1. Enter a descriptive title for your model. 1. For this guide, choose **Accelerate** hardware type. - **Default**: Utilizes only the CPU and GPU. - **Accelerate**: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the ***Accelerate*** option may lead to complications. 1. After confirming the selections, the subsequent screen appears: <img src=\"./img/ai ml_enn sdk_project_convert.png\" alt=\"drawing\" width=\"400\"/> ### Conversion 1. Select **Convert** to initiate the conversion process. 1. After the completion of conversion process, the **NNC Download** button is enabled. ### Download Model 1. Click **NNC Download** to obtain the converted NNC model file. 1. To view the logs for the conversion that has failed, click **Log Download**. You can download and examine the log files. 1. Copy the downloaded model to `${APP_ROOT}/app/src/main/assets`. # Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. ## Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as `Android.mk` and `Application.mk`. ### Android.mk The `Android.mk` file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := llog LOCAL_CFLAGS += Wall std=c++14 O3 LOCAL_CPPFLAGS += fexceptions frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` ### Application.mk: The `Application.mk` file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): ```cmake APP_ABI := arm64 v8a APP_STL := c++_static ``` ## Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. ```shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk build C . ``` This command instructs NDK to start the build process in the current directory. The `NDK_PROJECT_PATH` environment variable is set to the current directory. ## Verifying the Build After the build process is complete, the compiled program can be verified by checking the `libs` directory: ```shell ls libs/arm64-v8a/ ``` The compiled program is visible in the output. ## Troubleshooting If you encounter any issues during the build process, ensure the following: The `NDK_PROJECT_PATH` environment variable is correctly set. The `ANDROID_NDK_HOME` environment variable points to the correct location of the NDK installation. The paths in the `Android.mk` file are correct. # * How to use** ### `enntools initialization` To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. ```bash enntools init ``` ### * elt yaml file** These are the configuration values used in various modules of elt. ### **Detailed explanation for elt yaml file** --- **model_analyzer : dict** - **check : bool** : Check the op support status - **device : str** : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - **level : int** : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - **snc_input : bool** : analyze the snc model, false: analyze the original model --- **database_gen : dict** - **database_spec : str** : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) --- **converter : dict** - **device : str** : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - **do_quantize : bool** : Enable quantization - **eltsum_cvt : bool** : enable eltwise conversion process; eltsum -> concat + conv - **graph_opt : str** : Graph Optimizer",
      "<2-hop>\n\n1. **Download the Converted Model**: 1. If the conversion is successful, the **NNC Download** button is enabled. 1. Click NNC Download to download the NNC model. 1. Integrate the downloaded NNC model into your application required. ## 4. ENN tensor Framework APIs ### Data Type References For more information on the list of data types, refer to [API Reference](api-reference/#data-type-references). ### API Functions For more information on the list of API functions, refer to [API Reference](api-reference/#api-functions). ## 5. Advanced Topics ### Model Design Tips #### Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. | Architecture | Channel Alignment | | -- | -- | | Gen-4 | 32 | #### Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: **Option A **: Aligned corner: `False` Half pixel centers: `False` Performance: High speed **Option B **: Aligned corner: `True` Half pixel centers: `False` Compatibility: Gen 4 and later NPUs Performance: Medium speed **Option C **: Aligned corner: `False` Half pixel centers: `True` Note: Requires workaround Performance: Reduced speed #### Data Processing Procedures - **Pre-processing** and **Post-processing**: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. - **Memory Alignment**: Includes data transformation operations such as `split` and `reshape` during the pre-processing phase to ensure proper data alignment. #### Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. #### PReLU Use the `PReLU` activation function for optimal performance. Although `LeakyReLU` is functional, it may not provide the same level of efficiency. #### Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). ## 6. Troubleshooting ### FAQs Following are the responses to some of the most frequently asked questions: #### 1. How do I use Exynos AI Studio Service? The [Exynos AI Studio service](#enn-sdk-service) section provides detailed information on using the Exynos AI Studio service. #### 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. #### 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. <!-- ### Error Messages Following are some of the typical error messages that users may encounter, along with potential solutions: `To Be Updated` --> ## Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our [forums](https://soc-developer.semiconductor.samsung.com/user-lab/forum) for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our [1:1 Support](https://soc-developer.semiconductor.samsung.com/support/support11). # Introduction to Exynos AI High-Level Toolchain (EHT) **EHT** is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. ## System overview diagram ![overview_diagram](images/overview_diagram.png) ## [Converter](converter/converter.md) ## [Optimizer](optimizer/optimizer.md) ## [Quantizer](quantizer/quantizer.md) ## [Simulator](simulator/simulator.md) ## [Model Optimization Flow](model_optimization_flow/model_optimization_flow.md) # Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` # Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. ## Basic Quantization Methods ### Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. ### Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 ## Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. ## Debug API ### Layer-wise mixed precision quantiztion debug API The quantizer provides a"
    ],
    "reference": "After using the Exynos AI Studio Service to convert a model, the steps to download the converted CNNX model are as follows: If the conversion is successful, the **NNC Download** button is enabled. You need to click on the **NNC Download** button to download the NNC model. Once downloaded, you can integrate the NNC model into your application as required.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:33:18.750554+09:00",
    "chatbot_server": "",
    "user_comment": ""
  },
  {
    "user_input": "How can I download the NNC model after converting a CNNX model using the Exynos AI Studio Service?",
    "reference_contexts": [
      "<1-hop>\n\nmixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. # Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using *simulated quantization*. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - `get_quantization_sim`: Attain a CNNX quantization simulation session for manual run. ```python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) ``` `load_input_data`: Load the input data from the given dataset path. `run_inference`: Conduct inference on a CNNX model. `compare_model_by_inference`: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. `compare_model_by_layer` : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` # Exynos AI Studio ## Exynos AI Studio Service Use the [Exynos AI Studio Service](https://soc-developer.semiconductor.samsung.com/enn-sdk/project/) to convert trained TFLite models to NNC models by performing the following steps. ### Preparation 1. Access the ENN Ecosystem Portal - If you are a new user, sign up to create an account. - If you are a returning user, log in to ENN eco-system 1. Download Inception v4 `.tflite` model from [here](https://tfhub.dev/tensorflow/lite-model/inception_v4_quant/1/default/1). ### Project Creation 1. Navigate to the Exynos AI Studio Service [page](https://soc-developer.semiconductor.samsung.com/enn-sdk/project/). <img src=\"./img/ai ml_enn sdk_project.png\" alt=\"drawing\" width=\"400\"/> 1. Enter a descriptive title for your model. 1. For this guide, choose **Accelerate** hardware type. - **Default**: Utilizes only the CPU and GPU. - **Accelerate**: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the ***Accelerate*** option may lead to complications. 1. After confirming the selections, the subsequent screen appears: <img src=\"./img/ai ml_enn sdk_project_convert.png\" alt=\"drawing\" width=\"400\"/> ### Conversion 1. Select **Convert** to initiate the conversion process. 1. After the completion of conversion process, the **NNC Download** button is enabled. ### Download Model 1. Click **NNC Download** to obtain the converted NNC model file. 1. To view the logs for the conversion that has failed, click **Log Download**. You can download and examine the log files. 1. Copy the downloaded model to `${APP_ROOT}/app/src/main/assets`. # Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. ## Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as `Android.mk` and `Application.mk`. ### Android.mk The `Android.mk` file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := llog LOCAL_CFLAGS += Wall std=c++14 O3 LOCAL_CPPFLAGS += fexceptions frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` ### Application.mk: The `Application.mk` file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): ```cmake APP_ABI := arm64 v8a APP_STL := c++_static ``` ## Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. ```shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk build C . ``` This command instructs NDK to start the build process in the current directory. The `NDK_PROJECT_PATH` environment variable is set to the current directory. ## Verifying the Build After the build process is complete, the compiled program can be verified by checking the `libs` directory: ```shell ls libs/arm64-v8a/ ``` The compiled program is visible in the output. ## Troubleshooting If you encounter any issues during the build process, ensure the following: The `NDK_PROJECT_PATH` environment variable is correctly set. The `ANDROID_NDK_HOME` environment variable points to the correct location of the NDK installation. The paths in the `Android.mk` file are correct. # * How to use** ### `enntools initialization` To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. ```bash enntools init ``` ### * elt yaml file** These are the configuration values used in various modules of elt. ### **Detailed explanation for elt yaml file** --- **model_analyzer : dict** - **check : bool** : Check the op support status - **device : str** : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - **level : int** : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - **snc_input : bool** : analyze the snc model, false: analyze the original model --- **database_gen : dict** - **database_spec : str** : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) --- **converter : dict** - **device : str** : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - **do_quantize : bool** : Enable quantization - **eltsum_cvt : bool** : enable eltwise conversion process; eltsum -> concat + conv - **graph_opt : str** : Graph Optimizer",
      "<2-hop>\n\n1. **Download the Converted Model**: 1. If the conversion is successful, the **NNC Download** button is enabled. 1. Click NNC Download to download the NNC model. 1. Integrate the downloaded NNC model into your application required. ## 4. ENN tensor Framework APIs ### Data Type References For more information on the list of data types, refer to [API Reference](api-reference/#data-type-references). ### API Functions For more information on the list of API functions, refer to [API Reference](api-reference/#api-functions). ## 5. Advanced Topics ### Model Design Tips #### Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. | Architecture | Channel Alignment | | -- | -- | | Gen-4 | 32 | #### Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: **Option A **: Aligned corner: `False` Half pixel centers: `False` Performance: High speed **Option B **: Aligned corner: `True` Half pixel centers: `False` Compatibility: Gen 4 and later NPUs Performance: Medium speed **Option C **: Aligned corner: `False` Half pixel centers: `True` Note: Requires workaround Performance: Reduced speed #### Data Processing Procedures - **Pre-processing** and **Post-processing**: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. - **Memory Alignment**: Includes data transformation operations such as `split` and `reshape` during the pre-processing phase to ensure proper data alignment. #### Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. #### PReLU Use the `PReLU` activation function for optimal performance. Although `LeakyReLU` is functional, it may not provide the same level of efficiency. #### Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). ## 6. Troubleshooting ### FAQs Following are the responses to some of the most frequently asked questions: #### 1. How do I use Exynos AI Studio Service? The [Exynos AI Studio service](#enn-sdk-service) section provides detailed information on using the Exynos AI Studio service. #### 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. #### 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. <!-- ### Error Messages Following are some of the typical error messages that users may encounter, along with potential solutions: `To Be Updated` --> ## Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our [forums](https://soc-developer.semiconductor.samsung.com/user-lab/forum) for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our [1:1 Support](https://soc-developer.semiconductor.samsung.com/support/support11). # Introduction to Exynos AI High-Level Toolchain (EHT) **EHT** is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. ## System overview diagram ![overview_diagram](images/overview_diagram.png) ## [Converter](converter/converter.md) ## [Optimizer](optimizer/optimizer.md) ## [Quantizer](quantizer/quantizer.md) ## [Simulator](simulator/simulator.md) ## [Model Optimization Flow](model_optimization_flow/model_optimization_flow.md) # Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` # Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. ## Basic Quantization Methods ### Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. ### Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 ## Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. ## Debug API ### Layer-wise mixed precision quantiztion debug API The quantizer provides a"
    ],
    "reference": "To download the NNC model after converting a CNNX model using the Exynos AI Studio Service, first ensure that the conversion is successful. Once the conversion is complete, the **NNC Download** button will be enabled. You can then click on the **NNC Download** button to obtain the converted NNC model file.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:33:18.750554+09:00",
    "chatbot_server": "",
    "user_comment": ""
  },
  {
    "user_input": "What are the steps involved in converting TensorFlow Lite models to NNC models using Exynos AI Studio, and how does the optimization process differ based on the model type?",
    "reference_contexts": [
      "<1-hop>\n\n[SSH_FALSE, SSH_TRUE] --- ### * eht yaml file** The optimization file contains optimization information. It allows you to set the locations of input and output models and detailed configurations for each module. Optimization is performed differently depending on the model type. ### **Detailed explanation for eht yaml file** --- model_type : string** Specify the type of the input model. Optimization is performed differently depending on the model type, with details as follows CV LVM LLM --- quantizer : dict** **precision_weight : str ** : precision of weight(ex. int8, int16, fp16) **precision_activation : str ** : precision of activation(ex. int8, int16, fp16) **mpq_operator_dict : dict ** : When performing mixed precision quantization, input the operators and precision to be quantized with a precision different from the values specified in precision_weight and precision_activation above. **alpha : float ** : smoothquant migration strength **calibration_data_path : string ** : The path to the representative data to be used for calibration **calibration_args : dict : ** Arguments for processing calibration **samples : int ** : How many calibration data samples to use **seed : int ** : A value set as a seed for random selection **add_dummy_conv: bool ** : Whether apply the dummy conv algorithm **input_dtype: dict ** : Input data type for the quantized model **output_dtype: dict ** : Output data type for the quantized model --- simulator : dict** **metric : string **: The metric to be used for measurement **threshold : float ** : The threshold value of the metric that determines agreement / disagreement **input_data_path : string **: The path to the dataset for model inference --- optimizer : dict** **skip_4_dim_conversion : bool ** : If true, it does not convert to 4 dimensions before optimization; if false, it proceeds with the conversion. **overwrite_input_shapes : dict ** : Enter the input shape for models with undefined input shapes. **custom_template_path : dict **: Enter the templates to apply for optimization. Use the path of the Python file containing the template as the key, and the template within it as the value. ### `enntools conversion` When performing conversion, users can choose between using eht and elt depending on the mode. The detailed workflow is illustrated in the figure below. ![conversion_workflow](images/conversion_workflow.png) ```bash enntools conversion ``` # Introduction to Exynos AI Studio **Exynos AI Studio** is a software that provides optimization and conversion techniques for neural networks models. This software takes models as input and offers functionalities such as quantization, conversion, optimization, compilation to generate NNC models. ## System overview diagram ![overview_diagram](images/overview_diagram.png) # Exynos AI Studio Documentation ## Exynos AI Studio References ### [Exynos AI Studio Developer Guide](developer-guide) This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. ### [API References](api-reference) This API reference documentation provides a list of data types and functions of Exynos AI Studio. ### [Support Matrix](support-matrix) These support matrices provide information on the supported platforms and operators of Exynos AI Studio. *** ## Exynos AI Studio Usage Guide ### [Quick Start Guide](quick-start-guide) This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. ### [Exynos AI Studio Samples](enn-sdk-samples) This guide provides a list of samples of Exynos AI Studio and their explanation. #### [Getting Started With Android Samples](getting-started-with-android-samples) This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. #### [Getting Started with Native Samples](getting-started-with-native-samples) This guide provides a walkthrough for developing a native program using Exynos AI Studio. -- Last Updated: 2023 11 21 0321 UTC # Quick Start ## Run docker container ```bash docker run it - gpus all name exynos_ai_studio_container \\ v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 ``` ## RUN tutorial ```bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init ``` ```bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ ``` modify config file and set mode ```bash enntools conversion ``` # Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. ## Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for **NPU**, the models must be quantized. ## Supported Operators Exynos AI Studio supports the following operators | **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O |",
      "<2-hop>\n\nresult, 0 is success ### function EnnGetPreferencePerfMode ``` EnnReturn EnnGetPreferencePerfMode( uint32_t val_ptr ) ``` Get current information for Performance Mode. Parameters* : **val ** [OUT] current value of Performance Mode **Return**: EnnReturn result, 0 is success ### function EnnGetPreferenceTimeOut ``` EnnReturn EnnGetPreferenceTimeOut( uint32_t val_ptr ) ``` Get current information for Time Out. Parameters* : **val ** [OUT] current value of Time Out **Return**: EnnReturn result, 0 is success ### function EnnGetPreferencePriority ``` EnnReturn EnnGetPreferencePriority( uint32_t val_ptr ) ``` Get current information for NPU Priority. Parameters* : **val ** [OUT] current value of NPU Priority **Return**: EnnReturn result, 0 is success ### function EnnGetPreferenceCoreAffinity ``` EnnReturn EnnGetPreferenceCoreAffinity( uint32_t val_ptr ) ``` Get current information for NPU Core affinity. Parameters* : **val ** [OUT] current value of NPU Core affinity **Return**: EnnReturn result, 0 is success ### function EnnGetMetaInfo ``` EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) ``` Get Meta Information. Parameters* : **info_id ** info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. ```cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW ``` model_id* output_str** **Return**: EnnReturn result, 0 is success This API includes loaded model information as well as framework information ### function EnnSetExecMsgAlwaysOn ``` EnnReturn EnnSetExecMsgAlwaysOn() ``` Set frequency of execution message print. Parameters* : **rate ** if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return* : EnnReturn ------------------------------- Updated on 2023 08 11 at 16:24:05 +0900 # * Dataset preparation** This is a guideline for preparing the input dataset. ### * Dataset format** The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. # Model Requirements and Constraints ### model format Model should be prepared in ONNX format to start optimization. ### opset version EHT currently support ONNX opset version 13 ~ 17. # Exynos AI Studio Developer Guide ## Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. ## 1. Introduction [Exynos AI Studio](https://soc-developer.semiconductor.samsung.com/enn-sdk) allows users to convert trained [TensorFlow Lite](https://www.tensorflow.org/lite) neural network models to a format that can run efficiently in [Samsung Exynos](https://semiconductor.samsung.com/processor/) hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. ### Structure of Documentation - [Chapter 1](#1-introduction) introduces Exynos AI Studio and its eco-system. - [Chapter 2](#2-enn-sdks-capabilities) provides information on the features of Exynos AI Studio. - [Chapter 3](#3-tools) provides information on tools provided with Exynos AI Studio. - [Chapter 4](#4-enn-framework-api) provides information on ENN framework API. - The subsequent chapters provide additional information on Exynos AI Studio. ### Samples The list of samples for Exynos AI Studio is available in [Exynos AI Studio Samples](enn-sdk-samples). ### Support Support materials including forums, FAQs, and others are available at the [Exynos Eco-system web page](https://soc-developer.semiconductor.samsung.com/). ### Reporting Bugs To report a bug or issue, follow the instructions described in the [Reporting Exynos AI Studio Issues](#reporting-enn-sdk-issues). ## 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. ### Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. #### Model Conversion Use one of the [tools](#3-tools) that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. #### Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. > To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. ### Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the [Support Matrix](support-matrix). ## 3. Tools ### Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: 1. **Access the ENN Eco-system**: - If you are a new user, [sign up](https://soc-developer.semiconductor.samsung.com/register) to create an account. - If you are an existing user, log in to ENN eco-system. 1. **Navigate to the Service**: - Visit the [Exynos AI Studio service](https://soc-developer.semiconductor.samsung.com/enn-sdk/project/) page. 1. **Provide Project Information**: 1. Enter a descriptive title for your project. 1. Use the provided interface to upload your TFLite model. 1. **Choose Hardware Preferences**: - **Default**: Utilizes only the CPU and GPU. - **Accelerate**: Engages the NPU as an additional accelerator. > ***Warning***: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to [Support Matix](support-matrix). 1. **Initiate the Conversion**: 1. Click **Confirm** to verify your selections. 1. Click **Convert** to start the model conversion process."
    ],
    "reference": "To convert TensorFlow Lite models to NNC models using Exynos AI Studio, the following steps are involved: First, prepare a pre-trained NN model. Next, set parameters for the tools provided by Exynos AI Studio. After that, execute the tools for conversion. The optimization process is performed differently depending on the model type, as specified in the optimization file. For instance, the model type can be specified as CV, LVM, or LLM, and the optimization details will vary accordingly. Additionally, the optimization file allows for configurations such as quantization settings, input and output data types, and other parameters that influence how the model is optimized for deployment.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:33:18.750554+09:00",
    "chatbot_server": "",
    "user_comment": ""
  },
  {
    "user_input": "What are the requirements for models to be compatible with the NPU in Exynos AI Studio, and how does the performance mode affect NPU operations?",
    "reference_contexts": [
      "<1-hop>\n\n[SSH_FALSE, SSH_TRUE] --- ### * eht yaml file** The optimization file contains optimization information. It allows you to set the locations of input and output models and detailed configurations for each module. Optimization is performed differently depending on the model type. ### **Detailed explanation for eht yaml file** --- model_type : string** Specify the type of the input model. Optimization is performed differently depending on the model type, with details as follows CV LVM LLM --- quantizer : dict** **precision_weight : str ** : precision of weight(ex. int8, int16, fp16) **precision_activation : str ** : precision of activation(ex. int8, int16, fp16) **mpq_operator_dict : dict ** : When performing mixed precision quantization, input the operators and precision to be quantized with a precision different from the values specified in precision_weight and precision_activation above. **alpha : float ** : smoothquant migration strength **calibration_data_path : string ** : The path to the representative data to be used for calibration **calibration_args : dict : ** Arguments for processing calibration **samples : int ** : How many calibration data samples to use **seed : int ** : A value set as a seed for random selection **add_dummy_conv: bool ** : Whether apply the dummy conv algorithm **input_dtype: dict ** : Input data type for the quantized model **output_dtype: dict ** : Output data type for the quantized model --- simulator : dict** **metric : string **: The metric to be used for measurement **threshold : float ** : The threshold value of the metric that determines agreement / disagreement **input_data_path : string **: The path to the dataset for model inference --- optimizer : dict** **skip_4_dim_conversion : bool ** : If true, it does not convert to 4 dimensions before optimization; if false, it proceeds with the conversion. **overwrite_input_shapes : dict ** : Enter the input shape for models with undefined input shapes. **custom_template_path : dict **: Enter the templates to apply for optimization. Use the path of the Python file containing the template as the key, and the template within it as the value. ### `enntools conversion` When performing conversion, users can choose between using eht and elt depending on the mode. The detailed workflow is illustrated in the figure below. ![conversion_workflow](images/conversion_workflow.png) ```bash enntools conversion ``` # Introduction to Exynos AI Studio **Exynos AI Studio** is a software that provides optimization and conversion techniques for neural networks models. This software takes models as input and offers functionalities such as quantization, conversion, optimization, compilation to generate NNC models. ## System overview diagram ![overview_diagram](images/overview_diagram.png) # Exynos AI Studio Documentation ## Exynos AI Studio References ### [Exynos AI Studio Developer Guide](developer-guide) This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. ### [API References](api-reference) This API reference documentation provides a list of data types and functions of Exynos AI Studio. ### [Support Matrix](support-matrix) These support matrices provide information on the supported platforms and operators of Exynos AI Studio. *** ## Exynos AI Studio Usage Guide ### [Quick Start Guide](quick-start-guide) This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. ### [Exynos AI Studio Samples](enn-sdk-samples) This guide provides a list of samples of Exynos AI Studio and their explanation. #### [Getting Started With Android Samples](getting-started-with-android-samples) This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. #### [Getting Started with Native Samples](getting-started-with-native-samples) This guide provides a walkthrough for developing a native program using Exynos AI Studio. -- Last Updated: 2023 11 21 0321 UTC # Quick Start ## Run docker container ```bash docker run it - gpus all name exynos_ai_studio_container \\ v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 ``` ## RUN tutorial ```bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init ``` ```bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ ``` modify config file and set mode ```bash enntools conversion ``` # Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. ## Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for **NPU**, the models must be quantized. ## Supported Operators Exynos AI Studio supports the following operators | **Index** | **Operator_Name** | **TFLite** | **NPU** | **DSP** | **GPU** | **CPU** | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O |",
      "<2-hop>\n\nresult, 0 is success ### function EnnGetPreferencePerfMode ``` EnnReturn EnnGetPreferencePerfMode( uint32_t val_ptr ) ``` Get current information for Performance Mode. Parameters* : **val ** [OUT] current value of Performance Mode **Return**: EnnReturn result, 0 is success ### function EnnGetPreferenceTimeOut ``` EnnReturn EnnGetPreferenceTimeOut( uint32_t val_ptr ) ``` Get current information for Time Out. Parameters* : **val ** [OUT] current value of Time Out **Return**: EnnReturn result, 0 is success ### function EnnGetPreferencePriority ``` EnnReturn EnnGetPreferencePriority( uint32_t val_ptr ) ``` Get current information for NPU Priority. Parameters* : **val ** [OUT] current value of NPU Priority **Return**: EnnReturn result, 0 is success ### function EnnGetPreferenceCoreAffinity ``` EnnReturn EnnGetPreferenceCoreAffinity( uint32_t val_ptr ) ``` Get current information for NPU Core affinity. Parameters* : **val ** [OUT] current value of NPU Core affinity **Return**: EnnReturn result, 0 is success ### function EnnGetMetaInfo ``` EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) ``` Get Meta Information. Parameters* : **info_id ** info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. ```cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW ``` model_id* output_str** **Return**: EnnReturn result, 0 is success This API includes loaded model information as well as framework information ### function EnnSetExecMsgAlwaysOn ``` EnnReturn EnnSetExecMsgAlwaysOn() ``` Set frequency of execution message print. Parameters* : **rate ** if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return* : EnnReturn ------------------------------- Updated on 2023 08 11 at 16:24:05 +0900 # * Dataset preparation** This is a guideline for preparing the input dataset. ### * Dataset format** The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. # Model Requirements and Constraints ### model format Model should be prepared in ONNX format to start optimization. ### opset version EHT currently support ONNX opset version 13 ~ 17. # Exynos AI Studio Developer Guide ## Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. ## 1. Introduction [Exynos AI Studio](https://soc-developer.semiconductor.samsung.com/enn-sdk) allows users to convert trained [TensorFlow Lite](https://www.tensorflow.org/lite) neural network models to a format that can run efficiently in [Samsung Exynos](https://semiconductor.samsung.com/processor/) hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. ### Structure of Documentation - [Chapter 1](#1-introduction) introduces Exynos AI Studio and its eco-system. - [Chapter 2](#2-enn-sdks-capabilities) provides information on the features of Exynos AI Studio. - [Chapter 3](#3-tools) provides information on tools provided with Exynos AI Studio. - [Chapter 4](#4-enn-framework-api) provides information on ENN framework API. - The subsequent chapters provide additional information on Exynos AI Studio. ### Samples The list of samples for Exynos AI Studio is available in [Exynos AI Studio Samples](enn-sdk-samples). ### Support Support materials including forums, FAQs, and others are available at the [Exynos Eco-system web page](https://soc-developer.semiconductor.samsung.com/). ### Reporting Bugs To report a bug or issue, follow the instructions described in the [Reporting Exynos AI Studio Issues](#reporting-enn-sdk-issues). ## 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. ### Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. #### Model Conversion Use one of the [tools](#3-tools) that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. #### Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. > To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. ### Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the [Support Matrix](support-matrix). ## 3. Tools ### Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: 1. **Access the ENN Eco-system**: - If you are a new user, [sign up](https://soc-developer.semiconductor.samsung.com/register) to create an account. - If you are an existing user, log in to ENN eco-system. 1. **Navigate to the Service**: - Visit the [Exynos AI Studio service](https://soc-developer.semiconductor.samsung.com/enn-sdk/project/) page. 1. **Provide Project Information**: 1. Enter a descriptive title for your project. 1. Use the provided interface to upload your TFLite model. 1. **Choose Hardware Preferences**: - **Default**: Utilizes only the CPU and GPU. - **Accelerate**: Engages the NPU as an additional accelerator. > ***Warning***: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to [Support Matix](support-matrix). 1. **Initiate the Conversion**: 1. Click **Confirm** to verify your selections. 1. Click **Convert** to start the model conversion process."
    ],
    "reference": "To be compatible with the NPU in Exynos AI Studio, models must be quantized and prepared in ONNX format. Additionally, the models should support tensors with up to four dimensions and have a maximum size of 1 GB. The performance mode can be adjusted using the function EnnGetPreferencePerfMode, which retrieves the current value of the performance mode for the NPU. This setting can influence how efficiently the NPU operates during model execution.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 17:33:18.750554+09:00",
    "chatbot_server": "",
    "user_comment": ""
  },
  {
    "user_input": "What is the purpose of the ENN Framwork in AI application development?",
    "reference_contexts": [
      "Introduction EHT Converter Optimizer Quantizer Simulator Model Optimization Flow ELT Model Analyzer SCVT Quantizer NPU Simulator Performance Estimator Graph Generator Dataset and Model Quick Start System Requirements How to use ENN Framework Data Type References _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; API reference ENN Framework Data Type References _NumberOfBuffersInfo _ennBuffer _ennBufferInfo ENN Framwork API Functions Context initialize / deinitialize OpenModel / CloseModel related Memory Handling Setters and Getters for model Commit Buffer Execute Models Security, preference, get meta information.. title: _ennBuffer _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; Updated on 2023-08-11 at 16:24:05 +0900 title: _ennBufferInfo _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; Updated on 2023-08-11 at 16:24:05 +0900 title: _NumberOfBuffersInfo _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; Updated on 2023-08-11 at 16:24:05 +0900 title: Commit Buffer Commit Buffer Functions Name EnnReturn EnnBufferCommit (const EnnModelId model_id, const int session_id =0) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Functions Documentation function EnnBufferCommit EnnReturn EnnBufferCommit( const EnnModelId model_id, const int session_id =0 ) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Parameters: model_id [IN] model ID from load_model Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Context initialize / deinitialize Context initialize / deinitialize Functions Name EnnReturn EnnInitialize (void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. EnnReturn EnnDeinitialize (void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Functions Documentation function EnnInitialize EnnReturn EnnInitialize( void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. Return: EnnReturn result, 0 is success function EnnDeinitialize EnnReturn EnnDeinitialize( void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Security, preference, get meta information.. Security, preference, get meta information.. Functions Name EnnReturn EnnSetPreferencePresetId (const uint32_t val) Setting Preset ID for operation performance. EnnReturn EnnSetPreferencePerfConfigId (const uint32_t val) Setting PerfConfig ID for operation performance. EnnReturn EnnSetPreferencePerfMode (const uint32_t val) Setting Performance Mode. EnnReturn EnnSetPreferenceTimeOut (const uint32_t val) Setting Preset ID for time out. EnnReturn EnnSetPreferencePriority (const uint32_t val) Setting priority value for NPU. EnnReturn EnnSetPreferenceCoreAffinity (const uint32_t val) Setting affinity to set NPU core operation. EnnReturn EnnGetPreferencePresetId (uint32_t * val_ptr) Get current information for Preset ID. EnnReturn EnnGetPreferencePerfConfigId (uint32_t * val_ptr) Get current information for PerfConfig ID. EnnReturn EnnGetPreferencePerfMode (uint32_t * val_ptr) Get current information for Performance Mode. EnnReturn EnnGetPreferenceTimeOut (uint32_t * val_ptr) Get current information for Time Out. EnnReturn EnnGetPreferencePriority (uint32_t * val_ptr) Get current information for NPU Priority. EnnReturn EnnGetPreferenceCoreAffinity (uint32_t * val_ptr) Get current information for NPU Core affinity. EnnReturn EnnGetMetaInfo (const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]) Get Meta Information. EnnReturn EnnSetExecMsgAlwaysOn () Set frequency of execution message print. Functions Documentation function EnnSetPreferencePresetId EnnReturn EnnSetPreferencePresetId( const uint32_t val ) Setting Preset ID for operation performance. Parameters: val [IN] value to set preset ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfConfigId EnnReturn EnnSetPreferencePerfConfigId( const uint32_t val ) Setting PerfConfig ID for operation performance. Parameters: val [IN] value to set PerfConfig ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfMode EnnReturn EnnSetPreferencePerfMode( const uint32_t val ) Setting Performance Mode. Parameters: val [IN] value to set Performance Mode Return: EnnReturn result, 0 is success function EnnSetPreferenceTimeOut EnnReturn EnnSetPreferenceTimeOut( const uint32_t val ) Setting Preset ID for time out. Parameters: val [IN] value to set time out Return: EnnReturn result, 0 is success Note: in second function EnnSetPreferencePriority EnnReturn EnnSetPreferencePriority( const uint32_t val ) Setting priority value for NPU. Parameters: val [IN] value to set NPU job priority Return: EnnReturn result, 0 is success function EnnSetPreferenceCoreAffinity EnnReturn EnnSetPreferenceCoreAffinity( const uint32_t val ) Setting affinity to set NPU core operation. Parameters: val [IN] value to set affinity Return: EnnReturn result, 0 is success Note: in second function EnnGetPreferencePresetId EnnReturn EnnGetPreferencePresetId( uint32_t * val_ptr ) Get current information for Preset ID. Parameters: val [OUT] current value of Preset ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfConfigId EnnReturn EnnGetPreferencePerfConfigId( uint32_t * val_ptr ) Get current information for PerfConfig ID. Parameters: val [OUT] current value of PerfConfig ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfMode EnnReturn EnnGetPreferencePerfMode( uint32_t * val_ptr ) Get current information for Performance Mode. Parameters: val [OUT] current value of Performance Mode Return: EnnReturn result, 0 is success function EnnGetPreferenceTimeOut EnnReturn EnnGetPreferenceTimeOut( uint32_t * val_ptr ) Get current information for Time Out. Parameters: val [OUT] current value of Time Out Return: EnnReturn result, 0 is success function EnnGetPreferencePriority EnnReturn EnnGetPreferencePriority( uint32_t * val_ptr ) Get current information for NPU Priority. Parameters: val [OUT] current value of NPU Priority Return:"
    ],
    "reference": "The ENN Framework is designed to initialize and manage the context for AI applications, allowing developers to optimize neural network models specifically for Samsung Exynos hardware.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:09:12.255676+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is the primary function of Exynos AI Studio?",
    "reference_contexts": [
      "Index EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:09:12.255676+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is the maximum number of projects that can be created in Exynos AI Studio Service?",
    "reference_contexts": [
      "Troubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For"
    ],
    "reference": "Users can create a maximum of five projects with the Exynos AI Studio service.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:09:12.255676+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is Gen-5 in the context of AI application development?",
    "reference_contexts": [
      "this guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "Gen-5 is a System on Chip (SoC) type mentioned in the context of model analysis and conversion processes. It is one of the device options available for analyzing models and is included in the list of supported SoC types for various operations.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:09:12.255676+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the device types available in the context of Gen-4 for the Exynos AI Studio?",
    "reference_contexts": [
      "str : debug str for compiler - dequant_type : str : dequantiztion type - device : str : System on Chip type [Gen-3, Gen-3b, Gen-4, Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - enable_ofm_reuse : bool : Enable the reuse of OFM region for IMFM. - enable_stm : bool : Generate compile log including L1 tiling information - flc : bool : Enable featuremap lossless compression - fp16_swwa : bool : Enable NPU fp16 workaround with psum_init - input_conversion : str : Add a Tensor2Cell format converter node at start of network - mi : bool : multiple input compile - mo : bool : multiple output compile - multi_ncp : bool : generate multi-ncp(ucgo) custom op - multi_vc : bool : Introduce Multi-VC for OFM, IFM, and weight transfer - multicore : bool : Enable NPU multicore - optimization : str : Optimization choice [O1, O2, O3] - output_conversion : str : Add a Tensor2Cell format converter node at end of network - packed_ucgo : bool : true: generate one ucgo in nnc, false: generate multiple ucgos in nnc if there are multiple independent dsp OPs - preemption : bool : Setting priority of NNC while compiling - quant_type : str : quantiztion type - sync_npu_dsp : bool : simulator : dict - data_format : str : Indicate the position of channel of input [channel_first, channel_last] - use_randomdb : bool : Use randomdb to forward, just support single input - userdb : str : Simulation data set path (default path is {workspace}/DATA/data.txt) perf_estimator : dict - O2_enable : bool : O2 optimization (true or false) - O2_fm_forwarding : bool : feature-map forwarding (true or false) - SEG : bool : Set true if input model is Deeplab V3+ - SSD : bool : Set true if input model is SSD detection - bit_width_factor_FM : int : Select feature map bit width factor (1 or 2) - bit_width_factor_FP16 : bool : Set bit width factor as floating point (true or false) - bit_width_factor_weight : int : Select weight bit width factor (1 or 2) - core_num : int : 1 for single core, 2 for instance-1 - device : str : Select device type [Gen-3, Gen-4, Gen-5, Gen-5a] - json_report : bool : Enable report json format - nq_fold : bool : Speed-up by folding normalization parameters for input data (true: model input dtype is uint8, false: model input dtype is float32) profiler : dict - iter : int : This decides how many time the model inference will be processed. - mode : str : 1. Low power mode : Mode for low power consumption and low performance. 2. Balanced mode : Mode for medium power consumption and medium performance. 3. Performance mode : Mode for high power consumption and high performance. 4. Boost mode : Mode for very high power consumption and very high performance. [lowpower, balanced, performance, boost] - target : str : profiling target. [model, system] - test_type : str : ENN running mode [lib, service] - tv_threshold : float : The value is used for tolerance threshold of output match verification. - bitmatch_test : bool : if set true, visual profiler will compile nnc first - core_num : str : The number of NPU core. [single, multiple] - device : str : Target device. [Gen-3, Gen-4, Gen-5, Gen-5a, Gen-6, Gen-6npu, Gen-7] - device_id : str : id of the device connected to the server or PC running the enntools docker - remote_ssh_config_path : str : remote testing ssh config path (default path is {workspace}/DATA/remote_ssh_config.yaml) - ssh_bool : str : Connect to the device through ssh. [SSH_FALSE, SSH_TRUE] eht yaml file The optimization file contains optimization information. It allows you to set the locations of input and output models and detailed configurations for each module. Optimization is performed differently depending on the model type. Detailed explanation for eht yaml file model_type : string Specify the type of the input model. Optimization is performed differently depending on the model type, with details as follows CV LVM LLM quantizer : dict precision_weight : str : precision of weight(ex. int8, int16, fp16) precision_activation : str : precision of activation(ex. int8, int16, fp16) mpq_operator_dict : dict : When performing mixed precision quantization, input the operators and precision to be quantized with a precision different from the values specified in precision_weight and precision_activation above. alpha : float : smoothquant migration strength calibration_data_path : string : The path to the representative data to be used for calibration calibration_args : dict : Arguments for processing calibration samples : int : How many calibration data samples to use seed : int : A value set as a seed for random selection add_dummy_conv: bool : Whether apply the dummy conv algorithm input_dtype: dict : Input data type for the quantized model output_dtype: dict : Output data type for the quantized model simulator : dict metric : string: The metric to be used for measurement threshold : float : The threshold value of the metric that determines agreement / disagreement input_data_path : string: The path to the dataset for model inference optimizer : dict skip_4_dim_conversion : bool : If true, it does not convert to 4 dimensions before optimization; if false, it proceeds with the conversion. overwrite_input_shapes : dict : Enter the input shape for models with undefined input shapes. custom_template_path : dict: Enter the templates to apply for optimization. Use the path of the Python file containing the template as the key, and the template within it as the value. enntools conversion When performing conversion, users can choose between using eht and elt depending on the mode. The detailed workflow is illustrated in the figure below. bash enntools conversion Introduction to Exynos AI Studio Exynos AI Studio is a software that provides optimization and conversion techniques for neural networks models. This software takes models as input and offers functionalities such as quantization, conversion, optimization, compilation to generate NNC models. System overview diagram Exynos AI Studio Documentation Exynos AI Studio References Exynos AI Studio Developer Guide This guide"
    ],
    "reference": "The device types available in the context of Gen-4 for the Exynos AI Studio include Gen-3, Gen-4, Gen-5, and Gen-5a.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:09:12.255676+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps involved in executing a model using the ENN framework in Exynos AI Studio?",
    "reference_contexts": [
      "<1-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3.",
      "<2-hop>\n\nIntroduction EHT Converter Optimizer Quantizer Simulator Model Optimization Flow ELT Model Analyzer SCVT Quantizer NPU Simulator Performance Estimator Graph Generator Dataset and Model Quick Start System Requirements How to use ENN Framework Data Type References _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; API reference ENN Framework Data Type References _NumberOfBuffersInfo _ennBuffer _ennBufferInfo ENN Framwork API Functions Context initialize / deinitialize OpenModel / CloseModel related Memory Handling Setters and Getters for model Commit Buffer Execute Models Security, preference, get meta information.. title: _ennBuffer _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; Updated on 2023-08-11 at 16:24:05 +0900 title: _ennBufferInfo _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; Updated on 2023-08-11 at 16:24:05 +0900 title: _NumberOfBuffersInfo _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; Updated on 2023-08-11 at 16:24:05 +0900 title: Commit Buffer Commit Buffer Functions Name EnnReturn EnnBufferCommit (const EnnModelId model_id, const int session_id =0) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Functions Documentation function EnnBufferCommit EnnReturn EnnBufferCommit( const EnnModelId model_id, const int session_id =0 ) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Parameters: model_id [IN] model ID from load_model Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Context initialize / deinitialize Context initialize / deinitialize Functions Name EnnReturn EnnInitialize (void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. EnnReturn EnnDeinitialize (void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Functions Documentation function EnnInitialize EnnReturn EnnInitialize( void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. Return: EnnReturn result, 0 is success function EnnDeinitialize EnnReturn EnnDeinitialize( void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Security, preference, get meta information.. Security, preference, get meta information.. Functions Name EnnReturn EnnSetPreferencePresetId (const uint32_t val) Setting Preset ID for operation performance. EnnReturn EnnSetPreferencePerfConfigId (const uint32_t val) Setting PerfConfig ID for operation performance. EnnReturn EnnSetPreferencePerfMode (const uint32_t val) Setting Performance Mode. EnnReturn EnnSetPreferenceTimeOut (const uint32_t val) Setting Preset ID for time out. EnnReturn EnnSetPreferencePriority (const uint32_t val) Setting priority value for NPU. EnnReturn EnnSetPreferenceCoreAffinity (const uint32_t val) Setting affinity to set NPU core operation. EnnReturn EnnGetPreferencePresetId (uint32_t * val_ptr) Get current information for Preset ID. EnnReturn EnnGetPreferencePerfConfigId (uint32_t * val_ptr) Get current information for PerfConfig ID. EnnReturn EnnGetPreferencePerfMode (uint32_t * val_ptr) Get current information for Performance Mode. EnnReturn EnnGetPreferenceTimeOut (uint32_t * val_ptr) Get current information for Time Out. EnnReturn EnnGetPreferencePriority (uint32_t * val_ptr) Get current information for NPU Priority. EnnReturn EnnGetPreferenceCoreAffinity (uint32_t * val_ptr) Get current information for NPU Core affinity. EnnReturn EnnGetMetaInfo (const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]) Get Meta Information. EnnReturn EnnSetExecMsgAlwaysOn () Set frequency of execution message print. Functions Documentation function EnnSetPreferencePresetId EnnReturn EnnSetPreferencePresetId( const uint32_t val ) Setting Preset ID for operation performance. Parameters: val [IN] value to set preset ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfConfigId EnnReturn EnnSetPreferencePerfConfigId( const uint32_t val ) Setting PerfConfig ID for operation performance. Parameters: val [IN] value to set PerfConfig ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfMode EnnReturn EnnSetPreferencePerfMode( const uint32_t val ) Setting Performance Mode. Parameters: val [IN] value to set Performance Mode Return: EnnReturn result, 0 is success function EnnSetPreferenceTimeOut EnnReturn EnnSetPreferenceTimeOut( const uint32_t val ) Setting Preset ID for time out. Parameters: val [IN] value to set time out Return: EnnReturn result, 0 is success Note: in second function EnnSetPreferencePriority EnnReturn EnnSetPreferencePriority( const uint32_t val ) Setting priority value for NPU. Parameters: val [IN] value to set NPU job priority Return: EnnReturn result, 0 is success function EnnSetPreferenceCoreAffinity EnnReturn EnnSetPreferenceCoreAffinity( const uint32_t val ) Setting affinity to set NPU core operation. Parameters: val [IN] value to set affinity Return: EnnReturn result, 0 is success Note: in second function EnnGetPreferencePresetId EnnReturn EnnGetPreferencePresetId( uint32_t * val_ptr ) Get current information for Preset ID. Parameters: val [OUT] current value of Preset ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfConfigId EnnReturn EnnGetPreferencePerfConfigId( uint32_t * val_ptr ) Get current information for PerfConfig ID. Parameters: val [OUT] current value of PerfConfig ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfMode EnnReturn EnnGetPreferencePerfMode( uint32_t * val_ptr ) Get current information for Performance Mode. Parameters: val [OUT] current value of Performance Mode Return: EnnReturn result, 0 is success function EnnGetPreferenceTimeOut EnnReturn EnnGetPreferenceTimeOut( uint32_t * val_ptr ) Get current information for Time Out. Parameters: val [OUT] current value of Time Out Return: EnnReturn result, 0 is success function EnnGetPreferencePriority EnnReturn EnnGetPreferencePriority( uint32_t * val_ptr ) Get current information for NPU Priority. Parameters: val [OUT] current value of NPU Priority Return:"
    ],
    "reference": "To execute a model using the ENN framework in Exynos AI Studio, the following steps are involved: 1. Initialize the ENN framework. 2. Load the converted model into the ENN framework. 3. Allocate and commit all the necessary buffers for the model. 4. Copy input data to the input buffers. 5. Execute the model on the ENN framework. 6. Use data from the output buffers. To execute the model multiple times, repeat this process. Finally, perform the following cleanup steps: 1. Uncommit and release buffers allocated to the model. 2. Unload the model. 3. Deinitialize the ENN framework.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:09:12.255676+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps involved in using the ENN framework for executing a converted model in Exynos AI Studio, and how does this relate to the conversion of NN models to NNC models?",
    "reference_contexts": [
      "<1-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3.",
      "<2-hop>\n\ndescribes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "Using the ENN framework for executing a converted model in Exynos AI Studio involves several steps. First, the user must initialize the ENN framework. Next, the converted model is loaded into the ENN framework, followed by the allocation and commitment of all necessary buffers for the model. After that, input data is copied to the input buffers, and the model is executed on the ENN framework. Once the execution is complete, the user can use the data from the output buffers. To execute the model multiple times, this process is repeated. Finally, the user must uncommit and release the buffers allocated to the model, unload the model, and deinitialize the ENN framework. This process is crucial as it follows the initial step of converting Neural Network (NN) models to Neural Network Container (NNC) models, which is necessary for efficient execution on Samsung Exynos hardware.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:09:12.255676+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How can I convert a TFLite model to an NNC model using Exynos AI Studio, and what are the supported models and operators for this conversion?",
    "reference_contexts": [
      "<1-hop>\n\nTroubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For",
      "<2-hop>\n\ndescribes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "To convert a TFLite model to an NNC model using Exynos AI Studio, you can follow these steps: First, access the ENN Ecosystem Portal and create an account if you are a new user. Then, navigate to the Exynos AI Studio Service page and enter a descriptive title for your model. Exynos AI Studio supports the following models for conversion: TensorFlow Lite models, which should be converted using MLIR version 1.14 or higher, and models with a maximum size of 1 GB. Additionally, for NPU, the models must be quantized. The supported operators include various functions such as ABS, ADD, ARGMAX, AVGPOOL, CONVOLUTION, and many others, ensuring a wide range of functionalities for neural network applications.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:09:12.255676+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the differences in device support between Gen-5 and Gen-5a in the context of using enntools for model optimization?",
    "reference_contexts": [
      "<1-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :",
      "<2-hop>\n\nstr : debug str for compiler - dequant_type : str : dequantiztion type - device : str : System on Chip type [Gen-3, Gen-3b, Gen-4, Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - enable_ofm_reuse : bool : Enable the reuse of OFM region for IMFM. - enable_stm : bool : Generate compile log including L1 tiling information - flc : bool : Enable featuremap lossless compression - fp16_swwa : bool : Enable NPU fp16 workaround with psum_init - input_conversion : str : Add a Tensor2Cell format converter node at start of network - mi : bool : multiple input compile - mo : bool : multiple output compile - multi_ncp : bool : generate multi-ncp(ucgo) custom op - multi_vc : bool : Introduce Multi-VC for OFM, IFM, and weight transfer - multicore : bool : Enable NPU multicore - optimization : str : Optimization choice [O1, O2, O3] - output_conversion : str : Add a Tensor2Cell format converter node at end of network - packed_ucgo : bool : true: generate one ucgo in nnc, false: generate multiple ucgos in nnc if there are multiple independent dsp OPs - preemption : bool : Setting priority of NNC while compiling - quant_type : str : quantiztion type - sync_npu_dsp : bool : simulator : dict - data_format : str : Indicate the position of channel of input [channel_first, channel_last] - use_randomdb : bool : Use randomdb to forward, just support single input - userdb : str : Simulation data set path (default path is {workspace}/DATA/data.txt) perf_estimator : dict - O2_enable : bool : O2 optimization (true or false) - O2_fm_forwarding : bool : feature-map forwarding (true or false) - SEG : bool : Set true if input model is Deeplab V3+ - SSD : bool : Set true if input model is SSD detection - bit_width_factor_FM : int : Select feature map bit width factor (1 or 2) - bit_width_factor_FP16 : bool : Set bit width factor as floating point (true or false) - bit_width_factor_weight : int : Select weight bit width factor (1 or 2) - core_num : int : 1 for single core, 2 for instance-1 - device : str : Select device type [Gen-3, Gen-4, Gen-5, Gen-5a] - json_report : bool : Enable report json format - nq_fold : bool : Speed-up by folding normalization parameters for input data (true: model input dtype is uint8, false: model input dtype is float32) profiler : dict - iter : int : This decides how many time the model inference will be processed. - mode : str : 1. Low power mode : Mode for low power consumption and low performance. 2. Balanced mode : Mode for medium power consumption and medium performance. 3. Performance mode : Mode for high power consumption and high performance. 4. Boost mode : Mode for very high power consumption and very high performance. [lowpower, balanced, performance, boost] - target : str : profiling target. [model, system] - test_type : str : ENN running mode [lib, service] - tv_threshold : float : The value is used for tolerance threshold of output match verification. - bitmatch_test : bool : if set true, visual profiler will compile nnc first - core_num : str : The number of NPU core. [single, multiple] - device : str : Target device. [Gen-3, Gen-4, Gen-5, Gen-5a, Gen-6, Gen-6npu, Gen-7] - device_id : str : id of the device connected to the server or PC running the enntools docker - remote_ssh_config_path : str : remote testing ssh config path (default path is {workspace}/DATA/remote_ssh_config.yaml) - ssh_bool : str : Connect to the device through ssh. [SSH_FALSE, SSH_TRUE] eht yaml file The optimization file contains optimization information. It allows you to set the locations of input and output models and detailed configurations for each module. Optimization is performed differently depending on the model type. Detailed explanation for eht yaml file model_type : string Specify the type of the input model. Optimization is performed differently depending on the model type, with details as follows CV LVM LLM quantizer : dict precision_weight : str : precision of weight(ex. int8, int16, fp16) precision_activation : str : precision of activation(ex. int8, int16, fp16) mpq_operator_dict : dict : When performing mixed precision quantization, input the operators and precision to be quantized with a precision different from the values specified in precision_weight and precision_activation above. alpha : float : smoothquant migration strength calibration_data_path : string : The path to the representative data to be used for calibration calibration_args : dict : Arguments for processing calibration samples : int : How many calibration data samples to use seed : int : A value set as a seed for random selection add_dummy_conv: bool : Whether apply the dummy conv algorithm input_dtype: dict : Input data type for the quantized model output_dtype: dict : Output data type for the quantized model simulator : dict metric : string: The metric to be used for measurement threshold : float : The threshold value of the metric that determines agreement / disagreement input_data_path : string: The path to the dataset for model inference optimizer : dict skip_4_dim_conversion : bool : If true, it does not convert to 4 dimensions before optimization; if false, it proceeds with the conversion. overwrite_input_shapes : dict : Enter the input shape for models with undefined input shapes. custom_template_path : dict: Enter the templates to apply for optimization. Use the path of the Python file containing the template as the key, and the template within it as the value. enntools conversion When performing conversion, users can choose between using eht and elt depending on the mode. The detailed workflow is illustrated in the figure below. bash enntools conversion Introduction to Exynos AI Studio Exynos AI Studio is a software that provides optimization and conversion techniques for neural networks models. This software takes models as input and offers functionalities such as quantization, conversion, optimization, compilation to generate NNC models. System overview diagram Exynos AI Studio Documentation Exynos AI Studio References Exynos AI Studio Developer Guide This guide"
    ],
    "reference": "In the context of using enntools for model optimization, both Gen-5 and Gen-5a are supported as System on Chip types. However, the specific capabilities and optimizations available may vary between these generations, as indicated in the configuration values for the elt yaml file, which specifies the device type options including Gen-5 and Gen-5a.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:09:12.255676+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps to enable GPU support when converting NN models to NNC models using Exynos AI Studio?",
    "reference_contexts": [
      "<1-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :",
      "<2-hop>\n\ndescribes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "To enable GPU support when converting NN models to NNC models using Exynos AI Studio, you need to ensure that the configuration file includes the option 'gpu_enable: true' to enable inference on the GPU if it is available. Additionally, you should specify the 'gpu_id' in the configuration to indicate which GPU to use for quantization profiling. After setting these parameters, you can proceed with the conversion process as outlined in the Exynos AI Studio documentation.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:09:12.255676+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is the purpose of EnnModelId in the ENN Framework?",
    "reference_contexts": [
      "Introduction EHT Converter Optimizer Quantizer Simulator Model Optimization Flow ELT Model Analyzer SCVT Quantizer NPU Simulator Performance Estimator Graph Generator Dataset and Model Quick Start System Requirements How to use ENN Framework Data Type References _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; API reference ENN Framework Data Type References _NumberOfBuffersInfo _ennBuffer _ennBufferInfo ENN Framwork API Functions Context initialize / deinitialize OpenModel / CloseModel related Memory Handling Setters and Getters for model Commit Buffer Execute Models Security, preference, get meta information.. title: _ennBuffer _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; Updated on 2023-08-11 at 16:24:05 +0900 title: _ennBufferInfo _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; Updated on 2023-08-11 at 16:24:05 +0900 title: _NumberOfBuffersInfo _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; Updated on 2023-08-11 at 16:24:05 +0900 title: Commit Buffer Commit Buffer Functions Name EnnReturn EnnBufferCommit (const EnnModelId model_id, const int session_id =0) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Functions Documentation function EnnBufferCommit EnnReturn EnnBufferCommit( const EnnModelId model_id, const int session_id =0 ) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Parameters: model_id [IN] model ID from load_model Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Context initialize / deinitialize Context initialize / deinitialize Functions Name EnnReturn EnnInitialize (void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. EnnReturn EnnDeinitialize (void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Functions Documentation function EnnInitialize EnnReturn EnnInitialize( void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. Return: EnnReturn result, 0 is success function EnnDeinitialize EnnReturn EnnDeinitialize( void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Security, preference, get meta information.. Security, preference, get meta information.. Functions Name EnnReturn EnnSetPreferencePresetId (const uint32_t val) Setting Preset ID for operation performance. EnnReturn EnnSetPreferencePerfConfigId (const uint32_t val) Setting PerfConfig ID for operation performance. EnnReturn EnnSetPreferencePerfMode (const uint32_t val) Setting Performance Mode. EnnReturn EnnSetPreferenceTimeOut (const uint32_t val) Setting Preset ID for time out. EnnReturn EnnSetPreferencePriority (const uint32_t val) Setting priority value for NPU. EnnReturn EnnSetPreferenceCoreAffinity (const uint32_t val) Setting affinity to set NPU core operation. EnnReturn EnnGetPreferencePresetId (uint32_t * val_ptr) Get current information for Preset ID. EnnReturn EnnGetPreferencePerfConfigId (uint32_t * val_ptr) Get current information for PerfConfig ID. EnnReturn EnnGetPreferencePerfMode (uint32_t * val_ptr) Get current information for Performance Mode. EnnReturn EnnGetPreferenceTimeOut (uint32_t * val_ptr) Get current information for Time Out. EnnReturn EnnGetPreferencePriority (uint32_t * val_ptr) Get current information for NPU Priority. EnnReturn EnnGetPreferenceCoreAffinity (uint32_t * val_ptr) Get current information for NPU Core affinity. EnnReturn EnnGetMetaInfo (const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]) Get Meta Information. EnnReturn EnnSetExecMsgAlwaysOn () Set frequency of execution message print. Functions Documentation function EnnSetPreferencePresetId EnnReturn EnnSetPreferencePresetId( const uint32_t val ) Setting Preset ID for operation performance. Parameters: val [IN] value to set preset ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfConfigId EnnReturn EnnSetPreferencePerfConfigId( const uint32_t val ) Setting PerfConfig ID for operation performance. Parameters: val [IN] value to set PerfConfig ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfMode EnnReturn EnnSetPreferencePerfMode( const uint32_t val ) Setting Performance Mode. Parameters: val [IN] value to set Performance Mode Return: EnnReturn result, 0 is success function EnnSetPreferenceTimeOut EnnReturn EnnSetPreferenceTimeOut( const uint32_t val ) Setting Preset ID for time out. Parameters: val [IN] value to set time out Return: EnnReturn result, 0 is success Note: in second function EnnSetPreferencePriority EnnReturn EnnSetPreferencePriority( const uint32_t val ) Setting priority value for NPU. Parameters: val [IN] value to set NPU job priority Return: EnnReturn result, 0 is success function EnnSetPreferenceCoreAffinity EnnReturn EnnSetPreferenceCoreAffinity( const uint32_t val ) Setting affinity to set NPU core operation. Parameters: val [IN] value to set affinity Return: EnnReturn result, 0 is success Note: in second function EnnGetPreferencePresetId EnnReturn EnnGetPreferencePresetId( uint32_t * val_ptr ) Get current information for Preset ID. Parameters: val [OUT] current value of Preset ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfConfigId EnnReturn EnnGetPreferencePerfConfigId( uint32_t * val_ptr ) Get current information for PerfConfig ID. Parameters: val [OUT] current value of PerfConfig ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfMode EnnReturn EnnGetPreferencePerfMode( uint32_t * val_ptr ) Get current information for Performance Mode. Parameters: val [OUT] current value of Performance Mode Return: EnnReturn result, 0 is success function EnnGetPreferenceTimeOut EnnReturn EnnGetPreferenceTimeOut( uint32_t * val_ptr ) Get current information for Time Out. Parameters: val [OUT] current value of Time Out Return: EnnReturn result, 0 is success function EnnGetPreferencePriority EnnReturn EnnGetPreferencePriority( uint32_t * val_ptr ) Get current information for NPU Priority. Parameters: val [OUT] current value of NPU Priority Return:"
    ],
    "reference": "EnnModelId is used to identify a model in the ENN Framework, particularly when sending buffer-sets to the service core through functions like EnnBufferCommit.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is EnnMetaTypeId?",
    "reference_contexts": [
      "Introduction EHT Converter Optimizer Quantizer Simulator Model Optimization Flow ELT Model Analyzer SCVT Quantizer NPU Simulator Performance Estimator Graph Generator Dataset and Model Quick Start System Requirements How to use ENN Framework Data Type References _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; API reference ENN Framework Data Type References _NumberOfBuffersInfo _ennBuffer _ennBufferInfo ENN Framwork API Functions Context initialize / deinitialize OpenModel / CloseModel related Memory Handling Setters and Getters for model Commit Buffer Execute Models Security, preference, get meta information.. title: _ennBuffer _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; Updated on 2023-08-11 at 16:24:05 +0900 title: _ennBufferInfo _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; Updated on 2023-08-11 at 16:24:05 +0900 title: _NumberOfBuffersInfo _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; Updated on 2023-08-11 at 16:24:05 +0900 title: Commit Buffer Commit Buffer Functions Name EnnReturn EnnBufferCommit (const EnnModelId model_id, const int session_id =0) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Functions Documentation function EnnBufferCommit EnnReturn EnnBufferCommit( const EnnModelId model_id, const int session_id =0 ) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Parameters: model_id [IN] model ID from load_model Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Context initialize / deinitialize Context initialize / deinitialize Functions Name EnnReturn EnnInitialize (void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. EnnReturn EnnDeinitialize (void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Functions Documentation function EnnInitialize EnnReturn EnnInitialize( void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. Return: EnnReturn result, 0 is success function EnnDeinitialize EnnReturn EnnDeinitialize( void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Security, preference, get meta information.. Security, preference, get meta information.. Functions Name EnnReturn EnnSetPreferencePresetId (const uint32_t val) Setting Preset ID for operation performance. EnnReturn EnnSetPreferencePerfConfigId (const uint32_t val) Setting PerfConfig ID for operation performance. EnnReturn EnnSetPreferencePerfMode (const uint32_t val) Setting Performance Mode. EnnReturn EnnSetPreferenceTimeOut (const uint32_t val) Setting Preset ID for time out. EnnReturn EnnSetPreferencePriority (const uint32_t val) Setting priority value for NPU. EnnReturn EnnSetPreferenceCoreAffinity (const uint32_t val) Setting affinity to set NPU core operation. EnnReturn EnnGetPreferencePresetId (uint32_t * val_ptr) Get current information for Preset ID. EnnReturn EnnGetPreferencePerfConfigId (uint32_t * val_ptr) Get current information for PerfConfig ID. EnnReturn EnnGetPreferencePerfMode (uint32_t * val_ptr) Get current information for Performance Mode. EnnReturn EnnGetPreferenceTimeOut (uint32_t * val_ptr) Get current information for Time Out. EnnReturn EnnGetPreferencePriority (uint32_t * val_ptr) Get current information for NPU Priority. EnnReturn EnnGetPreferenceCoreAffinity (uint32_t * val_ptr) Get current information for NPU Core affinity. EnnReturn EnnGetMetaInfo (const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]) Get Meta Information. EnnReturn EnnSetExecMsgAlwaysOn () Set frequency of execution message print. Functions Documentation function EnnSetPreferencePresetId EnnReturn EnnSetPreferencePresetId( const uint32_t val ) Setting Preset ID for operation performance. Parameters: val [IN] value to set preset ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfConfigId EnnReturn EnnSetPreferencePerfConfigId( const uint32_t val ) Setting PerfConfig ID for operation performance. Parameters: val [IN] value to set PerfConfig ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfMode EnnReturn EnnSetPreferencePerfMode( const uint32_t val ) Setting Performance Mode. Parameters: val [IN] value to set Performance Mode Return: EnnReturn result, 0 is success function EnnSetPreferenceTimeOut EnnReturn EnnSetPreferenceTimeOut( const uint32_t val ) Setting Preset ID for time out. Parameters: val [IN] value to set time out Return: EnnReturn result, 0 is success Note: in second function EnnSetPreferencePriority EnnReturn EnnSetPreferencePriority( const uint32_t val ) Setting priority value for NPU. Parameters: val [IN] value to set NPU job priority Return: EnnReturn result, 0 is success function EnnSetPreferenceCoreAffinity EnnReturn EnnSetPreferenceCoreAffinity( const uint32_t val ) Setting affinity to set NPU core operation. Parameters: val [IN] value to set affinity Return: EnnReturn result, 0 is success Note: in second function EnnGetPreferencePresetId EnnReturn EnnGetPreferencePresetId( uint32_t * val_ptr ) Get current information for Preset ID. Parameters: val [OUT] current value of Preset ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfConfigId EnnReturn EnnGetPreferencePerfConfigId( uint32_t * val_ptr ) Get current information for PerfConfig ID. Parameters: val [OUT] current value of PerfConfig ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfMode EnnReturn EnnGetPreferencePerfMode( uint32_t * val_ptr ) Get current information for Performance Mode. Parameters: val [OUT] current value of Performance Mode Return: EnnReturn result, 0 is success function EnnGetPreferenceTimeOut EnnReturn EnnGetPreferenceTimeOut( uint32_t * val_ptr ) Get current information for Time Out. Parameters: val [OUT] current value of Time Out Return: EnnReturn result, 0 is success function EnnGetPreferencePriority EnnReturn EnnGetPreferencePriority( uint32_t * val_ptr ) Get current information for NPU Priority. Parameters: val [OUT] current value of NPU Priority Return:"
    ],
    "reference": "EnnMetaTypeId is used in the function EnnGetMetaInfo, which retrieves meta information based on the provided info_id and model_id.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What EnnReturn do when commit buffer?",
    "reference_contexts": [
      "Introduction EHT Converter Optimizer Quantizer Simulator Model Optimization Flow ELT Model Analyzer SCVT Quantizer NPU Simulator Performance Estimator Graph Generator Dataset and Model Quick Start System Requirements How to use ENN Framework Data Type References _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; API reference ENN Framework Data Type References _NumberOfBuffersInfo _ennBuffer _ennBufferInfo ENN Framwork API Functions Context initialize / deinitialize OpenModel / CloseModel related Memory Handling Setters and Getters for model Commit Buffer Execute Models Security, preference, get meta information.. title: _ennBuffer _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; Updated on 2023-08-11 at 16:24:05 +0900 title: _ennBufferInfo _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; Updated on 2023-08-11 at 16:24:05 +0900 title: _NumberOfBuffersInfo _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; Updated on 2023-08-11 at 16:24:05 +0900 title: Commit Buffer Commit Buffer Functions Name EnnReturn EnnBufferCommit (const EnnModelId model_id, const int session_id =0) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Functions Documentation function EnnBufferCommit EnnReturn EnnBufferCommit( const EnnModelId model_id, const int session_id =0 ) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Parameters: model_id [IN] model ID from load_model Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Context initialize / deinitialize Context initialize / deinitialize Functions Name EnnReturn EnnInitialize (void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. EnnReturn EnnDeinitialize (void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Functions Documentation function EnnInitialize EnnReturn EnnInitialize( void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. Return: EnnReturn result, 0 is success function EnnDeinitialize EnnReturn EnnDeinitialize( void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Security, preference, get meta information.. Security, preference, get meta information.. Functions Name EnnReturn EnnSetPreferencePresetId (const uint32_t val) Setting Preset ID for operation performance. EnnReturn EnnSetPreferencePerfConfigId (const uint32_t val) Setting PerfConfig ID for operation performance. EnnReturn EnnSetPreferencePerfMode (const uint32_t val) Setting Performance Mode. EnnReturn EnnSetPreferenceTimeOut (const uint32_t val) Setting Preset ID for time out. EnnReturn EnnSetPreferencePriority (const uint32_t val) Setting priority value for NPU. EnnReturn EnnSetPreferenceCoreAffinity (const uint32_t val) Setting affinity to set NPU core operation. EnnReturn EnnGetPreferencePresetId (uint32_t * val_ptr) Get current information for Preset ID. EnnReturn EnnGetPreferencePerfConfigId (uint32_t * val_ptr) Get current information for PerfConfig ID. EnnReturn EnnGetPreferencePerfMode (uint32_t * val_ptr) Get current information for Performance Mode. EnnReturn EnnGetPreferenceTimeOut (uint32_t * val_ptr) Get current information for Time Out. EnnReturn EnnGetPreferencePriority (uint32_t * val_ptr) Get current information for NPU Priority. EnnReturn EnnGetPreferenceCoreAffinity (uint32_t * val_ptr) Get current information for NPU Core affinity. EnnReturn EnnGetMetaInfo (const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]) Get Meta Information. EnnReturn EnnSetExecMsgAlwaysOn () Set frequency of execution message print. Functions Documentation function EnnSetPreferencePresetId EnnReturn EnnSetPreferencePresetId( const uint32_t val ) Setting Preset ID for operation performance. Parameters: val [IN] value to set preset ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfConfigId EnnReturn EnnSetPreferencePerfConfigId( const uint32_t val ) Setting PerfConfig ID for operation performance. Parameters: val [IN] value to set PerfConfig ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfMode EnnReturn EnnSetPreferencePerfMode( const uint32_t val ) Setting Performance Mode. Parameters: val [IN] value to set Performance Mode Return: EnnReturn result, 0 is success function EnnSetPreferenceTimeOut EnnReturn EnnSetPreferenceTimeOut( const uint32_t val ) Setting Preset ID for time out. Parameters: val [IN] value to set time out Return: EnnReturn result, 0 is success Note: in second function EnnSetPreferencePriority EnnReturn EnnSetPreferencePriority( const uint32_t val ) Setting priority value for NPU. Parameters: val [IN] value to set NPU job priority Return: EnnReturn result, 0 is success function EnnSetPreferenceCoreAffinity EnnReturn EnnSetPreferenceCoreAffinity( const uint32_t val ) Setting affinity to set NPU core operation. Parameters: val [IN] value to set affinity Return: EnnReturn result, 0 is success Note: in second function EnnGetPreferencePresetId EnnReturn EnnGetPreferencePresetId( uint32_t * val_ptr ) Get current information for Preset ID. Parameters: val [OUT] current value of Preset ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfConfigId EnnReturn EnnGetPreferencePerfConfigId( uint32_t * val_ptr ) Get current information for PerfConfig ID. Parameters: val [OUT] current value of PerfConfig ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfMode EnnReturn EnnGetPreferencePerfMode( uint32_t * val_ptr ) Get current information for Performance Mode. Parameters: val [OUT] current value of Performance Mode Return: EnnReturn result, 0 is success function EnnGetPreferenceTimeOut EnnReturn EnnGetPreferenceTimeOut( uint32_t * val_ptr ) Get current information for Time Out. Parameters: val [OUT] current value of Time Out Return: EnnReturn result, 0 is success function EnnGetPreferencePriority EnnReturn EnnGetPreferencePriority( uint32_t * val_ptr ) Get current information for NPU Priority. Parameters: val [OUT] current value of NPU Priority Return:"
    ],
    "reference": "EnnReturn EnnBufferCommit (const EnnModelId model_id, const int session_id =0) send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel(). Return is EnnReturn result, 0 is success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What role does the NPU play in the context of AI application development?",
    "reference_contexts": [
      "Introduction EHT Converter Optimizer Quantizer Simulator Model Optimization Flow ELT Model Analyzer SCVT Quantizer NPU Simulator Performance Estimator Graph Generator Dataset and Model Quick Start System Requirements How to use ENN Framework Data Type References _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; API reference ENN Framework Data Type References _NumberOfBuffersInfo _ennBuffer _ennBufferInfo ENN Framwork API Functions Context initialize / deinitialize OpenModel / CloseModel related Memory Handling Setters and Getters for model Commit Buffer Execute Models Security, preference, get meta information.. title: _ennBuffer _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; Updated on 2023-08-11 at 16:24:05 +0900 title: _ennBufferInfo _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; Updated on 2023-08-11 at 16:24:05 +0900 title: _NumberOfBuffersInfo _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; Updated on 2023-08-11 at 16:24:05 +0900 title: Commit Buffer Commit Buffer Functions Name EnnReturn EnnBufferCommit (const EnnModelId model_id, const int session_id =0) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Functions Documentation function EnnBufferCommit EnnReturn EnnBufferCommit( const EnnModelId model_id, const int session_id =0 ) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Parameters: model_id [IN] model ID from load_model Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Context initialize / deinitialize Context initialize / deinitialize Functions Name EnnReturn EnnInitialize (void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. EnnReturn EnnDeinitialize (void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Functions Documentation function EnnInitialize EnnReturn EnnInitialize( void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. Return: EnnReturn result, 0 is success function EnnDeinitialize EnnReturn EnnDeinitialize( void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Security, preference, get meta information.. Security, preference, get meta information.. Functions Name EnnReturn EnnSetPreferencePresetId (const uint32_t val) Setting Preset ID for operation performance. EnnReturn EnnSetPreferencePerfConfigId (const uint32_t val) Setting PerfConfig ID for operation performance. EnnReturn EnnSetPreferencePerfMode (const uint32_t val) Setting Performance Mode. EnnReturn EnnSetPreferenceTimeOut (const uint32_t val) Setting Preset ID for time out. EnnReturn EnnSetPreferencePriority (const uint32_t val) Setting priority value for NPU. EnnReturn EnnSetPreferenceCoreAffinity (const uint32_t val) Setting affinity to set NPU core operation. EnnReturn EnnGetPreferencePresetId (uint32_t * val_ptr) Get current information for Preset ID. EnnReturn EnnGetPreferencePerfConfigId (uint32_t * val_ptr) Get current information for PerfConfig ID. EnnReturn EnnGetPreferencePerfMode (uint32_t * val_ptr) Get current information for Performance Mode. EnnReturn EnnGetPreferenceTimeOut (uint32_t * val_ptr) Get current information for Time Out. EnnReturn EnnGetPreferencePriority (uint32_t * val_ptr) Get current information for NPU Priority. EnnReturn EnnGetPreferenceCoreAffinity (uint32_t * val_ptr) Get current information for NPU Core affinity. EnnReturn EnnGetMetaInfo (const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]) Get Meta Information. EnnReturn EnnSetExecMsgAlwaysOn () Set frequency of execution message print. Functions Documentation function EnnSetPreferencePresetId EnnReturn EnnSetPreferencePresetId( const uint32_t val ) Setting Preset ID for operation performance. Parameters: val [IN] value to set preset ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfConfigId EnnReturn EnnSetPreferencePerfConfigId( const uint32_t val ) Setting PerfConfig ID for operation performance. Parameters: val [IN] value to set PerfConfig ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfMode EnnReturn EnnSetPreferencePerfMode( const uint32_t val ) Setting Performance Mode. Parameters: val [IN] value to set Performance Mode Return: EnnReturn result, 0 is success function EnnSetPreferenceTimeOut EnnReturn EnnSetPreferenceTimeOut( const uint32_t val ) Setting Preset ID for time out. Parameters: val [IN] value to set time out Return: EnnReturn result, 0 is success Note: in second function EnnSetPreferencePriority EnnReturn EnnSetPreferencePriority( const uint32_t val ) Setting priority value for NPU. Parameters: val [IN] value to set NPU job priority Return: EnnReturn result, 0 is success function EnnSetPreferenceCoreAffinity EnnReturn EnnSetPreferenceCoreAffinity( const uint32_t val ) Setting affinity to set NPU core operation. Parameters: val [IN] value to set affinity Return: EnnReturn result, 0 is success Note: in second function EnnGetPreferencePresetId EnnReturn EnnGetPreferencePresetId( uint32_t * val_ptr ) Get current information for Preset ID. Parameters: val [OUT] current value of Preset ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfConfigId EnnReturn EnnGetPreferencePerfConfigId( uint32_t * val_ptr ) Get current information for PerfConfig ID. Parameters: val [OUT] current value of PerfConfig ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfMode EnnReturn EnnGetPreferencePerfMode( uint32_t * val_ptr ) Get current information for Performance Mode. Parameters: val [OUT] current value of Performance Mode Return: EnnReturn result, 0 is success function EnnGetPreferenceTimeOut EnnReturn EnnGetPreferenceTimeOut( uint32_t * val_ptr ) Get current information for Time Out. Parameters: val [OUT] current value of Time Out Return: EnnReturn result, 0 is success function EnnGetPreferencePriority EnnReturn EnnGetPreferencePriority( uint32_t * val_ptr ) Get current information for NPU Priority. Parameters: val [OUT] current value of NPU Priority Return:"
    ],
    "reference": "The NPU is involved in setting the priority value for NPU jobs and configuring core affinity for NPU operations, which are essential for optimizing performance in AI applications.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is EnnMetaTypeId used for in the ENN Framework?",
    "reference_contexts": [
      "Introduction EHT Converter Optimizer Quantizer Simulator Model Optimization Flow ELT Model Analyzer SCVT Quantizer NPU Simulator Performance Estimator Graph Generator Dataset and Model Quick Start System Requirements How to use ENN Framework Data Type References _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; API reference ENN Framework Data Type References _NumberOfBuffersInfo _ennBuffer _ennBufferInfo ENN Framwork API Functions Context initialize / deinitialize OpenModel / CloseModel related Memory Handling Setters and Getters for model Commit Buffer Execute Models Security, preference, get meta information.. title: _ennBuffer _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; Updated on 2023-08-11 at 16:24:05 +0900 title: _ennBufferInfo _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; Updated on 2023-08-11 at 16:24:05 +0900 title: _NumberOfBuffersInfo _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; Updated on 2023-08-11 at 16:24:05 +0900 title: Commit Buffer Commit Buffer Functions Name EnnReturn EnnBufferCommit (const EnnModelId model_id, const int session_id =0) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Functions Documentation function EnnBufferCommit EnnReturn EnnBufferCommit( const EnnModelId model_id, const int session_id =0 ) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Parameters: model_id [IN] model ID from load_model Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Context initialize / deinitialize Context initialize / deinitialize Functions Name EnnReturn EnnInitialize (void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. EnnReturn EnnDeinitialize (void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Functions Documentation function EnnInitialize EnnReturn EnnInitialize( void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. Return: EnnReturn result, 0 is success function EnnDeinitialize EnnReturn EnnDeinitialize( void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Security, preference, get meta information.. Security, preference, get meta information.. Functions Name EnnReturn EnnSetPreferencePresetId (const uint32_t val) Setting Preset ID for operation performance. EnnReturn EnnSetPreferencePerfConfigId (const uint32_t val) Setting PerfConfig ID for operation performance. EnnReturn EnnSetPreferencePerfMode (const uint32_t val) Setting Performance Mode. EnnReturn EnnSetPreferenceTimeOut (const uint32_t val) Setting Preset ID for time out. EnnReturn EnnSetPreferencePriority (const uint32_t val) Setting priority value for NPU. EnnReturn EnnSetPreferenceCoreAffinity (const uint32_t val) Setting affinity to set NPU core operation. EnnReturn EnnGetPreferencePresetId (uint32_t * val_ptr) Get current information for Preset ID. EnnReturn EnnGetPreferencePerfConfigId (uint32_t * val_ptr) Get current information for PerfConfig ID. EnnReturn EnnGetPreferencePerfMode (uint32_t * val_ptr) Get current information for Performance Mode. EnnReturn EnnGetPreferenceTimeOut (uint32_t * val_ptr) Get current information for Time Out. EnnReturn EnnGetPreferencePriority (uint32_t * val_ptr) Get current information for NPU Priority. EnnReturn EnnGetPreferenceCoreAffinity (uint32_t * val_ptr) Get current information for NPU Core affinity. EnnReturn EnnGetMetaInfo (const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]) Get Meta Information. EnnReturn EnnSetExecMsgAlwaysOn () Set frequency of execution message print. Functions Documentation function EnnSetPreferencePresetId EnnReturn EnnSetPreferencePresetId( const uint32_t val ) Setting Preset ID for operation performance. Parameters: val [IN] value to set preset ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfConfigId EnnReturn EnnSetPreferencePerfConfigId( const uint32_t val ) Setting PerfConfig ID for operation performance. Parameters: val [IN] value to set PerfConfig ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfMode EnnReturn EnnSetPreferencePerfMode( const uint32_t val ) Setting Performance Mode. Parameters: val [IN] value to set Performance Mode Return: EnnReturn result, 0 is success function EnnSetPreferenceTimeOut EnnReturn EnnSetPreferenceTimeOut( const uint32_t val ) Setting Preset ID for time out. Parameters: val [IN] value to set time out Return: EnnReturn result, 0 is success Note: in second function EnnSetPreferencePriority EnnReturn EnnSetPreferencePriority( const uint32_t val ) Setting priority value for NPU. Parameters: val [IN] value to set NPU job priority Return: EnnReturn result, 0 is success function EnnSetPreferenceCoreAffinity EnnReturn EnnSetPreferenceCoreAffinity( const uint32_t val ) Setting affinity to set NPU core operation. Parameters: val [IN] value to set affinity Return: EnnReturn result, 0 is success Note: in second function EnnGetPreferencePresetId EnnReturn EnnGetPreferencePresetId( uint32_t * val_ptr ) Get current information for Preset ID. Parameters: val [OUT] current value of Preset ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfConfigId EnnReturn EnnGetPreferencePerfConfigId( uint32_t * val_ptr ) Get current information for PerfConfig ID. Parameters: val [OUT] current value of PerfConfig ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfMode EnnReturn EnnGetPreferencePerfMode( uint32_t * val_ptr ) Get current information for Performance Mode. Parameters: val [OUT] current value of Performance Mode Return: EnnReturn result, 0 is success function EnnGetPreferenceTimeOut EnnReturn EnnGetPreferenceTimeOut( uint32_t * val_ptr ) Get current information for Time Out. Parameters: val [OUT] current value of Time Out Return: EnnReturn result, 0 is success function EnnGetPreferencePriority EnnReturn EnnGetPreferencePriority( uint32_t * val_ptr ) Get current information for NPU Priority. Parameters: val [OUT] current value of NPU Priority Return:"
    ],
    "reference": "EnnMetaTypeId is used in the ENN Framework to get meta information related to a specific model. It is a parameter in the function EnnGetMetaInfo, which retrieves meta information based on the provided info_id and model_id.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is the role of the NPU in the ENN Framework?",
    "reference_contexts": [
      "Introduction EHT Converter Optimizer Quantizer Simulator Model Optimization Flow ELT Model Analyzer SCVT Quantizer NPU Simulator Performance Estimator Graph Generator Dataset and Model Quick Start System Requirements How to use ENN Framework Data Type References _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; API reference ENN Framework Data Type References _NumberOfBuffersInfo _ennBuffer _ennBufferInfo ENN Framwork API Functions Context initialize / deinitialize OpenModel / CloseModel related Memory Handling Setters and Getters for model Commit Buffer Execute Models Security, preference, get meta information.. title: _ennBuffer _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; Updated on 2023-08-11 at 16:24:05 +0900 title: _ennBufferInfo _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; Updated on 2023-08-11 at 16:24:05 +0900 title: _NumberOfBuffersInfo _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; Updated on 2023-08-11 at 16:24:05 +0900 title: Commit Buffer Commit Buffer Functions Name EnnReturn EnnBufferCommit (const EnnModelId model_id, const int session_id =0) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Functions Documentation function EnnBufferCommit EnnReturn EnnBufferCommit( const EnnModelId model_id, const int session_id =0 ) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Parameters: model_id [IN] model ID from load_model Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Context initialize / deinitialize Context initialize / deinitialize Functions Name EnnReturn EnnInitialize (void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. EnnReturn EnnDeinitialize (void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Functions Documentation function EnnInitialize EnnReturn EnnInitialize( void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. Return: EnnReturn result, 0 is success function EnnDeinitialize EnnReturn EnnDeinitialize( void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Security, preference, get meta information.. Security, preference, get meta information.. Functions Name EnnReturn EnnSetPreferencePresetId (const uint32_t val) Setting Preset ID for operation performance. EnnReturn EnnSetPreferencePerfConfigId (const uint32_t val) Setting PerfConfig ID for operation performance. EnnReturn EnnSetPreferencePerfMode (const uint32_t val) Setting Performance Mode. EnnReturn EnnSetPreferenceTimeOut (const uint32_t val) Setting Preset ID for time out. EnnReturn EnnSetPreferencePriority (const uint32_t val) Setting priority value for NPU. EnnReturn EnnSetPreferenceCoreAffinity (const uint32_t val) Setting affinity to set NPU core operation. EnnReturn EnnGetPreferencePresetId (uint32_t * val_ptr) Get current information for Preset ID. EnnReturn EnnGetPreferencePerfConfigId (uint32_t * val_ptr) Get current information for PerfConfig ID. EnnReturn EnnGetPreferencePerfMode (uint32_t * val_ptr) Get current information for Performance Mode. EnnReturn EnnGetPreferenceTimeOut (uint32_t * val_ptr) Get current information for Time Out. EnnReturn EnnGetPreferencePriority (uint32_t * val_ptr) Get current information for NPU Priority. EnnReturn EnnGetPreferenceCoreAffinity (uint32_t * val_ptr) Get current information for NPU Core affinity. EnnReturn EnnGetMetaInfo (const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]) Get Meta Information. EnnReturn EnnSetExecMsgAlwaysOn () Set frequency of execution message print. Functions Documentation function EnnSetPreferencePresetId EnnReturn EnnSetPreferencePresetId( const uint32_t val ) Setting Preset ID for operation performance. Parameters: val [IN] value to set preset ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfConfigId EnnReturn EnnSetPreferencePerfConfigId( const uint32_t val ) Setting PerfConfig ID for operation performance. Parameters: val [IN] value to set PerfConfig ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfMode EnnReturn EnnSetPreferencePerfMode( const uint32_t val ) Setting Performance Mode. Parameters: val [IN] value to set Performance Mode Return: EnnReturn result, 0 is success function EnnSetPreferenceTimeOut EnnReturn EnnSetPreferenceTimeOut( const uint32_t val ) Setting Preset ID for time out. Parameters: val [IN] value to set time out Return: EnnReturn result, 0 is success Note: in second function EnnSetPreferencePriority EnnReturn EnnSetPreferencePriority( const uint32_t val ) Setting priority value for NPU. Parameters: val [IN] value to set NPU job priority Return: EnnReturn result, 0 is success function EnnSetPreferenceCoreAffinity EnnReturn EnnSetPreferenceCoreAffinity( const uint32_t val ) Setting affinity to set NPU core operation. Parameters: val [IN] value to set affinity Return: EnnReturn result, 0 is success Note: in second function EnnGetPreferencePresetId EnnReturn EnnGetPreferencePresetId( uint32_t * val_ptr ) Get current information for Preset ID. Parameters: val [OUT] current value of Preset ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfConfigId EnnReturn EnnGetPreferencePerfConfigId( uint32_t * val_ptr ) Get current information for PerfConfig ID. Parameters: val [OUT] current value of PerfConfig ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfMode EnnReturn EnnGetPreferencePerfMode( uint32_t * val_ptr ) Get current information for Performance Mode. Parameters: val [OUT] current value of Performance Mode Return: EnnReturn result, 0 is success function EnnGetPreferenceTimeOut EnnReturn EnnGetPreferenceTimeOut( uint32_t * val_ptr ) Get current information for Time Out. Parameters: val [OUT] current value of Time Out Return: EnnReturn result, 0 is success function EnnGetPreferencePriority EnnReturn EnnGetPreferencePriority( uint32_t * val_ptr ) Get current information for NPU Priority. Parameters: val [OUT] current value of NPU Priority Return:"
    ],
    "reference": "The NPU is involved in setting the priority value for operation performance and can have its core affinity set to optimize its operation within the ENN Framework.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is the purpose of the EnnMetaTypeId in the ENN Framework?",
    "reference_contexts": [
      "Introduction EHT Converter Optimizer Quantizer Simulator Model Optimization Flow ELT Model Analyzer SCVT Quantizer NPU Simulator Performance Estimator Graph Generator Dataset and Model Quick Start System Requirements How to use ENN Framework Data Type References _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; API reference ENN Framework Data Type References _NumberOfBuffersInfo _ennBuffer _ennBufferInfo ENN Framwork API Functions Context initialize / deinitialize OpenModel / CloseModel related Memory Handling Setters and Getters for model Commit Buffer Execute Models Security, preference, get meta information.. title: _ennBuffer _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; Updated on 2023-08-11 at 16:24:05 +0900 title: _ennBufferInfo _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; Updated on 2023-08-11 at 16:24:05 +0900 title: _NumberOfBuffersInfo _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; Updated on 2023-08-11 at 16:24:05 +0900 title: Commit Buffer Commit Buffer Functions Name EnnReturn EnnBufferCommit (const EnnModelId model_id, const int session_id =0) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Functions Documentation function EnnBufferCommit EnnReturn EnnBufferCommit( const EnnModelId model_id, const int session_id =0 ) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Parameters: model_id [IN] model ID from load_model Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Context initialize / deinitialize Context initialize / deinitialize Functions Name EnnReturn EnnInitialize (void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. EnnReturn EnnDeinitialize (void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Functions Documentation function EnnInitialize EnnReturn EnnInitialize( void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. Return: EnnReturn result, 0 is success function EnnDeinitialize EnnReturn EnnDeinitialize( void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Security, preference, get meta information.. Security, preference, get meta information.. Functions Name EnnReturn EnnSetPreferencePresetId (const uint32_t val) Setting Preset ID for operation performance. EnnReturn EnnSetPreferencePerfConfigId (const uint32_t val) Setting PerfConfig ID for operation performance. EnnReturn EnnSetPreferencePerfMode (const uint32_t val) Setting Performance Mode. EnnReturn EnnSetPreferenceTimeOut (const uint32_t val) Setting Preset ID for time out. EnnReturn EnnSetPreferencePriority (const uint32_t val) Setting priority value for NPU. EnnReturn EnnSetPreferenceCoreAffinity (const uint32_t val) Setting affinity to set NPU core operation. EnnReturn EnnGetPreferencePresetId (uint32_t * val_ptr) Get current information for Preset ID. EnnReturn EnnGetPreferencePerfConfigId (uint32_t * val_ptr) Get current information for PerfConfig ID. EnnReturn EnnGetPreferencePerfMode (uint32_t * val_ptr) Get current information for Performance Mode. EnnReturn EnnGetPreferenceTimeOut (uint32_t * val_ptr) Get current information for Time Out. EnnReturn EnnGetPreferencePriority (uint32_t * val_ptr) Get current information for NPU Priority. EnnReturn EnnGetPreferenceCoreAffinity (uint32_t * val_ptr) Get current information for NPU Core affinity. EnnReturn EnnGetMetaInfo (const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]) Get Meta Information. EnnReturn EnnSetExecMsgAlwaysOn () Set frequency of execution message print. Functions Documentation function EnnSetPreferencePresetId EnnReturn EnnSetPreferencePresetId( const uint32_t val ) Setting Preset ID for operation performance. Parameters: val [IN] value to set preset ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfConfigId EnnReturn EnnSetPreferencePerfConfigId( const uint32_t val ) Setting PerfConfig ID for operation performance. Parameters: val [IN] value to set PerfConfig ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfMode EnnReturn EnnSetPreferencePerfMode( const uint32_t val ) Setting Performance Mode. Parameters: val [IN] value to set Performance Mode Return: EnnReturn result, 0 is success function EnnSetPreferenceTimeOut EnnReturn EnnSetPreferenceTimeOut( const uint32_t val ) Setting Preset ID for time out. Parameters: val [IN] value to set time out Return: EnnReturn result, 0 is success Note: in second function EnnSetPreferencePriority EnnReturn EnnSetPreferencePriority( const uint32_t val ) Setting priority value for NPU. Parameters: val [IN] value to set NPU job priority Return: EnnReturn result, 0 is success function EnnSetPreferenceCoreAffinity EnnReturn EnnSetPreferenceCoreAffinity( const uint32_t val ) Setting affinity to set NPU core operation. Parameters: val [IN] value to set affinity Return: EnnReturn result, 0 is success Note: in second function EnnGetPreferencePresetId EnnReturn EnnGetPreferencePresetId( uint32_t * val_ptr ) Get current information for Preset ID. Parameters: val [OUT] current value of Preset ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfConfigId EnnReturn EnnGetPreferencePerfConfigId( uint32_t * val_ptr ) Get current information for PerfConfig ID. Parameters: val [OUT] current value of PerfConfig ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfMode EnnReturn EnnGetPreferencePerfMode( uint32_t * val_ptr ) Get current information for Performance Mode. Parameters: val [OUT] current value of Performance Mode Return: EnnReturn result, 0 is success function EnnGetPreferenceTimeOut EnnReturn EnnGetPreferenceTimeOut( uint32_t * val_ptr ) Get current information for Time Out. Parameters: val [OUT] current value of Time Out Return: EnnReturn result, 0 is success function EnnGetPreferencePriority EnnReturn EnnGetPreferencePriority( uint32_t * val_ptr ) Get current information for NPU Priority. Parameters: val [OUT] current value of NPU Priority Return:"
    ],
    "reference": "The EnnMetaTypeId is used in the ENN Framework to get meta information related to a specific model. It is a parameter in the function EnnGetMetaInfo, which retrieves the meta information for a given model ID.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is the purpose of EnnMetaTypeId in the ENN Framework?",
    "reference_contexts": [
      "Introduction EHT Converter Optimizer Quantizer Simulator Model Optimization Flow ELT Model Analyzer SCVT Quantizer NPU Simulator Performance Estimator Graph Generator Dataset and Model Quick Start System Requirements How to use ENN Framework Data Type References _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; API reference ENN Framework Data Type References _NumberOfBuffersInfo _ennBuffer _ennBufferInfo ENN Framwork API Functions Context initialize / deinitialize OpenModel / CloseModel related Memory Handling Setters and Getters for model Commit Buffer Execute Models Security, preference, get meta information.. title: _ennBuffer _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; Updated on 2023-08-11 at 16:24:05 +0900 title: _ennBufferInfo _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; Updated on 2023-08-11 at 16:24:05 +0900 title: _NumberOfBuffersInfo _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; Updated on 2023-08-11 at 16:24:05 +0900 title: Commit Buffer Commit Buffer Functions Name EnnReturn EnnBufferCommit (const EnnModelId model_id, const int session_id =0) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Functions Documentation function EnnBufferCommit EnnReturn EnnBufferCommit( const EnnModelId model_id, const int session_id =0 ) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Parameters: model_id [IN] model ID from load_model Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Context initialize / deinitialize Context initialize / deinitialize Functions Name EnnReturn EnnInitialize (void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. EnnReturn EnnDeinitialize (void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Functions Documentation function EnnInitialize EnnReturn EnnInitialize( void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. Return: EnnReturn result, 0 is success function EnnDeinitialize EnnReturn EnnDeinitialize( void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Security, preference, get meta information.. Security, preference, get meta information.. Functions Name EnnReturn EnnSetPreferencePresetId (const uint32_t val) Setting Preset ID for operation performance. EnnReturn EnnSetPreferencePerfConfigId (const uint32_t val) Setting PerfConfig ID for operation performance. EnnReturn EnnSetPreferencePerfMode (const uint32_t val) Setting Performance Mode. EnnReturn EnnSetPreferenceTimeOut (const uint32_t val) Setting Preset ID for time out. EnnReturn EnnSetPreferencePriority (const uint32_t val) Setting priority value for NPU. EnnReturn EnnSetPreferenceCoreAffinity (const uint32_t val) Setting affinity to set NPU core operation. EnnReturn EnnGetPreferencePresetId (uint32_t * val_ptr) Get current information for Preset ID. EnnReturn EnnGetPreferencePerfConfigId (uint32_t * val_ptr) Get current information for PerfConfig ID. EnnReturn EnnGetPreferencePerfMode (uint32_t * val_ptr) Get current information for Performance Mode. EnnReturn EnnGetPreferenceTimeOut (uint32_t * val_ptr) Get current information for Time Out. EnnReturn EnnGetPreferencePriority (uint32_t * val_ptr) Get current information for NPU Priority. EnnReturn EnnGetPreferenceCoreAffinity (uint32_t * val_ptr) Get current information for NPU Core affinity. EnnReturn EnnGetMetaInfo (const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]) Get Meta Information. EnnReturn EnnSetExecMsgAlwaysOn () Set frequency of execution message print. Functions Documentation function EnnSetPreferencePresetId EnnReturn EnnSetPreferencePresetId( const uint32_t val ) Setting Preset ID for operation performance. Parameters: val [IN] value to set preset ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfConfigId EnnReturn EnnSetPreferencePerfConfigId( const uint32_t val ) Setting PerfConfig ID for operation performance. Parameters: val [IN] value to set PerfConfig ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfMode EnnReturn EnnSetPreferencePerfMode( const uint32_t val ) Setting Performance Mode. Parameters: val [IN] value to set Performance Mode Return: EnnReturn result, 0 is success function EnnSetPreferenceTimeOut EnnReturn EnnSetPreferenceTimeOut( const uint32_t val ) Setting Preset ID for time out. Parameters: val [IN] value to set time out Return: EnnReturn result, 0 is success Note: in second function EnnSetPreferencePriority EnnReturn EnnSetPreferencePriority( const uint32_t val ) Setting priority value for NPU. Parameters: val [IN] value to set NPU job priority Return: EnnReturn result, 0 is success function EnnSetPreferenceCoreAffinity EnnReturn EnnSetPreferenceCoreAffinity( const uint32_t val ) Setting affinity to set NPU core operation. Parameters: val [IN] value to set affinity Return: EnnReturn result, 0 is success Note: in second function EnnGetPreferencePresetId EnnReturn EnnGetPreferencePresetId( uint32_t * val_ptr ) Get current information for Preset ID. Parameters: val [OUT] current value of Preset ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfConfigId EnnReturn EnnGetPreferencePerfConfigId( uint32_t * val_ptr ) Get current information for PerfConfig ID. Parameters: val [OUT] current value of PerfConfig ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfMode EnnReturn EnnGetPreferencePerfMode( uint32_t * val_ptr ) Get current information for Performance Mode. Parameters: val [OUT] current value of Performance Mode Return: EnnReturn result, 0 is success function EnnGetPreferenceTimeOut EnnReturn EnnGetPreferenceTimeOut( uint32_t * val_ptr ) Get current information for Time Out. Parameters: val [OUT] current value of Time Out Return: EnnReturn result, 0 is success function EnnGetPreferencePriority EnnReturn EnnGetPreferencePriority( uint32_t * val_ptr ) Get current information for NPU Priority. Parameters: val [OUT] current value of NPU Priority Return:"
    ],
    "reference": "EnnMetaTypeId is used in the ENN Framework to get meta information related to a specific model by calling the function EnnGetMetaInfo, which retrieves the meta information based on the provided info_id and model_id.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How does the ENN framework facilitate model execution in Exynos AI Studio?",
    "reference_contexts": [
      "Index EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "The ENN framework facilitates model execution in Exynos AI Studio by allowing users to initialize the framework, load the converted model, allocate and commit necessary buffers, copy input data to input buffers, execute the model, and use data from output buffers. This process can be repeated for multiple executions, and it concludes with uncommitting and releasing buffers, unloading the model, and de-initializing the framework.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "Can you explain what NNC is and how it relates to the Exynos AI Studio?",
    "reference_contexts": [
      "Index EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "NNC is an NN model format that can run efficiently in Samsung Exynos hardware. In the context of Exynos AI Studio, users convert trained TensorFlow Lite neural network models to NNC models to enable efficient execution on the hardware.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "Can you explain how Samsung Exynos AI Studio facilitates the conversion and execution of neural network models?",
    "reference_contexts": [
      "Index EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "Samsung Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. The workflow involves two main steps: first, the user converts NN models to NNC models, which is an NN model format optimized for Exynos hardware. Second, the user executes the converted model for inference using the ENN framework, which requires initializing the framework, loading the model, allocating necessary buffers, copying input data, executing the model, and then using the output data.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is the significance of Samsung in the context of Exynos AI Studio?",
    "reference_contexts": [
      "Index EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "Samsung is significant in the context of Exynos AI Studio as it provides the hardware on which the Exynos AI Studio operates. The studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is Exynos AI Studio used for in AI application development?",
    "reference_contexts": [
      "Index EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "Exynos AI Studio is used to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models and executing the converted model for inference.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is the purpose of NNC in the context of Exynos AI Studio?",
    "reference_contexts": [
      "Index EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "NNC is an NN model format that can run efficiently in Samsung Exynos hardware. It is used to convert trained TensorFlow Lite neural network models for efficient execution on Exynos AI Studio.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "Can you explain how Samsung Exynos AI Studio helps in the development of AI applications?",
    "reference_contexts": [
      "Index EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "Samsung Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models, which can be executed for inference. The workflow involves converting NN models to NNC models and then executing the converted model using the ENN framework.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How does Exynos AI Studio support TensorFlow Lite models?",
    "reference_contexts": [
      "Index EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. It supports TensorFlow Lite models, including tensors with up to four dimensions and models with a maximum size of 1 GB.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How do I use Exynos AI Stuido Service?",
    "reference_contexts": [
      "Troubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For"
    ],
    "reference": "The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "Can you explain the significance of Softmax in the context of advanced quantization methods?",
    "reference_contexts": [
      "Troubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For"
    ],
    "reference": "Softmax Bias Correction is an advanced quantization method that adds a float bias to the Softmax output layer to reduce the performance degradation caused by quantization.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How can I use PyTorch with Exynos AI Studio?",
    "reference_contexts": [
      "Troubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For"
    ],
    "reference": "You can use PyTorch with Exynos AI Studio by utilizing the PyTorch2ONNX Converter, which converts a PyTorch model to ONNX format. This requires input dimension information, dynamic axes, and an ONNX opset for exporting.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How can I convert a TFLITE model to an ONNX model using the Exynos AI Studio toolkit?",
    "reference_contexts": [
      "Troubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For"
    ],
    "reference": "To convert a TFLITE model to an ONNX model using the Exynos AI Studio toolkit, you can utilize the TF2ONNX Converter. This converter requires an ONNX opset for exporting. You can use the following example code: ```python from converter import api tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` This code snippet demonstrates how to set the input and output paths for the model conversion.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "Can you explain how to convert an ONNX model to CNNX using the Exynos AI Studio toolkit?",
    "reference_contexts": [
      "Troubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For"
    ],
    "reference": "To convert an ONNX model to CNNX using the Exynos AI Studio toolkit, you need to use the ONNX2CNNX Converter. This converter requires you to specify both the output model path and the output encodings path.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How can TensorFlow models be utilized within the Exynos AI Studio framework?",
    "reference_contexts": [
      "Troubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For"
    ],
    "reference": "TensorFlow models can be utilized within the Exynos AI Studio framework by using the TF2ONNX Converter, which converts a TF (or TFLITE) model to ONNX. This conversion requires an ONNX opset for exporting, allowing users to apply the EHT module's optimization features to their TensorFlow models.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How can I utilize PyTorch models within the Exynos AI Studio toolkit?",
    "reference_contexts": [
      "Troubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For"
    ],
    "reference": "You can utilize PyTorch models within the Exynos AI Studio toolkit by using the PyTorch2ONNX Converter, which converts a PyTorch model to ONNX format. This requires input dimension information, dynamic axes for maintaining dynamics after export, and an ONNX opset for exporting. Once converted to ONNX, you can further optimize the model using the Exynos AI High-Level Toolchain (EHT) and convert it to the desired SNC model.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the key features of the Exynos AI Studio service?",
    "reference_contexts": [
      "Troubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For"
    ],
    "reference": "The Exynos AI Studio service provides detailed information on using the service, allows users to create a maximum of five projects, and is currently free. It also encourages users to share questions, feedback, or bugs on forums or submit concerns to 1:1 Support for personalized assistance.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How does the Accelerate option enhance the performance of AI models on Samsung Exynos hardware?",
    "reference_contexts": [
      "this guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "The Accelerate option engages the NPU as an additional accelerator, which can enhance the performance of AI models. However, it is important to note that the NPU does not support all models, and choosing the Accelerate option may lead to complications.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "Can you explain how the NPU is utilized in the context of AI model deployment and what precautions should be taken when selecting it?",
    "reference_contexts": [
      "this guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "The NPU is engaged as an additional accelerator when the Accelerate hardware type is chosen. However, it is important to note that the NPU does not support all models, and selecting the Accelerate option may lead to complications.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What does NPU do in the context of hardware acceleration?",
    "reference_contexts": [
      "this guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "The NPU engages as an additional accelerator when the Accelerate hardware type is chosen. However, it is important to note that the NPU does not support all models, and selecting the Accelerate option may lead to complications.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What does the Accelerate option do in the context of hardware selection?",
    "reference_contexts": [
      "this guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "The Accelerate option engages the NPU as an additional accelerator, while the default setting utilizes only the CPU and GPU. However, it is important to note that the NPU does not support all models, and choosing the Accelerate option may lead to complications.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How does the GPU function in the context of hardware acceleration for AI applications?",
    "reference_contexts": [
      "this guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "In the context of hardware acceleration for AI applications, the GPU is utilized alongside the CPU to enhance performance. The default setting utilizes both the CPU and GPU, while the Accelerate option engages the NPU as an additional accelerator. However, it is important to note that the NPU does not support all models, and choosing the Accelerate option may lead to complications.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is Gen-5 in the context of AI model deployment?",
    "reference_contexts": [
      "this guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "Gen-5 refers to a System on Chip (SoC) type that can be specified in the elt yaml file for analyzing models and is part of the device options available for various processes.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is Application.mk used for in the NDK build process?",
    "reference_contexts": [
      "this guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "Application.mk specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used in the NDK build process.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the supported device types for the elt model analyzer in Gen-6?",
    "reference_contexts": [
      "this guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "The supported device types for the elt model analyzer in Gen-6 include System on Chip types such as Gen-5, Gen-5a, Gen-5b, and Gen-6.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the device types available in the context of Gen-3b?",
    "reference_contexts": [
      "str : debug str for compiler - dequant_type : str : dequantiztion type - device : str : System on Chip type [Gen-3, Gen-3b, Gen-4, Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - enable_ofm_reuse : bool : Enable the reuse of OFM region for IMFM. - enable_stm : bool : Generate compile log including L1 tiling information - flc : bool : Enable featuremap lossless compression - fp16_swwa : bool : Enable NPU fp16 workaround with psum_init - input_conversion : str : Add a Tensor2Cell format converter node at start of network - mi : bool : multiple input compile - mo : bool : multiple output compile - multi_ncp : bool : generate multi-ncp(ucgo) custom op - multi_vc : bool : Introduce Multi-VC for OFM, IFM, and weight transfer - multicore : bool : Enable NPU multicore - optimization : str : Optimization choice [O1, O2, O3] - output_conversion : str : Add a Tensor2Cell format converter node at end of network - packed_ucgo : bool : true: generate one ucgo in nnc, false: generate multiple ucgos in nnc if there are multiple independent dsp OPs - preemption : bool : Setting priority of NNC while compiling - quant_type : str : quantiztion type - sync_npu_dsp : bool : simulator : dict - data_format : str : Indicate the position of channel of input [channel_first, channel_last] - use_randomdb : bool : Use randomdb to forward, just support single input - userdb : str : Simulation data set path (default path is {workspace}/DATA/data.txt) perf_estimator : dict - O2_enable : bool : O2 optimization (true or false) - O2_fm_forwarding : bool : feature-map forwarding (true or false) - SEG : bool : Set true if input model is Deeplab V3+ - SSD : bool : Set true if input model is SSD detection - bit_width_factor_FM : int : Select feature map bit width factor (1 or 2) - bit_width_factor_FP16 : bool : Set bit width factor as floating point (true or false) - bit_width_factor_weight : int : Select weight bit width factor (1 or 2) - core_num : int : 1 for single core, 2 for instance-1 - device : str : Select device type [Gen-3, Gen-4, Gen-5, Gen-5a] - json_report : bool : Enable report json format - nq_fold : bool : Speed-up by folding normalization parameters for input data (true: model input dtype is uint8, false: model input dtype is float32) profiler : dict - iter : int : This decides how many time the model inference will be processed. - mode : str : 1. Low power mode : Mode for low power consumption and low performance. 2. Balanced mode : Mode for medium power consumption and medium performance. 3. Performance mode : Mode for high power consumption and high performance. 4. Boost mode : Mode for very high power consumption and very high performance. [lowpower, balanced, performance, boost] - target : str : profiling target. [model, system] - test_type : str : ENN running mode [lib, service] - tv_threshold : float : The value is used for tolerance threshold of output match verification. - bitmatch_test : bool : if set true, visual profiler will compile nnc first - core_num : str : The number of NPU core. [single, multiple] - device : str : Target device. [Gen-3, Gen-4, Gen-5, Gen-5a, Gen-6, Gen-6npu, Gen-7] - device_id : str : id of the device connected to the server or PC running the enntools docker - remote_ssh_config_path : str : remote testing ssh config path (default path is {workspace}/DATA/remote_ssh_config.yaml) - ssh_bool : str : Connect to the device through ssh. [SSH_FALSE, SSH_TRUE] eht yaml file The optimization file contains optimization information. It allows you to set the locations of input and output models and detailed configurations for each module. Optimization is performed differently depending on the model type. Detailed explanation for eht yaml file model_type : string Specify the type of the input model. Optimization is performed differently depending on the model type, with details as follows CV LVM LLM quantizer : dict precision_weight : str : precision of weight(ex. int8, int16, fp16) precision_activation : str : precision of activation(ex. int8, int16, fp16) mpq_operator_dict : dict : When performing mixed precision quantization, input the operators and precision to be quantized with a precision different from the values specified in precision_weight and precision_activation above. alpha : float : smoothquant migration strength calibration_data_path : string : The path to the representative data to be used for calibration calibration_args : dict : Arguments for processing calibration samples : int : How many calibration data samples to use seed : int : A value set as a seed for random selection add_dummy_conv: bool : Whether apply the dummy conv algorithm input_dtype: dict : Input data type for the quantized model output_dtype: dict : Output data type for the quantized model simulator : dict metric : string: The metric to be used for measurement threshold : float : The threshold value of the metric that determines agreement / disagreement input_data_path : string: The path to the dataset for model inference optimizer : dict skip_4_dim_conversion : bool : If true, it does not convert to 4 dimensions before optimization; if false, it proceeds with the conversion. overwrite_input_shapes : dict : Enter the input shape for models with undefined input shapes. custom_template_path : dict: Enter the templates to apply for optimization. Use the path of the Python file containing the template as the key, and the template within it as the value. enntools conversion When performing conversion, users can choose between using eht and elt depending on the mode. The detailed workflow is illustrated in the figure below. bash enntools conversion Introduction to Exynos AI Studio Exynos AI Studio is a software that provides optimization and conversion techniques for neural networks models. This software takes models as input and offers functionalities such as quantization, conversion, optimization, compilation to generate NNC models. System overview diagram Exynos AI Studio Documentation Exynos AI Studio References Exynos AI Studio Developer Guide This guide"
    ],
    "reference": "The device types available include Gen-3, Gen-3b, Gen-4, Gen-5, Gen-5a, Gen-5b, Gen-6, and Gen-7.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the device types available in the context of Gen-4?",
    "reference_contexts": [
      "str : debug str for compiler - dequant_type : str : dequantiztion type - device : str : System on Chip type [Gen-3, Gen-3b, Gen-4, Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - enable_ofm_reuse : bool : Enable the reuse of OFM region for IMFM. - enable_stm : bool : Generate compile log including L1 tiling information - flc : bool : Enable featuremap lossless compression - fp16_swwa : bool : Enable NPU fp16 workaround with psum_init - input_conversion : str : Add a Tensor2Cell format converter node at start of network - mi : bool : multiple input compile - mo : bool : multiple output compile - multi_ncp : bool : generate multi-ncp(ucgo) custom op - multi_vc : bool : Introduce Multi-VC for OFM, IFM, and weight transfer - multicore : bool : Enable NPU multicore - optimization : str : Optimization choice [O1, O2, O3] - output_conversion : str : Add a Tensor2Cell format converter node at end of network - packed_ucgo : bool : true: generate one ucgo in nnc, false: generate multiple ucgos in nnc if there are multiple independent dsp OPs - preemption : bool : Setting priority of NNC while compiling - quant_type : str : quantiztion type - sync_npu_dsp : bool : simulator : dict - data_format : str : Indicate the position of channel of input [channel_first, channel_last] - use_randomdb : bool : Use randomdb to forward, just support single input - userdb : str : Simulation data set path (default path is {workspace}/DATA/data.txt) perf_estimator : dict - O2_enable : bool : O2 optimization (true or false) - O2_fm_forwarding : bool : feature-map forwarding (true or false) - SEG : bool : Set true if input model is Deeplab V3+ - SSD : bool : Set true if input model is SSD detection - bit_width_factor_FM : int : Select feature map bit width factor (1 or 2) - bit_width_factor_FP16 : bool : Set bit width factor as floating point (true or false) - bit_width_factor_weight : int : Select weight bit width factor (1 or 2) - core_num : int : 1 for single core, 2 for instance-1 - device : str : Select device type [Gen-3, Gen-4, Gen-5, Gen-5a] - json_report : bool : Enable report json format - nq_fold : bool : Speed-up by folding normalization parameters for input data (true: model input dtype is uint8, false: model input dtype is float32) profiler : dict - iter : int : This decides how many time the model inference will be processed. - mode : str : 1. Low power mode : Mode for low power consumption and low performance. 2. Balanced mode : Mode for medium power consumption and medium performance. 3. Performance mode : Mode for high power consumption and high performance. 4. Boost mode : Mode for very high power consumption and very high performance. [lowpower, balanced, performance, boost] - target : str : profiling target. [model, system] - test_type : str : ENN running mode [lib, service] - tv_threshold : float : The value is used for tolerance threshold of output match verification. - bitmatch_test : bool : if set true, visual profiler will compile nnc first - core_num : str : The number of NPU core. [single, multiple] - device : str : Target device. [Gen-3, Gen-4, Gen-5, Gen-5a, Gen-6, Gen-6npu, Gen-7] - device_id : str : id of the device connected to the server or PC running the enntools docker - remote_ssh_config_path : str : remote testing ssh config path (default path is {workspace}/DATA/remote_ssh_config.yaml) - ssh_bool : str : Connect to the device through ssh. [SSH_FALSE, SSH_TRUE] eht yaml file The optimization file contains optimization information. It allows you to set the locations of input and output models and detailed configurations for each module. Optimization is performed differently depending on the model type. Detailed explanation for eht yaml file model_type : string Specify the type of the input model. Optimization is performed differently depending on the model type, with details as follows CV LVM LLM quantizer : dict precision_weight : str : precision of weight(ex. int8, int16, fp16) precision_activation : str : precision of activation(ex. int8, int16, fp16) mpq_operator_dict : dict : When performing mixed precision quantization, input the operators and precision to be quantized with a precision different from the values specified in precision_weight and precision_activation above. alpha : float : smoothquant migration strength calibration_data_path : string : The path to the representative data to be used for calibration calibration_args : dict : Arguments for processing calibration samples : int : How many calibration data samples to use seed : int : A value set as a seed for random selection add_dummy_conv: bool : Whether apply the dummy conv algorithm input_dtype: dict : Input data type for the quantized model output_dtype: dict : Output data type for the quantized model simulator : dict metric : string: The metric to be used for measurement threshold : float : The threshold value of the metric that determines agreement / disagreement input_data_path : string: The path to the dataset for model inference optimizer : dict skip_4_dim_conversion : bool : If true, it does not convert to 4 dimensions before optimization; if false, it proceeds with the conversion. overwrite_input_shapes : dict : Enter the input shape for models with undefined input shapes. custom_template_path : dict: Enter the templates to apply for optimization. Use the path of the Python file containing the template as the key, and the template within it as the value. enntools conversion When performing conversion, users can choose between using eht and elt depending on the mode. The detailed workflow is illustrated in the figure below. bash enntools conversion Introduction to Exynos AI Studio Exynos AI Studio is a software that provides optimization and conversion techniques for neural networks models. This software takes models as input and offers functionalities such as quantization, conversion, optimization, compilation to generate NNC models. System overview diagram Exynos AI Studio Documentation Exynos AI Studio References Exynos AI Studio Developer Guide This guide"
    ],
    "reference": "The device types available include Gen-3, Gen-4, Gen-5, and Gen-5a.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is Gen-5b in the context of device types?",
    "reference_contexts": [
      "str : debug str for compiler - dequant_type : str : dequantiztion type - device : str : System on Chip type [Gen-3, Gen-3b, Gen-4, Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - enable_ofm_reuse : bool : Enable the reuse of OFM region for IMFM. - enable_stm : bool : Generate compile log including L1 tiling information - flc : bool : Enable featuremap lossless compression - fp16_swwa : bool : Enable NPU fp16 workaround with psum_init - input_conversion : str : Add a Tensor2Cell format converter node at start of network - mi : bool : multiple input compile - mo : bool : multiple output compile - multi_ncp : bool : generate multi-ncp(ucgo) custom op - multi_vc : bool : Introduce Multi-VC for OFM, IFM, and weight transfer - multicore : bool : Enable NPU multicore - optimization : str : Optimization choice [O1, O2, O3] - output_conversion : str : Add a Tensor2Cell format converter node at end of network - packed_ucgo : bool : true: generate one ucgo in nnc, false: generate multiple ucgos in nnc if there are multiple independent dsp OPs - preemption : bool : Setting priority of NNC while compiling - quant_type : str : quantiztion type - sync_npu_dsp : bool : simulator : dict - data_format : str : Indicate the position of channel of input [channel_first, channel_last] - use_randomdb : bool : Use randomdb to forward, just support single input - userdb : str : Simulation data set path (default path is {workspace}/DATA/data.txt) perf_estimator : dict - O2_enable : bool : O2 optimization (true or false) - O2_fm_forwarding : bool : feature-map forwarding (true or false) - SEG : bool : Set true if input model is Deeplab V3+ - SSD : bool : Set true if input model is SSD detection - bit_width_factor_FM : int : Select feature map bit width factor (1 or 2) - bit_width_factor_FP16 : bool : Set bit width factor as floating point (true or false) - bit_width_factor_weight : int : Select weight bit width factor (1 or 2) - core_num : int : 1 for single core, 2 for instance-1 - device : str : Select device type [Gen-3, Gen-4, Gen-5, Gen-5a] - json_report : bool : Enable report json format - nq_fold : bool : Speed-up by folding normalization parameters for input data (true: model input dtype is uint8, false: model input dtype is float32) profiler : dict - iter : int : This decides how many time the model inference will be processed. - mode : str : 1. Low power mode : Mode for low power consumption and low performance. 2. Balanced mode : Mode for medium power consumption and medium performance. 3. Performance mode : Mode for high power consumption and high performance. 4. Boost mode : Mode for very high power consumption and very high performance. [lowpower, balanced, performance, boost] - target : str : profiling target. [model, system] - test_type : str : ENN running mode [lib, service] - tv_threshold : float : The value is used for tolerance threshold of output match verification. - bitmatch_test : bool : if set true, visual profiler will compile nnc first - core_num : str : The number of NPU core. [single, multiple] - device : str : Target device. [Gen-3, Gen-4, Gen-5, Gen-5a, Gen-6, Gen-6npu, Gen-7] - device_id : str : id of the device connected to the server or PC running the enntools docker - remote_ssh_config_path : str : remote testing ssh config path (default path is {workspace}/DATA/remote_ssh_config.yaml) - ssh_bool : str : Connect to the device through ssh. [SSH_FALSE, SSH_TRUE] eht yaml file The optimization file contains optimization information. It allows you to set the locations of input and output models and detailed configurations for each module. Optimization is performed differently depending on the model type. Detailed explanation for eht yaml file model_type : string Specify the type of the input model. Optimization is performed differently depending on the model type, with details as follows CV LVM LLM quantizer : dict precision_weight : str : precision of weight(ex. int8, int16, fp16) precision_activation : str : precision of activation(ex. int8, int16, fp16) mpq_operator_dict : dict : When performing mixed precision quantization, input the operators and precision to be quantized with a precision different from the values specified in precision_weight and precision_activation above. alpha : float : smoothquant migration strength calibration_data_path : string : The path to the representative data to be used for calibration calibration_args : dict : Arguments for processing calibration samples : int : How many calibration data samples to use seed : int : A value set as a seed for random selection add_dummy_conv: bool : Whether apply the dummy conv algorithm input_dtype: dict : Input data type for the quantized model output_dtype: dict : Output data type for the quantized model simulator : dict metric : string: The metric to be used for measurement threshold : float : The threshold value of the metric that determines agreement / disagreement input_data_path : string: The path to the dataset for model inference optimizer : dict skip_4_dim_conversion : bool : If true, it does not convert to 4 dimensions before optimization; if false, it proceeds with the conversion. overwrite_input_shapes : dict : Enter the input shape for models with undefined input shapes. custom_template_path : dict: Enter the templates to apply for optimization. Use the path of the Python file containing the template as the key, and the template within it as the value. enntools conversion When performing conversion, users can choose between using eht and elt depending on the mode. The detailed workflow is illustrated in the figure below. bash enntools conversion Introduction to Exynos AI Studio Exynos AI Studio is a software that provides optimization and conversion techniques for neural networks models. This software takes models as input and offers functionalities such as quantization, conversion, optimization, compilation to generate NNC models. System overview diagram Exynos AI Studio Documentation Exynos AI Studio References Exynos AI Studio Developer Guide This guide"
    ],
    "reference": "Gen-5b is one of the System on Chip types listed in the context, which includes various generations such as Gen-3, Gen-3b, Gen-4, Gen-5, Gen-5a, Gen-5b, Gen-6, and Gen-7.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "Can you explain the significance of the Gen-5a device in the context of Exynos AI Studio and its capabilities?",
    "reference_contexts": [
      "str : debug str for compiler - dequant_type : str : dequantiztion type - device : str : System on Chip type [Gen-3, Gen-3b, Gen-4, Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - enable_ofm_reuse : bool : Enable the reuse of OFM region for IMFM. - enable_stm : bool : Generate compile log including L1 tiling information - flc : bool : Enable featuremap lossless compression - fp16_swwa : bool : Enable NPU fp16 workaround with psum_init - input_conversion : str : Add a Tensor2Cell format converter node at start of network - mi : bool : multiple input compile - mo : bool : multiple output compile - multi_ncp : bool : generate multi-ncp(ucgo) custom op - multi_vc : bool : Introduce Multi-VC for OFM, IFM, and weight transfer - multicore : bool : Enable NPU multicore - optimization : str : Optimization choice [O1, O2, O3] - output_conversion : str : Add a Tensor2Cell format converter node at end of network - packed_ucgo : bool : true: generate one ucgo in nnc, false: generate multiple ucgos in nnc if there are multiple independent dsp OPs - preemption : bool : Setting priority of NNC while compiling - quant_type : str : quantiztion type - sync_npu_dsp : bool : simulator : dict - data_format : str : Indicate the position of channel of input [channel_first, channel_last] - use_randomdb : bool : Use randomdb to forward, just support single input - userdb : str : Simulation data set path (default path is {workspace}/DATA/data.txt) perf_estimator : dict - O2_enable : bool : O2 optimization (true or false) - O2_fm_forwarding : bool : feature-map forwarding (true or false) - SEG : bool : Set true if input model is Deeplab V3+ - SSD : bool : Set true if input model is SSD detection - bit_width_factor_FM : int : Select feature map bit width factor (1 or 2) - bit_width_factor_FP16 : bool : Set bit width factor as floating point (true or false) - bit_width_factor_weight : int : Select weight bit width factor (1 or 2) - core_num : int : 1 for single core, 2 for instance-1 - device : str : Select device type [Gen-3, Gen-4, Gen-5, Gen-5a] - json_report : bool : Enable report json format - nq_fold : bool : Speed-up by folding normalization parameters for input data (true: model input dtype is uint8, false: model input dtype is float32) profiler : dict - iter : int : This decides how many time the model inference will be processed. - mode : str : 1. Low power mode : Mode for low power consumption and low performance. 2. Balanced mode : Mode for medium power consumption and medium performance. 3. Performance mode : Mode for high power consumption and high performance. 4. Boost mode : Mode for very high power consumption and very high performance. [lowpower, balanced, performance, boost] - target : str : profiling target. [model, system] - test_type : str : ENN running mode [lib, service] - tv_threshold : float : The value is used for tolerance threshold of output match verification. - bitmatch_test : bool : if set true, visual profiler will compile nnc first - core_num : str : The number of NPU core. [single, multiple] - device : str : Target device. [Gen-3, Gen-4, Gen-5, Gen-5a, Gen-6, Gen-6npu, Gen-7] - device_id : str : id of the device connected to the server or PC running the enntools docker - remote_ssh_config_path : str : remote testing ssh config path (default path is {workspace}/DATA/remote_ssh_config.yaml) - ssh_bool : str : Connect to the device through ssh. [SSH_FALSE, SSH_TRUE] eht yaml file The optimization file contains optimization information. It allows you to set the locations of input and output models and detailed configurations for each module. Optimization is performed differently depending on the model type. Detailed explanation for eht yaml file model_type : string Specify the type of the input model. Optimization is performed differently depending on the model type, with details as follows CV LVM LLM quantizer : dict precision_weight : str : precision of weight(ex. int8, int16, fp16) precision_activation : str : precision of activation(ex. int8, int16, fp16) mpq_operator_dict : dict : When performing mixed precision quantization, input the operators and precision to be quantized with a precision different from the values specified in precision_weight and precision_activation above. alpha : float : smoothquant migration strength calibration_data_path : string : The path to the representative data to be used for calibration calibration_args : dict : Arguments for processing calibration samples : int : How many calibration data samples to use seed : int : A value set as a seed for random selection add_dummy_conv: bool : Whether apply the dummy conv algorithm input_dtype: dict : Input data type for the quantized model output_dtype: dict : Output data type for the quantized model simulator : dict metric : string: The metric to be used for measurement threshold : float : The threshold value of the metric that determines agreement / disagreement input_data_path : string: The path to the dataset for model inference optimizer : dict skip_4_dim_conversion : bool : If true, it does not convert to 4 dimensions before optimization; if false, it proceeds with the conversion. overwrite_input_shapes : dict : Enter the input shape for models with undefined input shapes. custom_template_path : dict: Enter the templates to apply for optimization. Use the path of the Python file containing the template as the key, and the template within it as the value. enntools conversion When performing conversion, users can choose between using eht and elt depending on the mode. The detailed workflow is illustrated in the figure below. bash enntools conversion Introduction to Exynos AI Studio Exynos AI Studio is a software that provides optimization and conversion techniques for neural networks models. This software takes models as input and offers functionalities such as quantization, conversion, optimization, compilation to generate NNC models. System overview diagram Exynos AI Studio Documentation Exynos AI Studio References Exynos AI Studio Developer Guide This guide"
    ],
    "reference": "The Gen-5a device is one of the System on Chip types supported by Exynos AI Studio, which allows for various configurations and optimizations in AI model deployment. It is included in a range of devices from Gen-3 to Gen-7, indicating its role in facilitating advanced AI functionalities and optimizations within the Exynos ecosystem.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How does the Exynos AI Studio handle models like Deeplab V3+ during the optimization process?",
    "reference_contexts": [
      "str : debug str for compiler - dequant_type : str : dequantiztion type - device : str : System on Chip type [Gen-3, Gen-3b, Gen-4, Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - enable_ofm_reuse : bool : Enable the reuse of OFM region for IMFM. - enable_stm : bool : Generate compile log including L1 tiling information - flc : bool : Enable featuremap lossless compression - fp16_swwa : bool : Enable NPU fp16 workaround with psum_init - input_conversion : str : Add a Tensor2Cell format converter node at start of network - mi : bool : multiple input compile - mo : bool : multiple output compile - multi_ncp : bool : generate multi-ncp(ucgo) custom op - multi_vc : bool : Introduce Multi-VC for OFM, IFM, and weight transfer - multicore : bool : Enable NPU multicore - optimization : str : Optimization choice [O1, O2, O3] - output_conversion : str : Add a Tensor2Cell format converter node at end of network - packed_ucgo : bool : true: generate one ucgo in nnc, false: generate multiple ucgos in nnc if there are multiple independent dsp OPs - preemption : bool : Setting priority of NNC while compiling - quant_type : str : quantiztion type - sync_npu_dsp : bool : simulator : dict - data_format : str : Indicate the position of channel of input [channel_first, channel_last] - use_randomdb : bool : Use randomdb to forward, just support single input - userdb : str : Simulation data set path (default path is {workspace}/DATA/data.txt) perf_estimator : dict - O2_enable : bool : O2 optimization (true or false) - O2_fm_forwarding : bool : feature-map forwarding (true or false) - SEG : bool : Set true if input model is Deeplab V3+ - SSD : bool : Set true if input model is SSD detection - bit_width_factor_FM : int : Select feature map bit width factor (1 or 2) - bit_width_factor_FP16 : bool : Set bit width factor as floating point (true or false) - bit_width_factor_weight : int : Select weight bit width factor (1 or 2) - core_num : int : 1 for single core, 2 for instance-1 - device : str : Select device type [Gen-3, Gen-4, Gen-5, Gen-5a] - json_report : bool : Enable report json format - nq_fold : bool : Speed-up by folding normalization parameters for input data (true: model input dtype is uint8, false: model input dtype is float32) profiler : dict - iter : int : This decides how many time the model inference will be processed. - mode : str : 1. Low power mode : Mode for low power consumption and low performance. 2. Balanced mode : Mode for medium power consumption and medium performance. 3. Performance mode : Mode for high power consumption and high performance. 4. Boost mode : Mode for very high power consumption and very high performance. [lowpower, balanced, performance, boost] - target : str : profiling target. [model, system] - test_type : str : ENN running mode [lib, service] - tv_threshold : float : The value is used for tolerance threshold of output match verification. - bitmatch_test : bool : if set true, visual profiler will compile nnc first - core_num : str : The number of NPU core. [single, multiple] - device : str : Target device. [Gen-3, Gen-4, Gen-5, Gen-5a, Gen-6, Gen-6npu, Gen-7] - device_id : str : id of the device connected to the server or PC running the enntools docker - remote_ssh_config_path : str : remote testing ssh config path (default path is {workspace}/DATA/remote_ssh_config.yaml) - ssh_bool : str : Connect to the device through ssh. [SSH_FALSE, SSH_TRUE] eht yaml file The optimization file contains optimization information. It allows you to set the locations of input and output models and detailed configurations for each module. Optimization is performed differently depending on the model type. Detailed explanation for eht yaml file model_type : string Specify the type of the input model. Optimization is performed differently depending on the model type, with details as follows CV LVM LLM quantizer : dict precision_weight : str : precision of weight(ex. int8, int16, fp16) precision_activation : str : precision of activation(ex. int8, int16, fp16) mpq_operator_dict : dict : When performing mixed precision quantization, input the operators and precision to be quantized with a precision different from the values specified in precision_weight and precision_activation above. alpha : float : smoothquant migration strength calibration_data_path : string : The path to the representative data to be used for calibration calibration_args : dict : Arguments for processing calibration samples : int : How many calibration data samples to use seed : int : A value set as a seed for random selection add_dummy_conv: bool : Whether apply the dummy conv algorithm input_dtype: dict : Input data type for the quantized model output_dtype: dict : Output data type for the quantized model simulator : dict metric : string: The metric to be used for measurement threshold : float : The threshold value of the metric that determines agreement / disagreement input_data_path : string: The path to the dataset for model inference optimizer : dict skip_4_dim_conversion : bool : If true, it does not convert to 4 dimensions before optimization; if false, it proceeds with the conversion. overwrite_input_shapes : dict : Enter the input shape for models with undefined input shapes. custom_template_path : dict: Enter the templates to apply for optimization. Use the path of the Python file containing the template as the key, and the template within it as the value. enntools conversion When performing conversion, users can choose between using eht and elt depending on the mode. The detailed workflow is illustrated in the figure below. bash enntools conversion Introduction to Exynos AI Studio Exynos AI Studio is a software that provides optimization and conversion techniques for neural networks models. This software takes models as input and offers functionalities such as quantization, conversion, optimization, compilation to generate NNC models. System overview diagram Exynos AI Studio Documentation Exynos AI Studio References Exynos AI Studio Developer Guide This guide"
    ],
    "reference": "In the Exynos AI Studio, when optimizing models like Deeplab V3+, the optimization file contains specific information that allows users to set the locations of input and output models along with detailed configurations for each module. The optimization process is tailored based on the model type, ensuring that the unique requirements of models such as Deeplab V3+ are met effectively.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What devices are included in the Gen-5 category?",
    "reference_contexts": [
      "str : debug str for compiler - dequant_type : str : dequantiztion type - device : str : System on Chip type [Gen-3, Gen-3b, Gen-4, Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - enable_ofm_reuse : bool : Enable the reuse of OFM region for IMFM. - enable_stm : bool : Generate compile log including L1 tiling information - flc : bool : Enable featuremap lossless compression - fp16_swwa : bool : Enable NPU fp16 workaround with psum_init - input_conversion : str : Add a Tensor2Cell format converter node at start of network - mi : bool : multiple input compile - mo : bool : multiple output compile - multi_ncp : bool : generate multi-ncp(ucgo) custom op - multi_vc : bool : Introduce Multi-VC for OFM, IFM, and weight transfer - multicore : bool : Enable NPU multicore - optimization : str : Optimization choice [O1, O2, O3] - output_conversion : str : Add a Tensor2Cell format converter node at end of network - packed_ucgo : bool : true: generate one ucgo in nnc, false: generate multiple ucgos in nnc if there are multiple independent dsp OPs - preemption : bool : Setting priority of NNC while compiling - quant_type : str : quantiztion type - sync_npu_dsp : bool : simulator : dict - data_format : str : Indicate the position of channel of input [channel_first, channel_last] - use_randomdb : bool : Use randomdb to forward, just support single input - userdb : str : Simulation data set path (default path is {workspace}/DATA/data.txt) perf_estimator : dict - O2_enable : bool : O2 optimization (true or false) - O2_fm_forwarding : bool : feature-map forwarding (true or false) - SEG : bool : Set true if input model is Deeplab V3+ - SSD : bool : Set true if input model is SSD detection - bit_width_factor_FM : int : Select feature map bit width factor (1 or 2) - bit_width_factor_FP16 : bool : Set bit width factor as floating point (true or false) - bit_width_factor_weight : int : Select weight bit width factor (1 or 2) - core_num : int : 1 for single core, 2 for instance-1 - device : str : Select device type [Gen-3, Gen-4, Gen-5, Gen-5a] - json_report : bool : Enable report json format - nq_fold : bool : Speed-up by folding normalization parameters for input data (true: model input dtype is uint8, false: model input dtype is float32) profiler : dict - iter : int : This decides how many time the model inference will be processed. - mode : str : 1. Low power mode : Mode for low power consumption and low performance. 2. Balanced mode : Mode for medium power consumption and medium performance. 3. Performance mode : Mode for high power consumption and high performance. 4. Boost mode : Mode for very high power consumption and very high performance. [lowpower, balanced, performance, boost] - target : str : profiling target. [model, system] - test_type : str : ENN running mode [lib, service] - tv_threshold : float : The value is used for tolerance threshold of output match verification. - bitmatch_test : bool : if set true, visual profiler will compile nnc first - core_num : str : The number of NPU core. [single, multiple] - device : str : Target device. [Gen-3, Gen-4, Gen-5, Gen-5a, Gen-6, Gen-6npu, Gen-7] - device_id : str : id of the device connected to the server or PC running the enntools docker - remote_ssh_config_path : str : remote testing ssh config path (default path is {workspace}/DATA/remote_ssh_config.yaml) - ssh_bool : str : Connect to the device through ssh. [SSH_FALSE, SSH_TRUE] eht yaml file The optimization file contains optimization information. It allows you to set the locations of input and output models and detailed configurations for each module. Optimization is performed differently depending on the model type. Detailed explanation for eht yaml file model_type : string Specify the type of the input model. Optimization is performed differently depending on the model type, with details as follows CV LVM LLM quantizer : dict precision_weight : str : precision of weight(ex. int8, int16, fp16) precision_activation : str : precision of activation(ex. int8, int16, fp16) mpq_operator_dict : dict : When performing mixed precision quantization, input the operators and precision to be quantized with a precision different from the values specified in precision_weight and precision_activation above. alpha : float : smoothquant migration strength calibration_data_path : string : The path to the representative data to be used for calibration calibration_args : dict : Arguments for processing calibration samples : int : How many calibration data samples to use seed : int : A value set as a seed for random selection add_dummy_conv: bool : Whether apply the dummy conv algorithm input_dtype: dict : Input data type for the quantized model output_dtype: dict : Output data type for the quantized model simulator : dict metric : string: The metric to be used for measurement threshold : float : The threshold value of the metric that determines agreement / disagreement input_data_path : string: The path to the dataset for model inference optimizer : dict skip_4_dim_conversion : bool : If true, it does not convert to 4 dimensions before optimization; if false, it proceeds with the conversion. overwrite_input_shapes : dict : Enter the input shape for models with undefined input shapes. custom_template_path : dict: Enter the templates to apply for optimization. Use the path of the Python file containing the template as the key, and the template within it as the value. enntools conversion When performing conversion, users can choose between using eht and elt depending on the mode. The detailed workflow is illustrated in the figure below. bash enntools conversion Introduction to Exynos AI Studio Exynos AI Studio is a software that provides optimization and conversion techniques for neural networks models. This software takes models as input and offers functionalities such as quantization, conversion, optimization, compilation to generate NNC models. System overview diagram Exynos AI Studio Documentation Exynos AI Studio References Exynos AI Studio Developer Guide This guide"
    ],
    "reference": "The devices included in the Gen-5 category are Gen-5, Gen-5a, and Gen-5b.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is the significance of Gen-5b in the context of device types?",
    "reference_contexts": [
      "str : debug str for compiler - dequant_type : str : dequantiztion type - device : str : System on Chip type [Gen-3, Gen-3b, Gen-4, Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - enable_ofm_reuse : bool : Enable the reuse of OFM region for IMFM. - enable_stm : bool : Generate compile log including L1 tiling information - flc : bool : Enable featuremap lossless compression - fp16_swwa : bool : Enable NPU fp16 workaround with psum_init - input_conversion : str : Add a Tensor2Cell format converter node at start of network - mi : bool : multiple input compile - mo : bool : multiple output compile - multi_ncp : bool : generate multi-ncp(ucgo) custom op - multi_vc : bool : Introduce Multi-VC for OFM, IFM, and weight transfer - multicore : bool : Enable NPU multicore - optimization : str : Optimization choice [O1, O2, O3] - output_conversion : str : Add a Tensor2Cell format converter node at end of network - packed_ucgo : bool : true: generate one ucgo in nnc, false: generate multiple ucgos in nnc if there are multiple independent dsp OPs - preemption : bool : Setting priority of NNC while compiling - quant_type : str : quantiztion type - sync_npu_dsp : bool : simulator : dict - data_format : str : Indicate the position of channel of input [channel_first, channel_last] - use_randomdb : bool : Use randomdb to forward, just support single input - userdb : str : Simulation data set path (default path is {workspace}/DATA/data.txt) perf_estimator : dict - O2_enable : bool : O2 optimization (true or false) - O2_fm_forwarding : bool : feature-map forwarding (true or false) - SEG : bool : Set true if input model is Deeplab V3+ - SSD : bool : Set true if input model is SSD detection - bit_width_factor_FM : int : Select feature map bit width factor (1 or 2) - bit_width_factor_FP16 : bool : Set bit width factor as floating point (true or false) - bit_width_factor_weight : int : Select weight bit width factor (1 or 2) - core_num : int : 1 for single core, 2 for instance-1 - device : str : Select device type [Gen-3, Gen-4, Gen-5, Gen-5a] - json_report : bool : Enable report json format - nq_fold : bool : Speed-up by folding normalization parameters for input data (true: model input dtype is uint8, false: model input dtype is float32) profiler : dict - iter : int : This decides how many time the model inference will be processed. - mode : str : 1. Low power mode : Mode for low power consumption and low performance. 2. Balanced mode : Mode for medium power consumption and medium performance. 3. Performance mode : Mode for high power consumption and high performance. 4. Boost mode : Mode for very high power consumption and very high performance. [lowpower, balanced, performance, boost] - target : str : profiling target. [model, system] - test_type : str : ENN running mode [lib, service] - tv_threshold : float : The value is used for tolerance threshold of output match verification. - bitmatch_test : bool : if set true, visual profiler will compile nnc first - core_num : str : The number of NPU core. [single, multiple] - device : str : Target device. [Gen-3, Gen-4, Gen-5, Gen-5a, Gen-6, Gen-6npu, Gen-7] - device_id : str : id of the device connected to the server or PC running the enntools docker - remote_ssh_config_path : str : remote testing ssh config path (default path is {workspace}/DATA/remote_ssh_config.yaml) - ssh_bool : str : Connect to the device through ssh. [SSH_FALSE, SSH_TRUE] eht yaml file The optimization file contains optimization information. It allows you to set the locations of input and output models and detailed configurations for each module. Optimization is performed differently depending on the model type. Detailed explanation for eht yaml file model_type : string Specify the type of the input model. Optimization is performed differently depending on the model type, with details as follows CV LVM LLM quantizer : dict precision_weight : str : precision of weight(ex. int8, int16, fp16) precision_activation : str : precision of activation(ex. int8, int16, fp16) mpq_operator_dict : dict : When performing mixed precision quantization, input the operators and precision to be quantized with a precision different from the values specified in precision_weight and precision_activation above. alpha : float : smoothquant migration strength calibration_data_path : string : The path to the representative data to be used for calibration calibration_args : dict : Arguments for processing calibration samples : int : How many calibration data samples to use seed : int : A value set as a seed for random selection add_dummy_conv: bool : Whether apply the dummy conv algorithm input_dtype: dict : Input data type for the quantized model output_dtype: dict : Output data type for the quantized model simulator : dict metric : string: The metric to be used for measurement threshold : float : The threshold value of the metric that determines agreement / disagreement input_data_path : string: The path to the dataset for model inference optimizer : dict skip_4_dim_conversion : bool : If true, it does not convert to 4 dimensions before optimization; if false, it proceeds with the conversion. overwrite_input_shapes : dict : Enter the input shape for models with undefined input shapes. custom_template_path : dict: Enter the templates to apply for optimization. Use the path of the Python file containing the template as the key, and the template within it as the value. enntools conversion When performing conversion, users can choose between using eht and elt depending on the mode. The detailed workflow is illustrated in the figure below. bash enntools conversion Introduction to Exynos AI Studio Exynos AI Studio is a software that provides optimization and conversion techniques for neural networks models. This software takes models as input and offers functionalities such as quantization, conversion, optimization, compilation to generate NNC models. System overview diagram Exynos AI Studio Documentation Exynos AI Studio References Exynos AI Studio Developer Guide This guide"
    ],
    "reference": "Gen-5b is one of the specified System on Chip types, which include Gen-3, Gen-3b, Gen-4, Gen-5, Gen-5a, Gen-5b, Gen-6, and Gen-7.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What devices are included in the Gen-6 category?",
    "reference_contexts": [
      "str : debug str for compiler - dequant_type : str : dequantiztion type - device : str : System on Chip type [Gen-3, Gen-3b, Gen-4, Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - enable_ofm_reuse : bool : Enable the reuse of OFM region for IMFM. - enable_stm : bool : Generate compile log including L1 tiling information - flc : bool : Enable featuremap lossless compression - fp16_swwa : bool : Enable NPU fp16 workaround with psum_init - input_conversion : str : Add a Tensor2Cell format converter node at start of network - mi : bool : multiple input compile - mo : bool : multiple output compile - multi_ncp : bool : generate multi-ncp(ucgo) custom op - multi_vc : bool : Introduce Multi-VC for OFM, IFM, and weight transfer - multicore : bool : Enable NPU multicore - optimization : str : Optimization choice [O1, O2, O3] - output_conversion : str : Add a Tensor2Cell format converter node at end of network - packed_ucgo : bool : true: generate one ucgo in nnc, false: generate multiple ucgos in nnc if there are multiple independent dsp OPs - preemption : bool : Setting priority of NNC while compiling - quant_type : str : quantiztion type - sync_npu_dsp : bool : simulator : dict - data_format : str : Indicate the position of channel of input [channel_first, channel_last] - use_randomdb : bool : Use randomdb to forward, just support single input - userdb : str : Simulation data set path (default path is {workspace}/DATA/data.txt) perf_estimator : dict - O2_enable : bool : O2 optimization (true or false) - O2_fm_forwarding : bool : feature-map forwarding (true or false) - SEG : bool : Set true if input model is Deeplab V3+ - SSD : bool : Set true if input model is SSD detection - bit_width_factor_FM : int : Select feature map bit width factor (1 or 2) - bit_width_factor_FP16 : bool : Set bit width factor as floating point (true or false) - bit_width_factor_weight : int : Select weight bit width factor (1 or 2) - core_num : int : 1 for single core, 2 for instance-1 - device : str : Select device type [Gen-3, Gen-4, Gen-5, Gen-5a] - json_report : bool : Enable report json format - nq_fold : bool : Speed-up by folding normalization parameters for input data (true: model input dtype is uint8, false: model input dtype is float32) profiler : dict - iter : int : This decides how many time the model inference will be processed. - mode : str : 1. Low power mode : Mode for low power consumption and low performance. 2. Balanced mode : Mode for medium power consumption and medium performance. 3. Performance mode : Mode for high power consumption and high performance. 4. Boost mode : Mode for very high power consumption and very high performance. [lowpower, balanced, performance, boost] - target : str : profiling target. [model, system] - test_type : str : ENN running mode [lib, service] - tv_threshold : float : The value is used for tolerance threshold of output match verification. - bitmatch_test : bool : if set true, visual profiler will compile nnc first - core_num : str : The number of NPU core. [single, multiple] - device : str : Target device. [Gen-3, Gen-4, Gen-5, Gen-5a, Gen-6, Gen-6npu, Gen-7] - device_id : str : id of the device connected to the server or PC running the enntools docker - remote_ssh_config_path : str : remote testing ssh config path (default path is {workspace}/DATA/remote_ssh_config.yaml) - ssh_bool : str : Connect to the device through ssh. [SSH_FALSE, SSH_TRUE] eht yaml file The optimization file contains optimization information. It allows you to set the locations of input and output models and detailed configurations for each module. Optimization is performed differently depending on the model type. Detailed explanation for eht yaml file model_type : string Specify the type of the input model. Optimization is performed differently depending on the model type, with details as follows CV LVM LLM quantizer : dict precision_weight : str : precision of weight(ex. int8, int16, fp16) precision_activation : str : precision of activation(ex. int8, int16, fp16) mpq_operator_dict : dict : When performing mixed precision quantization, input the operators and precision to be quantized with a precision different from the values specified in precision_weight and precision_activation above. alpha : float : smoothquant migration strength calibration_data_path : string : The path to the representative data to be used for calibration calibration_args : dict : Arguments for processing calibration samples : int : How many calibration data samples to use seed : int : A value set as a seed for random selection add_dummy_conv: bool : Whether apply the dummy conv algorithm input_dtype: dict : Input data type for the quantized model output_dtype: dict : Output data type for the quantized model simulator : dict metric : string: The metric to be used for measurement threshold : float : The threshold value of the metric that determines agreement / disagreement input_data_path : string: The path to the dataset for model inference optimizer : dict skip_4_dim_conversion : bool : If true, it does not convert to 4 dimensions before optimization; if false, it proceeds with the conversion. overwrite_input_shapes : dict : Enter the input shape for models with undefined input shapes. custom_template_path : dict: Enter the templates to apply for optimization. Use the path of the Python file containing the template as the key, and the template within it as the value. enntools conversion When performing conversion, users can choose between using eht and elt depending on the mode. The detailed workflow is illustrated in the figure below. bash enntools conversion Introduction to Exynos AI Studio Exynos AI Studio is a software that provides optimization and conversion techniques for neural networks models. This software takes models as input and offers functionalities such as quantization, conversion, optimization, compilation to generate NNC models. System overview diagram Exynos AI Studio Documentation Exynos AI Studio References Exynos AI Studio Developer Guide This guide"
    ],
    "reference": "The Gen-6 category includes devices such as Gen-6 and Gen-6npu.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is a Neural Network Container?",
    "reference_contexts": [
      "describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "A Neural Network Container (NNC) model is created by converting Neural Network (NN) models using the Exynos Neural Network Software Development Kit (Exynos AI Studio).",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What role does GPU play in Exynos AI Studio?",
    "reference_contexts": [
      "describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "The GPU is one of the supported operators in Exynos AI Studio, allowing for the execution of various operations on models, including convolution and pooling.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How does the CPU support the execution of models in Exynos AI Studio?",
    "reference_contexts": [
      "describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "The CPU is one of the supported operators in Exynos AI Studio, allowing for the execution of various operations such as ADD, DIV, and CONVOLUTION. It is essential for running models that may not be optimized for other hardware accelerators like NPU or DSP.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the requirements for models to be compatible with the NPU in Exynos AI Studio?",
    "reference_contexts": [
      "describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "For models to be compatible with the NPU in Exynos AI Studio, they must be quantized. Additionally, the supported models include TensorFlow Lite, which is recommended to be converted using MLIR version 1.14 or higher, and models can have a maximum size of 1 GB with up to four dimensions.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "Can you elaborate on the functionalities provided by Exynos AI Studio for AI model development and deployment?",
    "reference_contexts": [
      "describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "Exynos AI Studio offers a comprehensive set of functionalities for AI model development and deployment. It includes the Exynos Neural Network Software Development Kit, which provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. The toolkit also encompasses the ENN framework, which facilitates input to the model, execution of the model, and obtaining the output. Additionally, the API reference documentation lists various data types and functions available in Exynos AI Studio. The support matrices detail the supported platforms and operators, while the Quick Start Guide provides basic instructions for using the toolkit, including model conversion and execution on Exynos devices. Furthermore, Exynos AI Studio includes sample guides for developing applications, such as an image classification Android application and a native program.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "Can you explain the ENN framework and its role in the Exynos AI Studio toolkit?",
    "reference_contexts": [
      "describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "The ENN framework provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It includes information on providing input to the model, executing the model, and obtaining the output.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the supported operators for CPU in Exynos AI Studio?",
    "reference_contexts": [
      "describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "The supported operators for CPU in Exynos AI Studio include: ABS, ADD, ARGMAX, ARGMIN, AVGPOOL, CEIL, CONCATENATION, CONSTANT, CONVOLUTION, COS, CROP, DECONVOLUTION, DEPTH_TO_SPACE, DEPTHWISE_CONVOLUTION, DEPTHWISE_DECONVOLUTION, DEPTHWISE_DILATION_CONVOLUTION, DEQUANTIZE, DILATION_CONVOLUTION, DIV, ELEMENTWISE_DIV, ELEMENTWISE_MUL, ELEMENTWISE_SUB, ELEMENTWISE_SUM, EXP, FLATTEN, FLOOR, FLOOR_DIV, FULLY_CONNECTED, GATHER, GLOBAL_MAXPOOL, GROUP_CONVOLUTION, HARD_SWISH, LEAKY_RELU, LOG, LOGISTIC, MAXIMUM, MAXPOOL, MEAN, MINIMUM, and MIRROR_PAD.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What functionalities does Exynos AI Studio provide for AI model development?",
    "reference_contexts": [
      "describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "Exynos AI Studio provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models, information about the ENN framework, and guidance on providing input to the model, executing the model, and obtaining the output. It also includes API references, a support matrix for compatible platforms and operators, and a Quick Start Guide for basic usage.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How does Ubuntu 22.04 relate to the requirements for using the Exynos AI Studio service?",
    "reference_contexts": [
      "Tools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)"
    ],
    "reference": "Ubuntu 22.04 is specified as the operating system required for the Exynos AI Studio service, which is an online platform designed for converting TFLite models into NNC models. The service also requires a minimum of 8GB RAM, with 16GB or more recommended, and sufficient disk space of at least 100GB.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the minimum specifications for using the Intel Core i7 processor in AI application development?",
    "reference_contexts": [
      "Tools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)"
    ],
    "reference": "The minimum specifications for using the Intel Core i7 processor in AI application development include a 64-bit (x86-64) architecture, a minimum of 2GHz dual-core processor, at least 8GB RAM (16GB or more recommended), and sufficient disk space with a minimum of 100GB recommended.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How do I access the ENN Eco-system?",
    "reference_contexts": [
      "Tools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)"
    ],
    "reference": "To access the ENN Eco-system, if you are a new user, you need to sign up to create an account. If you are an existing user, you can log in to the ENN eco-system.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How does the Exynos AI Studio service compare to NVIDIA tools for AI model conversion?",
    "reference_contexts": [
      "Tools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)"
    ],
    "reference": "The Exynos AI Studio service is specifically designed to convert TFLite models into NNC models, allowing users to choose hardware preferences such as CPU, GPU, or NPU for acceleration. In contrast, NVIDIA tools are not mentioned in the context, so a direct comparison cannot be made. However, the Exynos AI Studio emphasizes the importance of hardware compatibility and model optimization for effective deployment on Samsung Exynos hardware.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How can I convert TFLite models into NNC models using the Exynos AI Studio service?",
    "reference_contexts": [
      "Tools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)"
    ],
    "reference": "To convert TFLite models into NNC models using the Exynos AI Studio service, you need to follow these steps: First, access the ENN Eco-system by signing up for a new account or logging in if you are an existing user. Then, navigate to the Exynos AI Studio service page. Provide project information by entering a descriptive title and uploading your TFLite model. Choose your hardware preferences, either default (using only CPU and GPU) or accelerate (engaging the NPU as an additional accelerator, keeping in mind that the NPU does not support all layers). After confirming your selections, click Convert to start the model conversion process. If successful, the NNC Download button will be enabled, allowing you to download the NNC model for integration into your application.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How to access ENN Eco-system?",
    "reference_contexts": [
      "Tools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)"
    ],
    "reference": "To access the ENN Eco-system, if you are a new user, you need to sign up to create an account. If you are an existing user, you should log in to the ENN eco-system.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "Wht is the software requirement for Exynos AI Studio on Ubuntu 22.04?",
    "reference_contexts": [
      "Tools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)"
    ],
    "reference": "The software requirements for Exynos AI Studio on Ubuntu 22.04 include a Linux operating system, NVIDIA driver version 450.80.02 or later, Docker version 19.03 or later (with NVIDIA Container Toolkit support), and the NVIDIA Container Toolkit (nvidia-docker2).",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How can I use TFLite models in Exynos AI Studio?",
    "reference_contexts": [
      "Tools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)"
    ],
    "reference": "To use TFLite models in Exynos AI Studio, you need to access the ENN Eco-system, sign up or log in, navigate to the Exynos AI Studio service page, and provide project information by entering a descriptive title and uploading your TFLite model. You can then choose hardware preferences, initiate the conversion by clicking Confirm and Convert, and finally download the converted NNC model if the conversion is successful.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps to utilize the GPU for model conversion and execution using Exynos AI Studio, and how does this relate to the support for different hardware types?",
    "reference_contexts": [
      "<1-hop>\n\ndescribes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |",
      "<2-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "To utilize the GPU for model conversion and execution using Exynos AI Studio, you first need to choose the Accelerate hardware type, which engages the NPU as an additional accelerator alongside the CPU and GPU. The process begins with converting Neural Network (NN) models to Neural Network Container (NNC) models. After selecting the Accelerate option, you initiate the conversion process by clicking 'Convert'. Once the conversion is complete, the NNC Download button becomes available for obtaining the converted model file. Additionally, the support matrix indicates that Exynos AI Studio supports various operators on the GPU, ensuring compatibility with models that can leverage GPU acceleration. This integration allows for optimized performance during model execution on Exynos devices.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How do you utilize the CPU and NPU for model conversion in Exynos AI Studio, and what are the potential complications when choosing the Accelerate option?",
    "reference_contexts": [
      "<1-hop>\n\ndescribes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |",
      "<2-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "To utilize the CPU and NPU for model conversion in Exynos AI Studio, you first need to select the Accelerate hardware type, which engages the NPU as an additional accelerator alongside the CPU. However, it is important to note that the NPU does not support all models, and choosing the Accelerate option may lead to complications during the conversion process. After confirming your selections, you can initiate the conversion by clicking 'Convert', which will enable the NNC Download button upon completion. If any conversion fails, you can download the logs to examine the issues.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How do you utilize the CPU and NPU when converting NN models to NNC models using Exynos AI Studio?",
    "reference_contexts": [
      "<1-hop>\n\ndescribes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |",
      "<2-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "To utilize the CPU and NPU when converting NN models to NNC models using Exynos AI Studio, you first need to choose the Accelerate hardware type, which engages the NPU as an additional accelerator alongside the CPU. However, it's important to note that the NPU does not support all models, and selecting the Accelerate option may lead to complications. After confirming your selections, you can initiate the conversion process by clicking Convert, which will enable the NNC Download button upon completion.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps to utilize the CPU and NPU for model conversion in Exynos AI Studio, and what should be considered regarding model compatibility?",
    "reference_contexts": [
      "<1-hop>\n\ndescribes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |",
      "<2-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "To utilize the CPU and NPU for model conversion in Exynos AI Studio, first, you need to choose the Accelerate hardware type, which engages the NPU as an additional accelerator. However, it is important to note that the NPU does not support all models, and choosing the Accelerate option may lead to complications. After confirming your selections, you can initiate the conversion process by clicking Convert. Once the conversion is complete, the NNC Download button will be enabled, allowing you to download the converted NNC model file. Additionally, ensure that the models you are using are compatible with Exynos AI Studio, as the support matrices provide information on the supported platforms and operators.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps to utilize the GPU for model conversion in Exynos AI Studio, and how does it differ from using only the CPU?",
    "reference_contexts": [
      "<1-hop>\n\ndescribes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |",
      "<2-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "To utilize the GPU for model conversion in Exynos AI Studio, you first need to select the 'Accelerate' hardware type during the conversion process. This option engages the NPU as an additional accelerator alongside the GPU. The steps include initiating the conversion by clicking 'Convert' after confirming your selections. Once the conversion is complete, the 'NNC Download' button becomes available for downloading the converted NNC model file. In contrast, if you choose to use only the CPU, the default setting will not engage the NPU, which may limit performance and efficiency during the model execution. Therefore, using the GPU in conjunction with the NPU can enhance the processing capabilities compared to relying solely on the CPU.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps to convert a TensorFlow Lite model to a Neural Network Container (NNC) model using Exynos AI Studio, and how does this process relate to the optimization techniques provided by the Exynos AI High-Level Toolchain (EHT)?",
    "reference_contexts": [
      "<1-hop>\n\nTroubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For",
      "<2-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "To convert a TensorFlow Lite model to a Neural Network Container (NNC) model using Exynos AI Studio, users must follow these steps: First, prepare a pre-trained NN model in TensorFlow Lite format. Next, set the parameters for the conversion tools provided by Exynos AI Studio. Finally, execute the tools to perform the conversion. This process is essential as NNC is a model format that can run efficiently on Samsung Exynos hardware. The Exynos AI High-Level Toolchain (EHT) complements this by offering optimization techniques for neural networks, including quantization and model optimization, which enhance the performance of the converted models. The EHT allows users to apply various optimization features to their TensorFlow models, ensuring that the models are not only converted but also optimized for efficient execution on the hardware.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is the process for converting TensorFlow Lite models to NNC models using Exynos AI Studio, and how does it relate to the support for TensorFlow?",
    "reference_contexts": [
      "<1-hop>\n\nTroubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For",
      "<2-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "The process for converting TensorFlow Lite models to NNC models using Exynos AI Studio involves several steps. First, users must prepare a pre-trained TensorFlow Lite model. Then, they set parameters for the conversion tools provided by Exynos AI Studio. After setting the parameters, users execute the tools to perform the conversion. This conversion is essential as NNC is a model format that can run efficiently on Samsung Exynos hardware. The Exynos AI Studio specifically supports TensorFlow Lite models, allowing users to leverage the capabilities of TensorFlow in their applications.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How can I convert a TensorFlow Lite model to ONNX using Exynos AI Studio?",
    "reference_contexts": [
      "<1-hop>\n\nTroubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For",
      "<2-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "To convert a TensorFlow Lite model to ONNX using Exynos AI Studio, you can use the TF2ONNX Converter. This converter requires an ONNX opset for exporting. You would need to specify the input model path for your TensorFlow Lite model and the output model path for the resulting ONNX model. An example code snippet for this conversion is as follows: ```python from converter import api tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` This process allows you to utilize the Exynos AI Studio's capabilities to optimize and convert your models effectively.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How can I convert a TensorFlow Lite model to an ONNX model using Exynos AI Studio, and what are the requirements for the model format?",
    "reference_contexts": [
      "<1-hop>\n\nTroubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For",
      "<2-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "To convert a TensorFlow Lite model to an ONNX model using Exynos AI Studio, you can use the TF2ONNX Converter, which requires an ONNX opset for exporting. The model must be prepared in ONNX format to start optimization, and the Exynos AI Studio currently supports ONNX opset versions 13 to 17. Additionally, the dataset format must use the .h5 file extension, and the internal data within the h5 file must be in float32 format.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How do you convert a TFLite model into an NNC model using the Exynos AI Studio service, and what precautions should you take regarding hardware preferences and unsupported layers?",
    "reference_contexts": [
      "<1-hop>\n\nTools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)",
      "<2-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "To convert a TFLite model into an NNC model using the Exynos AI Studio service, you first need to access the ENN Eco-system by signing up or logging in. Then, navigate to the Exynos AI Studio service page and provide project information by entering a descriptive title and uploading your TFLite model. When choosing hardware preferences, you can select the default option, which utilizes only the CPU and GPU, or the Accelerate option, which engages the NPU as an additional accelerator. However, it is important to note that the NPU does not support all layers, and using unsupported layers may lead to complications. After confirming your selections, you can initiate the conversion process by clicking Convert. If the conversion is successful, the NNC Download button will be enabled, allowing you to download the converted NNC model.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What steps must be taken to convert a TFLite model into an NNC model using the Exynos AI Studio service, and what considerations should be made regarding hardware preferences during this process?",
    "reference_contexts": [
      "<1-hop>\n\nTools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)",
      "<2-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "To convert a TFLite model into an NNC model using the Exynos AI Studio service, follow these steps: First, access the ENN Eco-system by signing up or logging in. Then, navigate to the Exynos AI Studio service page and provide project information by entering a descriptive title and uploading your TFLite model. Next, choose your hardware preferences: the default option utilizes only the CPU and GPU, while the 'Accelerate' option engages the NPU as an additional accelerator. However, it is important to note that the NPU does not support all layers, and using unsupported layers may lead to complications. After confirming your selections, click 'Convert' to initiate the conversion process. If successful, the NNC Download button will be enabled, allowing you to download the converted NNC model.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How do you convert TFLite models into NNC models using the Exynos AI Studio service, and what should you be cautious about regarding NPU support?",
    "reference_contexts": [
      "<1-hop>\n\nTools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)",
      "<2-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "To convert TFLite models into NNC models using the Exynos AI Studio service, you need to access the ENN Eco-system, sign up or log in, and navigate to the Exynos AI Studio service page. After providing project information and uploading your TFLite model, you can choose hardware preferences, either using the default settings (CPU and GPU) or selecting the Accelerate option to engage the NPU as an additional accelerator. However, it is important to note that the NPU does not support all layers, and using unsupported layers may lead to complications during the conversion process.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What steps are involved in converting a TFLite model to an NNC model using the Exynos AI Studio service, and what hardware preferences should be selected?",
    "reference_contexts": [
      "<1-hop>\n\nTools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)",
      "<2-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "To convert a TFLite model to an NNC model using the Exynos AI Studio service, follow these steps: First, access the ENN Eco-system by signing up or logging in. Then, navigate to the Exynos AI Studio service page and provide project information by entering a descriptive title and uploading your TFLite model. Next, choose your hardware preferences: you can select 'Default' to utilize only the CPU and GPU, or 'Accelerate' to engage the NPU as an additional accelerator. However, be aware that the NPU does not support all layers, and using unsupported layers may lead to complications. After confirming your selections, click 'Convert' to initiate the conversion process. If successful, the NNC Download button will be enabled, allowing you to download the converted NNC model.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What steps are involved in converting a TFLite model to an NNC model using the Exynos AI Studio service, and what hardware preferences should be selected for optimal performance?",
    "reference_contexts": [
      "<1-hop>\n\nTools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)",
      "<2-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "To convert a TFLite model to an NNC model using the Exynos AI Studio service, follow these steps: First, access the ENN Eco-system by signing up or logging in. Then, navigate to the Exynos AI Studio service page and provide project information by entering a descriptive title and uploading your TFLite model. Next, choose your hardware preferences: the default option utilizes only the CPU and GPU, while the 'Accelerate' option engages the NPU as an additional accelerator. However, be aware that the NPU does not support all layers, and using unsupported layers may lead to complications. After confirming your selections, click 'Convert' to initiate the conversion process. If successful, the NNC Download button will be enabled, allowing you to download the converted NNC model.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps involved in converting a TensorFlow Lite model to an NNC model using the Exynos AI Studio service?",
    "reference_contexts": [
      "<1-hop>\n\nTools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)",
      "<2-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "To convert a TensorFlow Lite model to an NNC model using the Exynos AI Studio service, follow these steps: 1. Access the ENN Eco-system by signing up or logging in. 2. Navigate to the Exynos AI Studio service page. 3. Provide project information by entering a descriptive title and uploading your TFLite model. 4. Choose hardware preferences, either using only the CPU and GPU or engaging the NPU as an additional accelerator, keeping in mind that the NPU does not support all layers. 5. Click Confirm to verify your selections and then click Convert to start the model conversion process. 6. If the conversion is successful, the NNC Download button will be enabled, allowing you to download the NNC model.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How do you convert TFLite models to NNC models using Exynos AI Studio?",
    "reference_contexts": [
      "<1-hop>\n\nTools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)",
      "<2-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "To convert TFLite models to NNC models using Exynos AI Studio, you need to access the Exynos AI Studio service, sign up or log in to the ENN eco-system, and navigate to the service page. Then, provide project information by entering a descriptive title and uploading your TFLite model. Choose your hardware preferences, either default (using only CPU and GPU) or accelerate (engaging the NPU as an additional accelerator). After confirming your selections, click 'Convert' to start the model conversion process. If successful, the NNC Download button will be enabled, allowing you to download the converted NNC model.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is the process for converting a TensorFlow Lite model to an NNC model using the Exynos AI Studio service?",
    "reference_contexts": [
      "<1-hop>\n\nTools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)",
      "<2-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "The process for converting a TensorFlow Lite model to an NNC model using the Exynos AI Studio service involves several steps: First, users must access the ENN Eco-system by signing up or logging in. Next, they navigate to the Exynos AI Studio service page and provide project information by entering a descriptive title and uploading the TFLite model. Users then choose hardware preferences, either utilizing only the CPU and GPU or engaging the NPU as an additional accelerator, keeping in mind that the NPU does not support all layers. After confirming their selections, they click 'Convert' to initiate the model conversion process. If successful, the NNC Download button becomes enabled, allowing users to download the converted NNC model for integration into their application.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How do you convert a TensorFlow Lite model to an NNC model using Exynos AI Studio, and what are the requirements for the dataset format?",
    "reference_contexts": [
      "<1-hop>\n\nTools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)",
      "<2-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "To convert a TensorFlow Lite model to an NNC model using Exynos AI Studio, you need to follow these steps: First, access the Exynos AI Studio service and log in to the ENN eco-system. Then, provide project information by entering a descriptive title and uploading your TFLite model. Choose your hardware preferences, either using only the CPU and GPU or engaging the NPU as an additional accelerator, keeping in mind that the NPU does not support all layers. After confirming your selections, click 'Convert' to start the model conversion process. If successful, you can download the NNC model using the NNC Download button. Regarding the dataset format, it must use the .h5 file extension, and the keys in the h5 dataset should match the input names of the model for proper mapping. Additionally, the internal data within the h5 file must be in float32 format, and the h5 file should be placed directly under the DATA folder generated after executing the 'enntools init' command.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps involved in converting a TensorFlow Lite model to an NNC model using the Exynos AI Studio, and how does the ENN framework facilitate the execution of the converted model?",
    "reference_contexts": [
      "<1-hop>\n\nTools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)",
      "<2-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "To convert a TensorFlow Lite model to an NNC model using the Exynos AI Studio, the following steps are involved: First, the user must prepare a pre-trained NN model and set parameters for the conversion tools. Then, the user executes the tools for conversion. Once the model is converted, the user can execute the NNC model using the ENN framework. The execution process involves initializing the ENN framework, loading the converted model, allocating and committing necessary buffers, copying input data to input buffers, executing the model, and using data from output buffers. This process can be repeated for multiple executions, and finally, the user must uncommit and release the buffers, unload the model, and deinitialize the ENN framework.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the key functionalities of the ENN framework as described in the Exynos AI Studio documentation, and how does it relate to the process of converting Neural Network models to Neural Network Container models?",
    "reference_contexts": [
      "<1-hop>\n\nIntroduction EHT Converter Optimizer Quantizer Simulator Model Optimization Flow ELT Model Analyzer SCVT Quantizer NPU Simulator Performance Estimator Graph Generator Dataset and Model Quick Start System Requirements How to use ENN Framework Data Type References _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; API reference ENN Framework Data Type References _NumberOfBuffersInfo _ennBuffer _ennBufferInfo ENN Framwork API Functions Context initialize / deinitialize OpenModel / CloseModel related Memory Handling Setters and Getters for model Commit Buffer Execute Models Security, preference, get meta information.. title: _ennBuffer _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; Updated on 2023-08-11 at 16:24:05 +0900 title: _ennBufferInfo _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; Updated on 2023-08-11 at 16:24:05 +0900 title: _NumberOfBuffersInfo _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; Updated on 2023-08-11 at 16:24:05 +0900 title: Commit Buffer Commit Buffer Functions Name EnnReturn EnnBufferCommit (const EnnModelId model_id, const int session_id =0) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Functions Documentation function EnnBufferCommit EnnReturn EnnBufferCommit( const EnnModelId model_id, const int session_id =0 ) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Parameters: model_id [IN] model ID from load_model Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Context initialize / deinitialize Context initialize / deinitialize Functions Name EnnReturn EnnInitialize (void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. EnnReturn EnnDeinitialize (void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Functions Documentation function EnnInitialize EnnReturn EnnInitialize( void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. Return: EnnReturn result, 0 is success function EnnDeinitialize EnnReturn EnnDeinitialize( void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Security, preference, get meta information.. Security, preference, get meta information.. Functions Name EnnReturn EnnSetPreferencePresetId (const uint32_t val) Setting Preset ID for operation performance. EnnReturn EnnSetPreferencePerfConfigId (const uint32_t val) Setting PerfConfig ID for operation performance. EnnReturn EnnSetPreferencePerfMode (const uint32_t val) Setting Performance Mode. EnnReturn EnnSetPreferenceTimeOut (const uint32_t val) Setting Preset ID for time out. EnnReturn EnnSetPreferencePriority (const uint32_t val) Setting priority value for NPU. EnnReturn EnnSetPreferenceCoreAffinity (const uint32_t val) Setting affinity to set NPU core operation. EnnReturn EnnGetPreferencePresetId (uint32_t * val_ptr) Get current information for Preset ID. EnnReturn EnnGetPreferencePerfConfigId (uint32_t * val_ptr) Get current information for PerfConfig ID. EnnReturn EnnGetPreferencePerfMode (uint32_t * val_ptr) Get current information for Performance Mode. EnnReturn EnnGetPreferenceTimeOut (uint32_t * val_ptr) Get current information for Time Out. EnnReturn EnnGetPreferencePriority (uint32_t * val_ptr) Get current information for NPU Priority. EnnReturn EnnGetPreferenceCoreAffinity (uint32_t * val_ptr) Get current information for NPU Core affinity. EnnReturn EnnGetMetaInfo (const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]) Get Meta Information. EnnReturn EnnSetExecMsgAlwaysOn () Set frequency of execution message print. Functions Documentation function EnnSetPreferencePresetId EnnReturn EnnSetPreferencePresetId( const uint32_t val ) Setting Preset ID for operation performance. Parameters: val [IN] value to set preset ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfConfigId EnnReturn EnnSetPreferencePerfConfigId( const uint32_t val ) Setting PerfConfig ID for operation performance. Parameters: val [IN] value to set PerfConfig ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfMode EnnReturn EnnSetPreferencePerfMode( const uint32_t val ) Setting Performance Mode. Parameters: val [IN] value to set Performance Mode Return: EnnReturn result, 0 is success function EnnSetPreferenceTimeOut EnnReturn EnnSetPreferenceTimeOut( const uint32_t val ) Setting Preset ID for time out. Parameters: val [IN] value to set time out Return: EnnReturn result, 0 is success Note: in second function EnnSetPreferencePriority EnnReturn EnnSetPreferencePriority( const uint32_t val ) Setting priority value for NPU. Parameters: val [IN] value to set NPU job priority Return: EnnReturn result, 0 is success function EnnSetPreferenceCoreAffinity EnnReturn EnnSetPreferenceCoreAffinity( const uint32_t val ) Setting affinity to set NPU core operation. Parameters: val [IN] value to set affinity Return: EnnReturn result, 0 is success Note: in second function EnnGetPreferencePresetId EnnReturn EnnGetPreferencePresetId( uint32_t * val_ptr ) Get current information for Preset ID. Parameters: val [OUT] current value of Preset ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfConfigId EnnReturn EnnGetPreferencePerfConfigId( uint32_t * val_ptr ) Get current information for PerfConfig ID. Parameters: val [OUT] current value of PerfConfig ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfMode EnnReturn EnnGetPreferencePerfMode( uint32_t * val_ptr ) Get current information for Performance Mode. Parameters: val [OUT] current value of Performance Mode Return: EnnReturn result, 0 is success function EnnGetPreferenceTimeOut EnnReturn EnnGetPreferenceTimeOut( uint32_t * val_ptr ) Get current information for Time Out. Parameters: val [OUT] current value of Time Out Return: EnnReturn result, 0 is success function EnnGetPreferencePriority EnnReturn EnnGetPreferencePriority( uint32_t * val_ptr ) Get current information for NPU Priority. Parameters: val [OUT] current value of NPU Priority Return:",
      "<2-hop>\n\ndescribes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "The ENN framework, as described in the Exynos AI Studio documentation, provides essential functionalities for model optimization and execution. It includes features such as context initialization and deinitialization, memory handling, and the ability to commit buffers for model execution. The framework allows developers to input data into the model, execute it, and obtain the output. Additionally, the documentation outlines the process of converting Neural Network (NN) models to Neural Network Container (NNC) models, which is crucial for deploying models on Exynos devices. This conversion process is part of the broader usage guide for Exynos AI Studio, which also includes API references and support matrices for compatible models and operators.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is the role of the ENN framework in the Exynos AI Studio for model execution and how does it relate to the conversion of Neural Network models?",
    "reference_contexts": [
      "<1-hop>\n\nIntroduction EHT Converter Optimizer Quantizer Simulator Model Optimization Flow ELT Model Analyzer SCVT Quantizer NPU Simulator Performance Estimator Graph Generator Dataset and Model Quick Start System Requirements How to use ENN Framework Data Type References _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; API reference ENN Framework Data Type References _NumberOfBuffersInfo _ennBuffer _ennBufferInfo ENN Framwork API Functions Context initialize / deinitialize OpenModel / CloseModel related Memory Handling Setters and Getters for model Commit Buffer Execute Models Security, preference, get meta information.. title: _ennBuffer _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; Updated on 2023-08-11 at 16:24:05 +0900 title: _ennBufferInfo _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; Updated on 2023-08-11 at 16:24:05 +0900 title: _NumberOfBuffersInfo _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; Updated on 2023-08-11 at 16:24:05 +0900 title: Commit Buffer Commit Buffer Functions Name EnnReturn EnnBufferCommit (const EnnModelId model_id, const int session_id =0) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Functions Documentation function EnnBufferCommit EnnReturn EnnBufferCommit( const EnnModelId model_id, const int session_id =0 ) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Parameters: model_id [IN] model ID from load_model Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Context initialize / deinitialize Context initialize / deinitialize Functions Name EnnReturn EnnInitialize (void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. EnnReturn EnnDeinitialize (void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Functions Documentation function EnnInitialize EnnReturn EnnInitialize( void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. Return: EnnReturn result, 0 is success function EnnDeinitialize EnnReturn EnnDeinitialize( void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Security, preference, get meta information.. Security, preference, get meta information.. Functions Name EnnReturn EnnSetPreferencePresetId (const uint32_t val) Setting Preset ID for operation performance. EnnReturn EnnSetPreferencePerfConfigId (const uint32_t val) Setting PerfConfig ID for operation performance. EnnReturn EnnSetPreferencePerfMode (const uint32_t val) Setting Performance Mode. EnnReturn EnnSetPreferenceTimeOut (const uint32_t val) Setting Preset ID for time out. EnnReturn EnnSetPreferencePriority (const uint32_t val) Setting priority value for NPU. EnnReturn EnnSetPreferenceCoreAffinity (const uint32_t val) Setting affinity to set NPU core operation. EnnReturn EnnGetPreferencePresetId (uint32_t * val_ptr) Get current information for Preset ID. EnnReturn EnnGetPreferencePerfConfigId (uint32_t * val_ptr) Get current information for PerfConfig ID. EnnReturn EnnGetPreferencePerfMode (uint32_t * val_ptr) Get current information for Performance Mode. EnnReturn EnnGetPreferenceTimeOut (uint32_t * val_ptr) Get current information for Time Out. EnnReturn EnnGetPreferencePriority (uint32_t * val_ptr) Get current information for NPU Priority. EnnReturn EnnGetPreferenceCoreAffinity (uint32_t * val_ptr) Get current information for NPU Core affinity. EnnReturn EnnGetMetaInfo (const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]) Get Meta Information. EnnReturn EnnSetExecMsgAlwaysOn () Set frequency of execution message print. Functions Documentation function EnnSetPreferencePresetId EnnReturn EnnSetPreferencePresetId( const uint32_t val ) Setting Preset ID for operation performance. Parameters: val [IN] value to set preset ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfConfigId EnnReturn EnnSetPreferencePerfConfigId( const uint32_t val ) Setting PerfConfig ID for operation performance. Parameters: val [IN] value to set PerfConfig ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfMode EnnReturn EnnSetPreferencePerfMode( const uint32_t val ) Setting Performance Mode. Parameters: val [IN] value to set Performance Mode Return: EnnReturn result, 0 is success function EnnSetPreferenceTimeOut EnnReturn EnnSetPreferenceTimeOut( const uint32_t val ) Setting Preset ID for time out. Parameters: val [IN] value to set time out Return: EnnReturn result, 0 is success Note: in second function EnnSetPreferencePriority EnnReturn EnnSetPreferencePriority( const uint32_t val ) Setting priority value for NPU. Parameters: val [IN] value to set NPU job priority Return: EnnReturn result, 0 is success function EnnSetPreferenceCoreAffinity EnnReturn EnnSetPreferenceCoreAffinity( const uint32_t val ) Setting affinity to set NPU core operation. Parameters: val [IN] value to set affinity Return: EnnReturn result, 0 is success Note: in second function EnnGetPreferencePresetId EnnReturn EnnGetPreferencePresetId( uint32_t * val_ptr ) Get current information for Preset ID. Parameters: val [OUT] current value of Preset ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfConfigId EnnReturn EnnGetPreferencePerfConfigId( uint32_t * val_ptr ) Get current information for PerfConfig ID. Parameters: val [OUT] current value of PerfConfig ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfMode EnnReturn EnnGetPreferencePerfMode( uint32_t * val_ptr ) Get current information for Performance Mode. Parameters: val [OUT] current value of Performance Mode Return: EnnReturn result, 0 is success function EnnGetPreferenceTimeOut EnnReturn EnnGetPreferenceTimeOut( uint32_t * val_ptr ) Get current information for Time Out. Parameters: val [OUT] current value of Time Out Return: EnnReturn result, 0 is success function EnnGetPreferencePriority EnnReturn EnnGetPreferencePriority( uint32_t * val_ptr ) Get current information for NPU Priority. Parameters: val [OUT] current value of NPU Priority Return:",
      "<2-hop>\n\ndescribes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "The ENN framework plays a crucial role in the Exynos AI Studio by providing the necessary functions for executing models and obtaining outputs. It facilitates the conversion of Neural Network (NN) models to Neural Network Container (NNC) models, which can then be executed on Exynos devices. The framework includes various API functions that handle model initialization, buffer management, and performance settings, ensuring that the models are optimized for deployment.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is the role of the ENN framework in the Exynos AI Studio, and how does it facilitate the conversion of Neural Network models?",
    "reference_contexts": [
      "<1-hop>\n\nIntroduction EHT Converter Optimizer Quantizer Simulator Model Optimization Flow ELT Model Analyzer SCVT Quantizer NPU Simulator Performance Estimator Graph Generator Dataset and Model Quick Start System Requirements How to use ENN Framework Data Type References _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; API reference ENN Framework Data Type References _NumberOfBuffersInfo _ennBuffer _ennBufferInfo ENN Framwork API Functions Context initialize / deinitialize OpenModel / CloseModel related Memory Handling Setters and Getters for model Commit Buffer Execute Models Security, preference, get meta information.. title: _ennBuffer _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; Updated on 2023-08-11 at 16:24:05 +0900 title: _ennBufferInfo _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; Updated on 2023-08-11 at 16:24:05 +0900 title: _NumberOfBuffersInfo _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; Updated on 2023-08-11 at 16:24:05 +0900 title: Commit Buffer Commit Buffer Functions Name EnnReturn EnnBufferCommit (const EnnModelId model_id, const int session_id =0) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Functions Documentation function EnnBufferCommit EnnReturn EnnBufferCommit( const EnnModelId model_id, const int session_id =0 ) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Parameters: model_id [IN] model ID from load_model Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Context initialize / deinitialize Context initialize / deinitialize Functions Name EnnReturn EnnInitialize (void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. EnnReturn EnnDeinitialize (void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Functions Documentation function EnnInitialize EnnReturn EnnInitialize( void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. Return: EnnReturn result, 0 is success function EnnDeinitialize EnnReturn EnnDeinitialize( void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Security, preference, get meta information.. Security, preference, get meta information.. Functions Name EnnReturn EnnSetPreferencePresetId (const uint32_t val) Setting Preset ID for operation performance. EnnReturn EnnSetPreferencePerfConfigId (const uint32_t val) Setting PerfConfig ID for operation performance. EnnReturn EnnSetPreferencePerfMode (const uint32_t val) Setting Performance Mode. EnnReturn EnnSetPreferenceTimeOut (const uint32_t val) Setting Preset ID for time out. EnnReturn EnnSetPreferencePriority (const uint32_t val) Setting priority value for NPU. EnnReturn EnnSetPreferenceCoreAffinity (const uint32_t val) Setting affinity to set NPU core operation. EnnReturn EnnGetPreferencePresetId (uint32_t * val_ptr) Get current information for Preset ID. EnnReturn EnnGetPreferencePerfConfigId (uint32_t * val_ptr) Get current information for PerfConfig ID. EnnReturn EnnGetPreferencePerfMode (uint32_t * val_ptr) Get current information for Performance Mode. EnnReturn EnnGetPreferenceTimeOut (uint32_t * val_ptr) Get current information for Time Out. EnnReturn EnnGetPreferencePriority (uint32_t * val_ptr) Get current information for NPU Priority. EnnReturn EnnGetPreferenceCoreAffinity (uint32_t * val_ptr) Get current information for NPU Core affinity. EnnReturn EnnGetMetaInfo (const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]) Get Meta Information. EnnReturn EnnSetExecMsgAlwaysOn () Set frequency of execution message print. Functions Documentation function EnnSetPreferencePresetId EnnReturn EnnSetPreferencePresetId( const uint32_t val ) Setting Preset ID for operation performance. Parameters: val [IN] value to set preset ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfConfigId EnnReturn EnnSetPreferencePerfConfigId( const uint32_t val ) Setting PerfConfig ID for operation performance. Parameters: val [IN] value to set PerfConfig ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfMode EnnReturn EnnSetPreferencePerfMode( const uint32_t val ) Setting Performance Mode. Parameters: val [IN] value to set Performance Mode Return: EnnReturn result, 0 is success function EnnSetPreferenceTimeOut EnnReturn EnnSetPreferenceTimeOut( const uint32_t val ) Setting Preset ID for time out. Parameters: val [IN] value to set time out Return: EnnReturn result, 0 is success Note: in second function EnnSetPreferencePriority EnnReturn EnnSetPreferencePriority( const uint32_t val ) Setting priority value for NPU. Parameters: val [IN] value to set NPU job priority Return: EnnReturn result, 0 is success function EnnSetPreferenceCoreAffinity EnnReturn EnnSetPreferenceCoreAffinity( const uint32_t val ) Setting affinity to set NPU core operation. Parameters: val [IN] value to set affinity Return: EnnReturn result, 0 is success Note: in second function EnnGetPreferencePresetId EnnReturn EnnGetPreferencePresetId( uint32_t * val_ptr ) Get current information for Preset ID. Parameters: val [OUT] current value of Preset ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfConfigId EnnReturn EnnGetPreferencePerfConfigId( uint32_t * val_ptr ) Get current information for PerfConfig ID. Parameters: val [OUT] current value of PerfConfig ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfMode EnnReturn EnnGetPreferencePerfMode( uint32_t * val_ptr ) Get current information for Performance Mode. Parameters: val [OUT] current value of Performance Mode Return: EnnReturn result, 0 is success function EnnGetPreferenceTimeOut EnnReturn EnnGetPreferenceTimeOut( uint32_t * val_ptr ) Get current information for Time Out. Parameters: val [OUT] current value of Time Out Return: EnnReturn result, 0 is success function EnnGetPreferencePriority EnnReturn EnnGetPreferencePriority( uint32_t * val_ptr ) Get current information for NPU Priority. Parameters: val [OUT] current value of NPU Priority Return:",
      "<2-hop>\n\ndescribes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "The ENN framework plays a crucial role in the Exynos AI Studio by providing the necessary tools and functions for converting Neural Network (NN) models into Neural Network Container (NNC) models. It includes APIs for initializing and deinitializing contexts, managing buffers, and executing models. The framework allows developers to input data into the model, execute it, and obtain outputs, thereby streamlining the process of deploying AI models on Exynos hardware.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is the role of the ENN framework in the Exynos AI Studio, and how does it facilitate the conversion of Neural Network models to Neural Network Container models?",
    "reference_contexts": [
      "<1-hop>\n\nIntroduction EHT Converter Optimizer Quantizer Simulator Model Optimization Flow ELT Model Analyzer SCVT Quantizer NPU Simulator Performance Estimator Graph Generator Dataset and Model Quick Start System Requirements How to use ENN Framework Data Type References _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; API reference ENN Framework Data Type References _NumberOfBuffersInfo _ennBuffer _ennBufferInfo ENN Framwork API Functions Context initialize / deinitialize OpenModel / CloseModel related Memory Handling Setters and Getters for model Commit Buffer Execute Models Security, preference, get meta information.. title: _ennBuffer _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; Updated on 2023-08-11 at 16:24:05 +0900 title: _ennBufferInfo _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; Updated on 2023-08-11 at 16:24:05 +0900 title: _NumberOfBuffersInfo _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; Updated on 2023-08-11 at 16:24:05 +0900 title: Commit Buffer Commit Buffer Functions Name EnnReturn EnnBufferCommit (const EnnModelId model_id, const int session_id =0) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Functions Documentation function EnnBufferCommit EnnReturn EnnBufferCommit( const EnnModelId model_id, const int session_id =0 ) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Parameters: model_id [IN] model ID from load_model Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Context initialize / deinitialize Context initialize / deinitialize Functions Name EnnReturn EnnInitialize (void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. EnnReturn EnnDeinitialize (void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Functions Documentation function EnnInitialize EnnReturn EnnInitialize( void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. Return: EnnReturn result, 0 is success function EnnDeinitialize EnnReturn EnnDeinitialize( void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Security, preference, get meta information.. Security, preference, get meta information.. Functions Name EnnReturn EnnSetPreferencePresetId (const uint32_t val) Setting Preset ID for operation performance. EnnReturn EnnSetPreferencePerfConfigId (const uint32_t val) Setting PerfConfig ID for operation performance. EnnReturn EnnSetPreferencePerfMode (const uint32_t val) Setting Performance Mode. EnnReturn EnnSetPreferenceTimeOut (const uint32_t val) Setting Preset ID for time out. EnnReturn EnnSetPreferencePriority (const uint32_t val) Setting priority value for NPU. EnnReturn EnnSetPreferenceCoreAffinity (const uint32_t val) Setting affinity to set NPU core operation. EnnReturn EnnGetPreferencePresetId (uint32_t * val_ptr) Get current information for Preset ID. EnnReturn EnnGetPreferencePerfConfigId (uint32_t * val_ptr) Get current information for PerfConfig ID. EnnReturn EnnGetPreferencePerfMode (uint32_t * val_ptr) Get current information for Performance Mode. EnnReturn EnnGetPreferenceTimeOut (uint32_t * val_ptr) Get current information for Time Out. EnnReturn EnnGetPreferencePriority (uint32_t * val_ptr) Get current information for NPU Priority. EnnReturn EnnGetPreferenceCoreAffinity (uint32_t * val_ptr) Get current information for NPU Core affinity. EnnReturn EnnGetMetaInfo (const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]) Get Meta Information. EnnReturn EnnSetExecMsgAlwaysOn () Set frequency of execution message print. Functions Documentation function EnnSetPreferencePresetId EnnReturn EnnSetPreferencePresetId( const uint32_t val ) Setting Preset ID for operation performance. Parameters: val [IN] value to set preset ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfConfigId EnnReturn EnnSetPreferencePerfConfigId( const uint32_t val ) Setting PerfConfig ID for operation performance. Parameters: val [IN] value to set PerfConfig ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfMode EnnReturn EnnSetPreferencePerfMode( const uint32_t val ) Setting Performance Mode. Parameters: val [IN] value to set Performance Mode Return: EnnReturn result, 0 is success function EnnSetPreferenceTimeOut EnnReturn EnnSetPreferenceTimeOut( const uint32_t val ) Setting Preset ID for time out. Parameters: val [IN] value to set time out Return: EnnReturn result, 0 is success Note: in second function EnnSetPreferencePriority EnnReturn EnnSetPreferencePriority( const uint32_t val ) Setting priority value for NPU. Parameters: val [IN] value to set NPU job priority Return: EnnReturn result, 0 is success function EnnSetPreferenceCoreAffinity EnnReturn EnnSetPreferenceCoreAffinity( const uint32_t val ) Setting affinity to set NPU core operation. Parameters: val [IN] value to set affinity Return: EnnReturn result, 0 is success Note: in second function EnnGetPreferencePresetId EnnReturn EnnGetPreferencePresetId( uint32_t * val_ptr ) Get current information for Preset ID. Parameters: val [OUT] current value of Preset ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfConfigId EnnReturn EnnGetPreferencePerfConfigId( uint32_t * val_ptr ) Get current information for PerfConfig ID. Parameters: val [OUT] current value of PerfConfig ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfMode EnnReturn EnnGetPreferencePerfMode( uint32_t * val_ptr ) Get current information for Performance Mode. Parameters: val [OUT] current value of Performance Mode Return: EnnReturn result, 0 is success function EnnGetPreferenceTimeOut EnnReturn EnnGetPreferenceTimeOut( uint32_t * val_ptr ) Get current information for Time Out. Parameters: val [OUT] current value of Time Out Return: EnnReturn result, 0 is success function EnnGetPreferencePriority EnnReturn EnnGetPreferencePriority( uint32_t * val_ptr ) Get current information for NPU Priority. Parameters: val [OUT] current value of NPU Priority Return:",
      "<2-hop>\n\ndescribes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "The ENN framework plays a crucial role in the Exynos AI Studio by providing the necessary tools and functions to convert Neural Network (NN) models into Neural Network Container (NNC) models. It includes a comprehensive API reference that outlines various data types and functions essential for model execution and output retrieval. The framework allows developers to initialize and deinitialize contexts, manage memory, and commit buffers for model execution. Additionally, it supports various operations and configurations that enhance performance, making it easier for developers to optimize their AI models for deployment on Exynos hardware.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is the process for converting NN models to NNC models using Exynos AI Studio, and how does the NPU enhance this process?",
    "reference_contexts": [
      "<1-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3.",
      "<2-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "The process for converting NN models to NNC models using Exynos AI Studio involves two main steps: first, the user prepares a pre-trained NN model and sets parameters for the conversion tools. Then, the user executes the tools to initiate the conversion process. The NNC format is designed to run efficiently on Samsung Exynos hardware, and when the NPU is engaged as an additional accelerator, it enhances the model's execution speed. However, it is important to note that the NPU does not support all models, which may lead to complications during the conversion process.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps involved in converting a Neural Network (NN) model to a Neural Network Container (NNC) model using Exynos AI Studio, and how does the NPU enhance the execution of these models?",
    "reference_contexts": [
      "<1-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3.",
      "<2-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "To convert a Neural Network (NN) model to a Neural Network Container (NNC) model using Exynos AI Studio, the following steps are involved: First, the user prepares a pre-trained NN model and sets parameters for the conversion tools. Then, the user executes the tools to initiate the conversion process. After the conversion is completed, the NNC Download button becomes enabled, allowing the user to download the converted NNC model file. It is important to note that the NNC format is designed to run efficiently on Samsung Exynos hardware. The NPU (Neural Processing Unit) can be engaged as an additional accelerator during execution, which enhances the performance of the model. However, it is crucial to be aware that the NPU does not support all models, and choosing the Accelerate option may lead to complications if the model is incompatible.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps involved in converting NN models to NNC models using Exynos AI Studio, and how does the NPU enhance the execution of these models?",
    "reference_contexts": [
      "<1-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3.",
      "<2-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "To convert NN models to NNC models using Exynos AI Studio, the user must follow these steps: First, prepare a pre-trained NN model. Next, set parameters for the conversion tools and execute them to initiate the conversion process. After the conversion is complete, the NNC Download button becomes enabled, allowing the user to download the converted NNC model file. The user can also view logs for any conversion failures by clicking Log Download. The NPU enhances the execution of these models by acting as an additional accelerator when the 'Accelerate' hardware type is chosen. This option allows the NPU to assist in processing, although it is important to note that not all models are supported by the NPU, which may lead to complications.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps involved in converting NN models to NNC models using Exynos AI Studio, and how does the NPU enhance this process?",
    "reference_contexts": [
      "<1-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3.",
      "<2-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "To convert NN models to NNC models using Exynos AI Studio, the user must follow these steps: First, prepare a pre-trained NN model. Next, set parameters for the conversion tools and execute them to initiate the conversion process. After the conversion is complete, the NNC Download button becomes enabled, allowing the user to download the converted NNC model file. The NPU enhances this process by acting as an additional accelerator when the 'Accelerate' hardware type is selected, although it is important to note that the NPU does not support all models, which may lead to complications.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How do you convert NN models to NNC models using Exynos AI Studio, and what is the significance of the NNC format?",
    "reference_contexts": [
      "<1-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3.",
      "<2-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "To convert NN models to NNC models using Exynos AI Studio, the user must first prepare a pre-trained NN model and set parameters for the conversion tools. The conversion process is initiated by selecting 'Convert' to start the conversion, after which the NNC Download button becomes enabled to obtain the converted NNC model file. The NNC format is significant because it allows neural network models to run efficiently on Samsung Exynos hardware, optimizing performance during inference.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What functions are available in the ENN framework for managing model execution and obtaining meta information?",
    "reference_contexts": [
      "<1-hop>\n\nIntroduction EHT Converter Optimizer Quantizer Simulator Model Optimization Flow ELT Model Analyzer SCVT Quantizer NPU Simulator Performance Estimator Graph Generator Dataset and Model Quick Start System Requirements How to use ENN Framework Data Type References _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; API reference ENN Framework Data Type References _NumberOfBuffersInfo _ennBuffer _ennBufferInfo ENN Framwork API Functions Context initialize / deinitialize OpenModel / CloseModel related Memory Handling Setters and Getters for model Commit Buffer Execute Models Security, preference, get meta information.. title: _ennBuffer _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; Updated on 2023-08-11 at 16:24:05 +0900 title: _ennBufferInfo _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; Updated on 2023-08-11 at 16:24:05 +0900 title: _NumberOfBuffersInfo _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; Updated on 2023-08-11 at 16:24:05 +0900 title: Commit Buffer Commit Buffer Functions Name EnnReturn EnnBufferCommit (const EnnModelId model_id, const int session_id =0) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Functions Documentation function EnnBufferCommit EnnReturn EnnBufferCommit( const EnnModelId model_id, const int session_id =0 ) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Parameters: model_id [IN] model ID from load_model Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Context initialize / deinitialize Context initialize / deinitialize Functions Name EnnReturn EnnInitialize (void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. EnnReturn EnnDeinitialize (void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Functions Documentation function EnnInitialize EnnReturn EnnInitialize( void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. Return: EnnReturn result, 0 is success function EnnDeinitialize EnnReturn EnnDeinitialize( void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Security, preference, get meta information.. Security, preference, get meta information.. Functions Name EnnReturn EnnSetPreferencePresetId (const uint32_t val) Setting Preset ID for operation performance. EnnReturn EnnSetPreferencePerfConfigId (const uint32_t val) Setting PerfConfig ID for operation performance. EnnReturn EnnSetPreferencePerfMode (const uint32_t val) Setting Performance Mode. EnnReturn EnnSetPreferenceTimeOut (const uint32_t val) Setting Preset ID for time out. EnnReturn EnnSetPreferencePriority (const uint32_t val) Setting priority value for NPU. EnnReturn EnnSetPreferenceCoreAffinity (const uint32_t val) Setting affinity to set NPU core operation. EnnReturn EnnGetPreferencePresetId (uint32_t * val_ptr) Get current information for Preset ID. EnnReturn EnnGetPreferencePerfConfigId (uint32_t * val_ptr) Get current information for PerfConfig ID. EnnReturn EnnGetPreferencePerfMode (uint32_t * val_ptr) Get current information for Performance Mode. EnnReturn EnnGetPreferenceTimeOut (uint32_t * val_ptr) Get current information for Time Out. EnnReturn EnnGetPreferencePriority (uint32_t * val_ptr) Get current information for NPU Priority. EnnReturn EnnGetPreferenceCoreAffinity (uint32_t * val_ptr) Get current information for NPU Core affinity. EnnReturn EnnGetMetaInfo (const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]) Get Meta Information. EnnReturn EnnSetExecMsgAlwaysOn () Set frequency of execution message print. Functions Documentation function EnnSetPreferencePresetId EnnReturn EnnSetPreferencePresetId( const uint32_t val ) Setting Preset ID for operation performance. Parameters: val [IN] value to set preset ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfConfigId EnnReturn EnnSetPreferencePerfConfigId( const uint32_t val ) Setting PerfConfig ID for operation performance. Parameters: val [IN] value to set PerfConfig ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfMode EnnReturn EnnSetPreferencePerfMode( const uint32_t val ) Setting Performance Mode. Parameters: val [IN] value to set Performance Mode Return: EnnReturn result, 0 is success function EnnSetPreferenceTimeOut EnnReturn EnnSetPreferenceTimeOut( const uint32_t val ) Setting Preset ID for time out. Parameters: val [IN] value to set time out Return: EnnReturn result, 0 is success Note: in second function EnnSetPreferencePriority EnnReturn EnnSetPreferencePriority( const uint32_t val ) Setting priority value for NPU. Parameters: val [IN] value to set NPU job priority Return: EnnReturn result, 0 is success function EnnSetPreferenceCoreAffinity EnnReturn EnnSetPreferenceCoreAffinity( const uint32_t val ) Setting affinity to set NPU core operation. Parameters: val [IN] value to set affinity Return: EnnReturn result, 0 is success Note: in second function EnnGetPreferencePresetId EnnReturn EnnGetPreferencePresetId( uint32_t * val_ptr ) Get current information for Preset ID. Parameters: val [OUT] current value of Preset ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfConfigId EnnReturn EnnGetPreferencePerfConfigId( uint32_t * val_ptr ) Get current information for PerfConfig ID. Parameters: val [OUT] current value of PerfConfig ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfMode EnnReturn EnnGetPreferencePerfMode( uint32_t * val_ptr ) Get current information for Performance Mode. Parameters: val [OUT] current value of Performance Mode Return: EnnReturn result, 0 is success function EnnGetPreferenceTimeOut EnnReturn EnnGetPreferenceTimeOut( uint32_t * val_ptr ) Get current information for Time Out. Parameters: val [OUT] current value of Time Out Return: EnnReturn result, 0 is success function EnnGetPreferencePriority EnnReturn EnnGetPreferencePriority( uint32_t * val_ptr ) Get current information for NPU Priority. Parameters: val [OUT] current value of NPU Priority Return:",
      "<2-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "The ENN framework provides several functions for managing model execution and obtaining meta information. For model execution, the functions include 'EnnInitialize' to initialize the framework, 'EnnDeinitialize' to deinitialize it, and 'EnnBufferCommit' to send buffer sets to the service core. To obtain meta information, the function 'EnnGetMetaInfo' retrieves information about the loaded model and framework, while 'EnnGetPreferenceCoreAffinity' provides current information regarding NPU core affinity.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps involved in initializing the ENN framework and how does it relate to the performance settings for NPU core affinity?",
    "reference_contexts": [
      "<1-hop>\n\nIntroduction EHT Converter Optimizer Quantizer Simulator Model Optimization Flow ELT Model Analyzer SCVT Quantizer NPU Simulator Performance Estimator Graph Generator Dataset and Model Quick Start System Requirements How to use ENN Framework Data Type References _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; API reference ENN Framework Data Type References _NumberOfBuffersInfo _ennBuffer _ennBufferInfo ENN Framwork API Functions Context initialize / deinitialize OpenModel / CloseModel related Memory Handling Setters and Getters for model Commit Buffer Execute Models Security, preference, get meta information.. title: _ennBuffer _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; Updated on 2023-08-11 at 16:24:05 +0900 title: _ennBufferInfo _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; Updated on 2023-08-11 at 16:24:05 +0900 title: _NumberOfBuffersInfo _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; Updated on 2023-08-11 at 16:24:05 +0900 title: Commit Buffer Commit Buffer Functions Name EnnReturn EnnBufferCommit (const EnnModelId model_id, const int session_id =0) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Functions Documentation function EnnBufferCommit EnnReturn EnnBufferCommit( const EnnModelId model_id, const int session_id =0 ) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Parameters: model_id [IN] model ID from load_model Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Context initialize / deinitialize Context initialize / deinitialize Functions Name EnnReturn EnnInitialize (void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. EnnReturn EnnDeinitialize (void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Functions Documentation function EnnInitialize EnnReturn EnnInitialize( void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. Return: EnnReturn result, 0 is success function EnnDeinitialize EnnReturn EnnDeinitialize( void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Security, preference, get meta information.. Security, preference, get meta information.. Functions Name EnnReturn EnnSetPreferencePresetId (const uint32_t val) Setting Preset ID for operation performance. EnnReturn EnnSetPreferencePerfConfigId (const uint32_t val) Setting PerfConfig ID for operation performance. EnnReturn EnnSetPreferencePerfMode (const uint32_t val) Setting Performance Mode. EnnReturn EnnSetPreferenceTimeOut (const uint32_t val) Setting Preset ID for time out. EnnReturn EnnSetPreferencePriority (const uint32_t val) Setting priority value for NPU. EnnReturn EnnSetPreferenceCoreAffinity (const uint32_t val) Setting affinity to set NPU core operation. EnnReturn EnnGetPreferencePresetId (uint32_t * val_ptr) Get current information for Preset ID. EnnReturn EnnGetPreferencePerfConfigId (uint32_t * val_ptr) Get current information for PerfConfig ID. EnnReturn EnnGetPreferencePerfMode (uint32_t * val_ptr) Get current information for Performance Mode. EnnReturn EnnGetPreferenceTimeOut (uint32_t * val_ptr) Get current information for Time Out. EnnReturn EnnGetPreferencePriority (uint32_t * val_ptr) Get current information for NPU Priority. EnnReturn EnnGetPreferenceCoreAffinity (uint32_t * val_ptr) Get current information for NPU Core affinity. EnnReturn EnnGetMetaInfo (const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]) Get Meta Information. EnnReturn EnnSetExecMsgAlwaysOn () Set frequency of execution message print. Functions Documentation function EnnSetPreferencePresetId EnnReturn EnnSetPreferencePresetId( const uint32_t val ) Setting Preset ID for operation performance. Parameters: val [IN] value to set preset ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfConfigId EnnReturn EnnSetPreferencePerfConfigId( const uint32_t val ) Setting PerfConfig ID for operation performance. Parameters: val [IN] value to set PerfConfig ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfMode EnnReturn EnnSetPreferencePerfMode( const uint32_t val ) Setting Performance Mode. Parameters: val [IN] value to set Performance Mode Return: EnnReturn result, 0 is success function EnnSetPreferenceTimeOut EnnReturn EnnSetPreferenceTimeOut( const uint32_t val ) Setting Preset ID for time out. Parameters: val [IN] value to set time out Return: EnnReturn result, 0 is success Note: in second function EnnSetPreferencePriority EnnReturn EnnSetPreferencePriority( const uint32_t val ) Setting priority value for NPU. Parameters: val [IN] value to set NPU job priority Return: EnnReturn result, 0 is success function EnnSetPreferenceCoreAffinity EnnReturn EnnSetPreferenceCoreAffinity( const uint32_t val ) Setting affinity to set NPU core operation. Parameters: val [IN] value to set affinity Return: EnnReturn result, 0 is success Note: in second function EnnGetPreferencePresetId EnnReturn EnnGetPreferencePresetId( uint32_t * val_ptr ) Get current information for Preset ID. Parameters: val [OUT] current value of Preset ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfConfigId EnnReturn EnnGetPreferencePerfConfigId( uint32_t * val_ptr ) Get current information for PerfConfig ID. Parameters: val [OUT] current value of PerfConfig ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfMode EnnReturn EnnGetPreferencePerfMode( uint32_t * val_ptr ) Get current information for Performance Mode. Parameters: val [OUT] current value of Performance Mode Return: EnnReturn result, 0 is success function EnnGetPreferenceTimeOut EnnReturn EnnGetPreferenceTimeOut( uint32_t * val_ptr ) Get current information for Time Out. Parameters: val [OUT] current value of Time Out Return: EnnReturn result, 0 is success function EnnGetPreferencePriority EnnReturn EnnGetPreferencePriority( uint32_t * val_ptr ) Get current information for NPU Priority. Parameters: val [OUT] current value of NPU Priority Return:",
      "<2-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "To initialize the ENN framework, the function EnnInitialize() is called, which generates context in the caller's process. After initialization, the user can set various performance settings, including NPU core affinity, using the function EnnSetPreferenceCoreAffinity(). This function retrieves the current information for NPU core affinity, allowing the user to optimize the performance of the model execution within the ENN framework.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps involved in using the ENN framework for model execution and how does it relate to the dataset preparation guidelines?",
    "reference_contexts": [
      "<1-hop>\n\nIntroduction EHT Converter Optimizer Quantizer Simulator Model Optimization Flow ELT Model Analyzer SCVT Quantizer NPU Simulator Performance Estimator Graph Generator Dataset and Model Quick Start System Requirements How to use ENN Framework Data Type References _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; API reference ENN Framework Data Type References _NumberOfBuffersInfo _ennBuffer _ennBufferInfo ENN Framwork API Functions Context initialize / deinitialize OpenModel / CloseModel related Memory Handling Setters and Getters for model Commit Buffer Execute Models Security, preference, get meta information.. title: _ennBuffer _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; Updated on 2023-08-11 at 16:24:05 +0900 title: _ennBufferInfo _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; Updated on 2023-08-11 at 16:24:05 +0900 title: _NumberOfBuffersInfo _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; Updated on 2023-08-11 at 16:24:05 +0900 title: Commit Buffer Commit Buffer Functions Name EnnReturn EnnBufferCommit (const EnnModelId model_id, const int session_id =0) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Functions Documentation function EnnBufferCommit EnnReturn EnnBufferCommit( const EnnModelId model_id, const int session_id =0 ) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Parameters: model_id [IN] model ID from load_model Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Context initialize / deinitialize Context initialize / deinitialize Functions Name EnnReturn EnnInitialize (void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. EnnReturn EnnDeinitialize (void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Functions Documentation function EnnInitialize EnnReturn EnnInitialize( void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. Return: EnnReturn result, 0 is success function EnnDeinitialize EnnReturn EnnDeinitialize( void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Security, preference, get meta information.. Security, preference, get meta information.. Functions Name EnnReturn EnnSetPreferencePresetId (const uint32_t val) Setting Preset ID for operation performance. EnnReturn EnnSetPreferencePerfConfigId (const uint32_t val) Setting PerfConfig ID for operation performance. EnnReturn EnnSetPreferencePerfMode (const uint32_t val) Setting Performance Mode. EnnReturn EnnSetPreferenceTimeOut (const uint32_t val) Setting Preset ID for time out. EnnReturn EnnSetPreferencePriority (const uint32_t val) Setting priority value for NPU. EnnReturn EnnSetPreferenceCoreAffinity (const uint32_t val) Setting affinity to set NPU core operation. EnnReturn EnnGetPreferencePresetId (uint32_t * val_ptr) Get current information for Preset ID. EnnReturn EnnGetPreferencePerfConfigId (uint32_t * val_ptr) Get current information for PerfConfig ID. EnnReturn EnnGetPreferencePerfMode (uint32_t * val_ptr) Get current information for Performance Mode. EnnReturn EnnGetPreferenceTimeOut (uint32_t * val_ptr) Get current information for Time Out. EnnReturn EnnGetPreferencePriority (uint32_t * val_ptr) Get current information for NPU Priority. EnnReturn EnnGetPreferenceCoreAffinity (uint32_t * val_ptr) Get current information for NPU Core affinity. EnnReturn EnnGetMetaInfo (const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]) Get Meta Information. EnnReturn EnnSetExecMsgAlwaysOn () Set frequency of execution message print. Functions Documentation function EnnSetPreferencePresetId EnnReturn EnnSetPreferencePresetId( const uint32_t val ) Setting Preset ID for operation performance. Parameters: val [IN] value to set preset ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfConfigId EnnReturn EnnSetPreferencePerfConfigId( const uint32_t val ) Setting PerfConfig ID for operation performance. Parameters: val [IN] value to set PerfConfig ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfMode EnnReturn EnnSetPreferencePerfMode( const uint32_t val ) Setting Performance Mode. Parameters: val [IN] value to set Performance Mode Return: EnnReturn result, 0 is success function EnnSetPreferenceTimeOut EnnReturn EnnSetPreferenceTimeOut( const uint32_t val ) Setting Preset ID for time out. Parameters: val [IN] value to set time out Return: EnnReturn result, 0 is success Note: in second function EnnSetPreferencePriority EnnReturn EnnSetPreferencePriority( const uint32_t val ) Setting priority value for NPU. Parameters: val [IN] value to set NPU job priority Return: EnnReturn result, 0 is success function EnnSetPreferenceCoreAffinity EnnReturn EnnSetPreferenceCoreAffinity( const uint32_t val ) Setting affinity to set NPU core operation. Parameters: val [IN] value to set affinity Return: EnnReturn result, 0 is success Note: in second function EnnGetPreferencePresetId EnnReturn EnnGetPreferencePresetId( uint32_t * val_ptr ) Get current information for Preset ID. Parameters: val [OUT] current value of Preset ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfConfigId EnnReturn EnnGetPreferencePerfConfigId( uint32_t * val_ptr ) Get current information for PerfConfig ID. Parameters: val [OUT] current value of PerfConfig ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfMode EnnReturn EnnGetPreferencePerfMode( uint32_t * val_ptr ) Get current information for Performance Mode. Parameters: val [OUT] current value of Performance Mode Return: EnnReturn result, 0 is success function EnnGetPreferenceTimeOut EnnReturn EnnGetPreferenceTimeOut( uint32_t * val_ptr ) Get current information for Time Out. Parameters: val [OUT] current value of Time Out Return: EnnReturn result, 0 is success function EnnGetPreferencePriority EnnReturn EnnGetPreferencePriority( uint32_t * val_ptr ) Get current information for NPU Priority. Parameters: val [OUT] current value of NPU Priority Return:",
      "<2-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "Using the ENN framework for model execution involves several steps. First, the user must initialize the ENN framework. Next, the converted model is loaded into the ENN framework. After that, all necessary buffers for the model are allocated and committed. The user then copies input data to the input buffers and executes the model on the ENN framework. Finally, the user can use data from the output buffers. To execute the model multiple times, this process is repeated. After execution, the user must uncommit and release the buffers allocated to the model, unload the model, and deinitialize the ENN framework. This process is crucial as it ensures that the model runs efficiently on Samsung Exynos hardware. Additionally, dataset preparation is essential, as the dataset must be in the correct format, specifically using the .h5 file extension, and the internal data must be in float32 format to match the model's input requirements.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps involved in initializing the ENN framework and how does it relate to the performance settings such as core affinity and preset ID?",
    "reference_contexts": [
      "<1-hop>\n\nIntroduction EHT Converter Optimizer Quantizer Simulator Model Optimization Flow ELT Model Analyzer SCVT Quantizer NPU Simulator Performance Estimator Graph Generator Dataset and Model Quick Start System Requirements How to use ENN Framework Data Type References _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; API reference ENN Framework Data Type References _NumberOfBuffersInfo _ennBuffer _ennBufferInfo ENN Framwork API Functions Context initialize / deinitialize OpenModel / CloseModel related Memory Handling Setters and Getters for model Commit Buffer Execute Models Security, preference, get meta information.. title: _ennBuffer _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; Updated on 2023-08-11 at 16:24:05 +0900 title: _ennBufferInfo _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; Updated on 2023-08-11 at 16:24:05 +0900 title: _NumberOfBuffersInfo _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; Updated on 2023-08-11 at 16:24:05 +0900 title: Commit Buffer Commit Buffer Functions Name EnnReturn EnnBufferCommit (const EnnModelId model_id, const int session_id =0) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Functions Documentation function EnnBufferCommit EnnReturn EnnBufferCommit( const EnnModelId model_id, const int session_id =0 ) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Parameters: model_id [IN] model ID from load_model Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Context initialize / deinitialize Context initialize / deinitialize Functions Name EnnReturn EnnInitialize (void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. EnnReturn EnnDeinitialize (void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Functions Documentation function EnnInitialize EnnReturn EnnInitialize( void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. Return: EnnReturn result, 0 is success function EnnDeinitialize EnnReturn EnnDeinitialize( void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Security, preference, get meta information.. Security, preference, get meta information.. Functions Name EnnReturn EnnSetPreferencePresetId (const uint32_t val) Setting Preset ID for operation performance. EnnReturn EnnSetPreferencePerfConfigId (const uint32_t val) Setting PerfConfig ID for operation performance. EnnReturn EnnSetPreferencePerfMode (const uint32_t val) Setting Performance Mode. EnnReturn EnnSetPreferenceTimeOut (const uint32_t val) Setting Preset ID for time out. EnnReturn EnnSetPreferencePriority (const uint32_t val) Setting priority value for NPU. EnnReturn EnnSetPreferenceCoreAffinity (const uint32_t val) Setting affinity to set NPU core operation. EnnReturn EnnGetPreferencePresetId (uint32_t * val_ptr) Get current information for Preset ID. EnnReturn EnnGetPreferencePerfConfigId (uint32_t * val_ptr) Get current information for PerfConfig ID. EnnReturn EnnGetPreferencePerfMode (uint32_t * val_ptr) Get current information for Performance Mode. EnnReturn EnnGetPreferenceTimeOut (uint32_t * val_ptr) Get current information for Time Out. EnnReturn EnnGetPreferencePriority (uint32_t * val_ptr) Get current information for NPU Priority. EnnReturn EnnGetPreferenceCoreAffinity (uint32_t * val_ptr) Get current information for NPU Core affinity. EnnReturn EnnGetMetaInfo (const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]) Get Meta Information. EnnReturn EnnSetExecMsgAlwaysOn () Set frequency of execution message print. Functions Documentation function EnnSetPreferencePresetId EnnReturn EnnSetPreferencePresetId( const uint32_t val ) Setting Preset ID for operation performance. Parameters: val [IN] value to set preset ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfConfigId EnnReturn EnnSetPreferencePerfConfigId( const uint32_t val ) Setting PerfConfig ID for operation performance. Parameters: val [IN] value to set PerfConfig ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfMode EnnReturn EnnSetPreferencePerfMode( const uint32_t val ) Setting Performance Mode. Parameters: val [IN] value to set Performance Mode Return: EnnReturn result, 0 is success function EnnSetPreferenceTimeOut EnnReturn EnnSetPreferenceTimeOut( const uint32_t val ) Setting Preset ID for time out. Parameters: val [IN] value to set time out Return: EnnReturn result, 0 is success Note: in second function EnnSetPreferencePriority EnnReturn EnnSetPreferencePriority( const uint32_t val ) Setting priority value for NPU. Parameters: val [IN] value to set NPU job priority Return: EnnReturn result, 0 is success function EnnSetPreferenceCoreAffinity EnnReturn EnnSetPreferenceCoreAffinity( const uint32_t val ) Setting affinity to set NPU core operation. Parameters: val [IN] value to set affinity Return: EnnReturn result, 0 is success Note: in second function EnnGetPreferencePresetId EnnReturn EnnGetPreferencePresetId( uint32_t * val_ptr ) Get current information for Preset ID. Parameters: val [OUT] current value of Preset ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfConfigId EnnReturn EnnGetPreferencePerfConfigId( uint32_t * val_ptr ) Get current information for PerfConfig ID. Parameters: val [OUT] current value of PerfConfig ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfMode EnnReturn EnnGetPreferencePerfMode( uint32_t * val_ptr ) Get current information for Performance Mode. Parameters: val [OUT] current value of Performance Mode Return: EnnReturn result, 0 is success function EnnGetPreferenceTimeOut EnnReturn EnnGetPreferenceTimeOut( uint32_t * val_ptr ) Get current information for Time Out. Parameters: val [OUT] current value of Time Out Return: EnnReturn result, 0 is success function EnnGetPreferencePriority EnnReturn EnnGetPreferencePriority( uint32_t * val_ptr ) Get current information for NPU Priority. Parameters: val [OUT] current value of NPU Priority Return:",
      "<2-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "To initialize the ENN framework, the function EnnInitialize() is called, which generates a context in the caller's process. This is followed by loading the converted model into the ENN framework and allocating necessary buffers. Performance settings such as core affinity can be adjusted using the function EnnSetPreferenceCoreAffinity(), which sets the affinity for NPU core operation. Additionally, the preset ID for operation performance can be set using EnnSetPreferencePresetId(). These settings are crucial for optimizing the execution of models within the ENN framework.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps involved in initializing the ENN framework and how does it relate to the execution of models in Exynos AI Studio?",
    "reference_contexts": [
      "<1-hop>\n\nIntroduction EHT Converter Optimizer Quantizer Simulator Model Optimization Flow ELT Model Analyzer SCVT Quantizer NPU Simulator Performance Estimator Graph Generator Dataset and Model Quick Start System Requirements How to use ENN Framework Data Type References _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; API reference ENN Framework Data Type References _NumberOfBuffersInfo _ennBuffer _ennBufferInfo ENN Framwork API Functions Context initialize / deinitialize OpenModel / CloseModel related Memory Handling Setters and Getters for model Commit Buffer Execute Models Security, preference, get meta information.. title: _ennBuffer _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; Updated on 2023-08-11 at 16:24:05 +0900 title: _ennBufferInfo _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; Updated on 2023-08-11 at 16:24:05 +0900 title: _NumberOfBuffersInfo _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; Updated on 2023-08-11 at 16:24:05 +0900 title: Commit Buffer Commit Buffer Functions Name EnnReturn EnnBufferCommit (const EnnModelId model_id, const int session_id =0) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Functions Documentation function EnnBufferCommit EnnReturn EnnBufferCommit( const EnnModelId model_id, const int session_id =0 ) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Parameters: model_id [IN] model ID from load_model Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Context initialize / deinitialize Context initialize / deinitialize Functions Name EnnReturn EnnInitialize (void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. EnnReturn EnnDeinitialize (void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Functions Documentation function EnnInitialize EnnReturn EnnInitialize( void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. Return: EnnReturn result, 0 is success function EnnDeinitialize EnnReturn EnnDeinitialize( void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Security, preference, get meta information.. Security, preference, get meta information.. Functions Name EnnReturn EnnSetPreferencePresetId (const uint32_t val) Setting Preset ID for operation performance. EnnReturn EnnSetPreferencePerfConfigId (const uint32_t val) Setting PerfConfig ID for operation performance. EnnReturn EnnSetPreferencePerfMode (const uint32_t val) Setting Performance Mode. EnnReturn EnnSetPreferenceTimeOut (const uint32_t val) Setting Preset ID for time out. EnnReturn EnnSetPreferencePriority (const uint32_t val) Setting priority value for NPU. EnnReturn EnnSetPreferenceCoreAffinity (const uint32_t val) Setting affinity to set NPU core operation. EnnReturn EnnGetPreferencePresetId (uint32_t * val_ptr) Get current information for Preset ID. EnnReturn EnnGetPreferencePerfConfigId (uint32_t * val_ptr) Get current information for PerfConfig ID. EnnReturn EnnGetPreferencePerfMode (uint32_t * val_ptr) Get current information for Performance Mode. EnnReturn EnnGetPreferenceTimeOut (uint32_t * val_ptr) Get current information for Time Out. EnnReturn EnnGetPreferencePriority (uint32_t * val_ptr) Get current information for NPU Priority. EnnReturn EnnGetPreferenceCoreAffinity (uint32_t * val_ptr) Get current information for NPU Core affinity. EnnReturn EnnGetMetaInfo (const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]) Get Meta Information. EnnReturn EnnSetExecMsgAlwaysOn () Set frequency of execution message print. Functions Documentation function EnnSetPreferencePresetId EnnReturn EnnSetPreferencePresetId( const uint32_t val ) Setting Preset ID for operation performance. Parameters: val [IN] value to set preset ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfConfigId EnnReturn EnnSetPreferencePerfConfigId( const uint32_t val ) Setting PerfConfig ID for operation performance. Parameters: val [IN] value to set PerfConfig ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfMode EnnReturn EnnSetPreferencePerfMode( const uint32_t val ) Setting Performance Mode. Parameters: val [IN] value to set Performance Mode Return: EnnReturn result, 0 is success function EnnSetPreferenceTimeOut EnnReturn EnnSetPreferenceTimeOut( const uint32_t val ) Setting Preset ID for time out. Parameters: val [IN] value to set time out Return: EnnReturn result, 0 is success Note: in second function EnnSetPreferencePriority EnnReturn EnnSetPreferencePriority( const uint32_t val ) Setting priority value for NPU. Parameters: val [IN] value to set NPU job priority Return: EnnReturn result, 0 is success function EnnSetPreferenceCoreAffinity EnnReturn EnnSetPreferenceCoreAffinity( const uint32_t val ) Setting affinity to set NPU core operation. Parameters: val [IN] value to set affinity Return: EnnReturn result, 0 is success Note: in second function EnnGetPreferencePresetId EnnReturn EnnGetPreferencePresetId( uint32_t * val_ptr ) Get current information for Preset ID. Parameters: val [OUT] current value of Preset ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfConfigId EnnReturn EnnGetPreferencePerfConfigId( uint32_t * val_ptr ) Get current information for PerfConfig ID. Parameters: val [OUT] current value of PerfConfig ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfMode EnnReturn EnnGetPreferencePerfMode( uint32_t * val_ptr ) Get current information for Performance Mode. Parameters: val [OUT] current value of Performance Mode Return: EnnReturn result, 0 is success function EnnGetPreferenceTimeOut EnnReturn EnnGetPreferenceTimeOut( uint32_t * val_ptr ) Get current information for Time Out. Parameters: val [OUT] current value of Time Out Return: EnnReturn result, 0 is success function EnnGetPreferencePriority EnnReturn EnnGetPreferencePriority( uint32_t * val_ptr ) Get current information for NPU Priority. Parameters: val [OUT] current value of NPU Priority Return:",
      "<2-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "To initialize the ENN framework, the function EnnInitialize() is called, which generates context in the caller's process. This is crucial as it sets up the environment for executing models. After initialization, the user can load the converted model into the ENN framework, allocate and commit all necessary buffers for the model, and then proceed to copy input data to input buffers. The execution of the model is performed by calling the appropriate functions within the ENN framework, allowing for efficient inference on Samsung Exynos hardware.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How do you convert TensorFlow Lite models to NNC models using Exynos AI Studio?",
    "reference_contexts": [
      "<1-hop>\n\ndescribes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |",
      "<2-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "To convert TensorFlow Lite models to NNC models using Exynos AI Studio, you first need to prepare a pre-trained NN model in ONNX format. Then, set the parameters for the conversion tools provided by Exynos AI Studio. After that, execute the tools for conversion. The converted NNC model can then be executed for inference using the ENN framework, which involves initializing the framework, loading the model, and allocating necessary buffers.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps involved in converting TensorFlow Lite models to NNC models using Exynos AI Studio, and what are the requirements for the input dataset?",
    "reference_contexts": [
      "<1-hop>\n\ndescribes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |",
      "<2-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "To convert TensorFlow Lite models to NNC models using Exynos AI Studio, the following steps are involved: First, prepare a pre-trained neural network (NN) model. Next, set parameters for the conversion tools provided by Exynos AI Studio. Finally, execute the tools for conversion. The input dataset must be in .h5 file format, with keys matching the input names of the model for proper mapping. Additionally, the internal data within the .h5 file must be in float32 format, and the .h5 file should be placed directly under the DATA folder generated after executing the 'enntools init' command in the workspace.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps involved in using the ENN framework for executing a model after converting it to NNC format using Exynos AI Studio?",
    "reference_contexts": [
      "<1-hop>\n\ndescribes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |",
      "<2-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "To use the ENN framework for executing a model after converting it to NNC format using Exynos AI Studio, follow these steps: 1. Initialize the ENN framework. 2. Load the converted model into the ENN framework. 3. Allocate and commit all necessary buffers for the model. 4. Copy input data to the input buffers. 5. Execute the model on the ENN framework. 6. Use data from the output buffers. To execute the model multiple times, repeat the process from step 4. Finally, perform the following cleanup steps: 1. Uncommit and release buffers allocated to the model. 2. Unload the model. 3. Deinitialize the ENN framework.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the key differences in the configuration options for the Gen-5 and Gen-5a devices in the context of using enntools for model optimization and conversion?",
    "reference_contexts": [
      "<1-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :",
      "<2-hop>\n\nstr : debug str for compiler - dequant_type : str : dequantiztion type - device : str : System on Chip type [Gen-3, Gen-3b, Gen-4, Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - enable_ofm_reuse : bool : Enable the reuse of OFM region for IMFM. - enable_stm : bool : Generate compile log including L1 tiling information - flc : bool : Enable featuremap lossless compression - fp16_swwa : bool : Enable NPU fp16 workaround with psum_init - input_conversion : str : Add a Tensor2Cell format converter node at start of network - mi : bool : multiple input compile - mo : bool : multiple output compile - multi_ncp : bool : generate multi-ncp(ucgo) custom op - multi_vc : bool : Introduce Multi-VC for OFM, IFM, and weight transfer - multicore : bool : Enable NPU multicore - optimization : str : Optimization choice [O1, O2, O3] - output_conversion : str : Add a Tensor2Cell format converter node at end of network - packed_ucgo : bool : true: generate one ucgo in nnc, false: generate multiple ucgos in nnc if there are multiple independent dsp OPs - preemption : bool : Setting priority of NNC while compiling - quant_type : str : quantiztion type - sync_npu_dsp : bool : simulator : dict - data_format : str : Indicate the position of channel of input [channel_first, channel_last] - use_randomdb : bool : Use randomdb to forward, just support single input - userdb : str : Simulation data set path (default path is {workspace}/DATA/data.txt) perf_estimator : dict - O2_enable : bool : O2 optimization (true or false) - O2_fm_forwarding : bool : feature-map forwarding (true or false) - SEG : bool : Set true if input model is Deeplab V3+ - SSD : bool : Set true if input model is SSD detection - bit_width_factor_FM : int : Select feature map bit width factor (1 or 2) - bit_width_factor_FP16 : bool : Set bit width factor as floating point (true or false) - bit_width_factor_weight : int : Select weight bit width factor (1 or 2) - core_num : int : 1 for single core, 2 for instance-1 - device : str : Select device type [Gen-3, Gen-4, Gen-5, Gen-5a] - json_report : bool : Enable report json format - nq_fold : bool : Speed-up by folding normalization parameters for input data (true: model input dtype is uint8, false: model input dtype is float32) profiler : dict - iter : int : This decides how many time the model inference will be processed. - mode : str : 1. Low power mode : Mode for low power consumption and low performance. 2. Balanced mode : Mode for medium power consumption and medium performance. 3. Performance mode : Mode for high power consumption and high performance. 4. Boost mode : Mode for very high power consumption and very high performance. [lowpower, balanced, performance, boost] - target : str : profiling target. [model, system] - test_type : str : ENN running mode [lib, service] - tv_threshold : float : The value is used for tolerance threshold of output match verification. - bitmatch_test : bool : if set true, visual profiler will compile nnc first - core_num : str : The number of NPU core. [single, multiple] - device : str : Target device. [Gen-3, Gen-4, Gen-5, Gen-5a, Gen-6, Gen-6npu, Gen-7] - device_id : str : id of the device connected to the server or PC running the enntools docker - remote_ssh_config_path : str : remote testing ssh config path (default path is {workspace}/DATA/remote_ssh_config.yaml) - ssh_bool : str : Connect to the device through ssh. [SSH_FALSE, SSH_TRUE] eht yaml file The optimization file contains optimization information. It allows you to set the locations of input and output models and detailed configurations for each module. Optimization is performed differently depending on the model type. Detailed explanation for eht yaml file model_type : string Specify the type of the input model. Optimization is performed differently depending on the model type, with details as follows CV LVM LLM quantizer : dict precision_weight : str : precision of weight(ex. int8, int16, fp16) precision_activation : str : precision of activation(ex. int8, int16, fp16) mpq_operator_dict : dict : When performing mixed precision quantization, input the operators and precision to be quantized with a precision different from the values specified in precision_weight and precision_activation above. alpha : float : smoothquant migration strength calibration_data_path : string : The path to the representative data to be used for calibration calibration_args : dict : Arguments for processing calibration samples : int : How many calibration data samples to use seed : int : A value set as a seed for random selection add_dummy_conv: bool : Whether apply the dummy conv algorithm input_dtype: dict : Input data type for the quantized model output_dtype: dict : Output data type for the quantized model simulator : dict metric : string: The metric to be used for measurement threshold : float : The threshold value of the metric that determines agreement / disagreement input_data_path : string: The path to the dataset for model inference optimizer : dict skip_4_dim_conversion : bool : If true, it does not convert to 4 dimensions before optimization; if false, it proceeds with the conversion. overwrite_input_shapes : dict : Enter the input shape for models with undefined input shapes. custom_template_path : dict: Enter the templates to apply for optimization. Use the path of the Python file containing the template as the key, and the template within it as the value. enntools conversion When performing conversion, users can choose between using eht and elt depending on the mode. The detailed workflow is illustrated in the figure below. bash enntools conversion Introduction to Exynos AI Studio Exynos AI Studio is a software that provides optimization and conversion techniques for neural networks models. This software takes models as input and offers functionalities such as quantization, conversion, optimization, compilation to generate NNC models. System overview diagram Exynos AI Studio Documentation Exynos AI Studio References Exynos AI Studio Developer Guide This guide"
    ],
    "reference": "The configuration options for Gen-5 and Gen-5a devices in the context of using enntools for model optimization and conversion include specifying the System on Chip type in the elt yaml file, where both devices are listed as options. The device parameter can be set to either Gen-5 or Gen-5a, allowing users to tailor the optimization process based on the specific capabilities of each device. Additionally, the optimization strategies may differ based on the device's architecture, which can affect parameters such as quantization type and performance settings during the model conversion process.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the differences in device support between Gen-4 and Gen-6 in the context of using enntools for model optimization?",
    "reference_contexts": [
      "<1-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :",
      "<2-hop>\n\nstr : debug str for compiler - dequant_type : str : dequantiztion type - device : str : System on Chip type [Gen-3, Gen-3b, Gen-4, Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - enable_ofm_reuse : bool : Enable the reuse of OFM region for IMFM. - enable_stm : bool : Generate compile log including L1 tiling information - flc : bool : Enable featuremap lossless compression - fp16_swwa : bool : Enable NPU fp16 workaround with psum_init - input_conversion : str : Add a Tensor2Cell format converter node at start of network - mi : bool : multiple input compile - mo : bool : multiple output compile - multi_ncp : bool : generate multi-ncp(ucgo) custom op - multi_vc : bool : Introduce Multi-VC for OFM, IFM, and weight transfer - multicore : bool : Enable NPU multicore - optimization : str : Optimization choice [O1, O2, O3] - output_conversion : str : Add a Tensor2Cell format converter node at end of network - packed_ucgo : bool : true: generate one ucgo in nnc, false: generate multiple ucgos in nnc if there are multiple independent dsp OPs - preemption : bool : Setting priority of NNC while compiling - quant_type : str : quantiztion type - sync_npu_dsp : bool : simulator : dict - data_format : str : Indicate the position of channel of input [channel_first, channel_last] - use_randomdb : bool : Use randomdb to forward, just support single input - userdb : str : Simulation data set path (default path is {workspace}/DATA/data.txt) perf_estimator : dict - O2_enable : bool : O2 optimization (true or false) - O2_fm_forwarding : bool : feature-map forwarding (true or false) - SEG : bool : Set true if input model is Deeplab V3+ - SSD : bool : Set true if input model is SSD detection - bit_width_factor_FM : int : Select feature map bit width factor (1 or 2) - bit_width_factor_FP16 : bool : Set bit width factor as floating point (true or false) - bit_width_factor_weight : int : Select weight bit width factor (1 or 2) - core_num : int : 1 for single core, 2 for instance-1 - device : str : Select device type [Gen-3, Gen-4, Gen-5, Gen-5a] - json_report : bool : Enable report json format - nq_fold : bool : Speed-up by folding normalization parameters for input data (true: model input dtype is uint8, false: model input dtype is float32) profiler : dict - iter : int : This decides how many time the model inference will be processed. - mode : str : 1. Low power mode : Mode for low power consumption and low performance. 2. Balanced mode : Mode for medium power consumption and medium performance. 3. Performance mode : Mode for high power consumption and high performance. 4. Boost mode : Mode for very high power consumption and very high performance. [lowpower, balanced, performance, boost] - target : str : profiling target. [model, system] - test_type : str : ENN running mode [lib, service] - tv_threshold : float : The value is used for tolerance threshold of output match verification. - bitmatch_test : bool : if set true, visual profiler will compile nnc first - core_num : str : The number of NPU core. [single, multiple] - device : str : Target device. [Gen-3, Gen-4, Gen-5, Gen-5a, Gen-6, Gen-6npu, Gen-7] - device_id : str : id of the device connected to the server or PC running the enntools docker - remote_ssh_config_path : str : remote testing ssh config path (default path is {workspace}/DATA/remote_ssh_config.yaml) - ssh_bool : str : Connect to the device through ssh. [SSH_FALSE, SSH_TRUE] eht yaml file The optimization file contains optimization information. It allows you to set the locations of input and output models and detailed configurations for each module. Optimization is performed differently depending on the model type. Detailed explanation for eht yaml file model_type : string Specify the type of the input model. Optimization is performed differently depending on the model type, with details as follows CV LVM LLM quantizer : dict precision_weight : str : precision of weight(ex. int8, int16, fp16) precision_activation : str : precision of activation(ex. int8, int16, fp16) mpq_operator_dict : dict : When performing mixed precision quantization, input the operators and precision to be quantized with a precision different from the values specified in precision_weight and precision_activation above. alpha : float : smoothquant migration strength calibration_data_path : string : The path to the representative data to be used for calibration calibration_args : dict : Arguments for processing calibration samples : int : How many calibration data samples to use seed : int : A value set as a seed for random selection add_dummy_conv: bool : Whether apply the dummy conv algorithm input_dtype: dict : Input data type for the quantized model output_dtype: dict : Output data type for the quantized model simulator : dict metric : string: The metric to be used for measurement threshold : float : The threshold value of the metric that determines agreement / disagreement input_data_path : string: The path to the dataset for model inference optimizer : dict skip_4_dim_conversion : bool : If true, it does not convert to 4 dimensions before optimization; if false, it proceeds with the conversion. overwrite_input_shapes : dict : Enter the input shape for models with undefined input shapes. custom_template_path : dict: Enter the templates to apply for optimization. Use the path of the Python file containing the template as the key, and the template within it as the value. enntools conversion When performing conversion, users can choose between using eht and elt depending on the mode. The detailed workflow is illustrated in the figure below. bash enntools conversion Introduction to Exynos AI Studio Exynos AI Studio is a software that provides optimization and conversion techniques for neural networks models. This software takes models as input and offers functionalities such as quantization, conversion, optimization, compilation to generate NNC models. System overview diagram Exynos AI Studio Documentation Exynos AI Studio References Exynos AI Studio Developer Guide This guide"
    ],
    "reference": "In the context of using enntools for model optimization, Gen-4 supports devices such as Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, and Gen-4Multi (only for fp16), while Gen-6 supports devices including Gen-5, Gen-5a, Gen-5b, and Gen-6. This indicates that Gen-6 has a more advanced range of device compatibility compared to Gen-4.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the differences in device support between Gen-4 and Gen-5 in the context of using enntools for model optimization and conversion?",
    "reference_contexts": [
      "<1-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :",
      "<2-hop>\n\nstr : debug str for compiler - dequant_type : str : dequantiztion type - device : str : System on Chip type [Gen-3, Gen-3b, Gen-4, Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - enable_ofm_reuse : bool : Enable the reuse of OFM region for IMFM. - enable_stm : bool : Generate compile log including L1 tiling information - flc : bool : Enable featuremap lossless compression - fp16_swwa : bool : Enable NPU fp16 workaround with psum_init - input_conversion : str : Add a Tensor2Cell format converter node at start of network - mi : bool : multiple input compile - mo : bool : multiple output compile - multi_ncp : bool : generate multi-ncp(ucgo) custom op - multi_vc : bool : Introduce Multi-VC for OFM, IFM, and weight transfer - multicore : bool : Enable NPU multicore - optimization : str : Optimization choice [O1, O2, O3] - output_conversion : str : Add a Tensor2Cell format converter node at end of network - packed_ucgo : bool : true: generate one ucgo in nnc, false: generate multiple ucgos in nnc if there are multiple independent dsp OPs - preemption : bool : Setting priority of NNC while compiling - quant_type : str : quantiztion type - sync_npu_dsp : bool : simulator : dict - data_format : str : Indicate the position of channel of input [channel_first, channel_last] - use_randomdb : bool : Use randomdb to forward, just support single input - userdb : str : Simulation data set path (default path is {workspace}/DATA/data.txt) perf_estimator : dict - O2_enable : bool : O2 optimization (true or false) - O2_fm_forwarding : bool : feature-map forwarding (true or false) - SEG : bool : Set true if input model is Deeplab V3+ - SSD : bool : Set true if input model is SSD detection - bit_width_factor_FM : int : Select feature map bit width factor (1 or 2) - bit_width_factor_FP16 : bool : Set bit width factor as floating point (true or false) - bit_width_factor_weight : int : Select weight bit width factor (1 or 2) - core_num : int : 1 for single core, 2 for instance-1 - device : str : Select device type [Gen-3, Gen-4, Gen-5, Gen-5a] - json_report : bool : Enable report json format - nq_fold : bool : Speed-up by folding normalization parameters for input data (true: model input dtype is uint8, false: model input dtype is float32) profiler : dict - iter : int : This decides how many time the model inference will be processed. - mode : str : 1. Low power mode : Mode for low power consumption and low performance. 2. Balanced mode : Mode for medium power consumption and medium performance. 3. Performance mode : Mode for high power consumption and high performance. 4. Boost mode : Mode for very high power consumption and very high performance. [lowpower, balanced, performance, boost] - target : str : profiling target. [model, system] - test_type : str : ENN running mode [lib, service] - tv_threshold : float : The value is used for tolerance threshold of output match verification. - bitmatch_test : bool : if set true, visual profiler will compile nnc first - core_num : str : The number of NPU core. [single, multiple] - device : str : Target device. [Gen-3, Gen-4, Gen-5, Gen-5a, Gen-6, Gen-6npu, Gen-7] - device_id : str : id of the device connected to the server or PC running the enntools docker - remote_ssh_config_path : str : remote testing ssh config path (default path is {workspace}/DATA/remote_ssh_config.yaml) - ssh_bool : str : Connect to the device through ssh. [SSH_FALSE, SSH_TRUE] eht yaml file The optimization file contains optimization information. It allows you to set the locations of input and output models and detailed configurations for each module. Optimization is performed differently depending on the model type. Detailed explanation for eht yaml file model_type : string Specify the type of the input model. Optimization is performed differently depending on the model type, with details as follows CV LVM LLM quantizer : dict precision_weight : str : precision of weight(ex. int8, int16, fp16) precision_activation : str : precision of activation(ex. int8, int16, fp16) mpq_operator_dict : dict : When performing mixed precision quantization, input the operators and precision to be quantized with a precision different from the values specified in precision_weight and precision_activation above. alpha : float : smoothquant migration strength calibration_data_path : string : The path to the representative data to be used for calibration calibration_args : dict : Arguments for processing calibration samples : int : How many calibration data samples to use seed : int : A value set as a seed for random selection add_dummy_conv: bool : Whether apply the dummy conv algorithm input_dtype: dict : Input data type for the quantized model output_dtype: dict : Output data type for the quantized model simulator : dict metric : string: The metric to be used for measurement threshold : float : The threshold value of the metric that determines agreement / disagreement input_data_path : string: The path to the dataset for model inference optimizer : dict skip_4_dim_conversion : bool : If true, it does not convert to 4 dimensions before optimization; if false, it proceeds with the conversion. overwrite_input_shapes : dict : Enter the input shape for models with undefined input shapes. custom_template_path : dict: Enter the templates to apply for optimization. Use the path of the Python file containing the template as the key, and the template within it as the value. enntools conversion When performing conversion, users can choose between using eht and elt depending on the mode. The detailed workflow is illustrated in the figure below. bash enntools conversion Introduction to Exynos AI Studio Exynos AI Studio is a software that provides optimization and conversion techniques for neural networks models. This software takes models as input and offers functionalities such as quantization, conversion, optimization, compilation to generate NNC models. System overview diagram Exynos AI Studio Documentation Exynos AI Studio References Exynos AI Studio Developer Guide This guide"
    ],
    "reference": "In the context of using enntools for model optimization and conversion, Gen-4 supports devices such as Gen-2, Gen-2a, Gen-3, Gen-3DSP, and Gen-4, while Gen-5 supports devices including Gen-5, Gen-5a, and Gen-5b. This indicates that Gen-5 has a more advanced range of device compatibility compared to Gen-4, which is crucial for developers optimizing AI models for deployment on Samsung Exynos hardware.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the differences in device support between Gen-4 and Gen-6 in the context of using enntools for model optimization and conversion?",
    "reference_contexts": [
      "<1-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :",
      "<2-hop>\n\nstr : debug str for compiler - dequant_type : str : dequantiztion type - device : str : System on Chip type [Gen-3, Gen-3b, Gen-4, Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - enable_ofm_reuse : bool : Enable the reuse of OFM region for IMFM. - enable_stm : bool : Generate compile log including L1 tiling information - flc : bool : Enable featuremap lossless compression - fp16_swwa : bool : Enable NPU fp16 workaround with psum_init - input_conversion : str : Add a Tensor2Cell format converter node at start of network - mi : bool : multiple input compile - mo : bool : multiple output compile - multi_ncp : bool : generate multi-ncp(ucgo) custom op - multi_vc : bool : Introduce Multi-VC for OFM, IFM, and weight transfer - multicore : bool : Enable NPU multicore - optimization : str : Optimization choice [O1, O2, O3] - output_conversion : str : Add a Tensor2Cell format converter node at end of network - packed_ucgo : bool : true: generate one ucgo in nnc, false: generate multiple ucgos in nnc if there are multiple independent dsp OPs - preemption : bool : Setting priority of NNC while compiling - quant_type : str : quantiztion type - sync_npu_dsp : bool : simulator : dict - data_format : str : Indicate the position of channel of input [channel_first, channel_last] - use_randomdb : bool : Use randomdb to forward, just support single input - userdb : str : Simulation data set path (default path is {workspace}/DATA/data.txt) perf_estimator : dict - O2_enable : bool : O2 optimization (true or false) - O2_fm_forwarding : bool : feature-map forwarding (true or false) - SEG : bool : Set true if input model is Deeplab V3+ - SSD : bool : Set true if input model is SSD detection - bit_width_factor_FM : int : Select feature map bit width factor (1 or 2) - bit_width_factor_FP16 : bool : Set bit width factor as floating point (true or false) - bit_width_factor_weight : int : Select weight bit width factor (1 or 2) - core_num : int : 1 for single core, 2 for instance-1 - device : str : Select device type [Gen-3, Gen-4, Gen-5, Gen-5a] - json_report : bool : Enable report json format - nq_fold : bool : Speed-up by folding normalization parameters for input data (true: model input dtype is uint8, false: model input dtype is float32) profiler : dict - iter : int : This decides how many time the model inference will be processed. - mode : str : 1. Low power mode : Mode for low power consumption and low performance. 2. Balanced mode : Mode for medium power consumption and medium performance. 3. Performance mode : Mode for high power consumption and high performance. 4. Boost mode : Mode for very high power consumption and very high performance. [lowpower, balanced, performance, boost] - target : str : profiling target. [model, system] - test_type : str : ENN running mode [lib, service] - tv_threshold : float : The value is used for tolerance threshold of output match verification. - bitmatch_test : bool : if set true, visual profiler will compile nnc first - core_num : str : The number of NPU core. [single, multiple] - device : str : Target device. [Gen-3, Gen-4, Gen-5, Gen-5a, Gen-6, Gen-6npu, Gen-7] - device_id : str : id of the device connected to the server or PC running the enntools docker - remote_ssh_config_path : str : remote testing ssh config path (default path is {workspace}/DATA/remote_ssh_config.yaml) - ssh_bool : str : Connect to the device through ssh. [SSH_FALSE, SSH_TRUE] eht yaml file The optimization file contains optimization information. It allows you to set the locations of input and output models and detailed configurations for each module. Optimization is performed differently depending on the model type. Detailed explanation for eht yaml file model_type : string Specify the type of the input model. Optimization is performed differently depending on the model type, with details as follows CV LVM LLM quantizer : dict precision_weight : str : precision of weight(ex. int8, int16, fp16) precision_activation : str : precision of activation(ex. int8, int16, fp16) mpq_operator_dict : dict : When performing mixed precision quantization, input the operators and precision to be quantized with a precision different from the values specified in precision_weight and precision_activation above. alpha : float : smoothquant migration strength calibration_data_path : string : The path to the representative data to be used for calibration calibration_args : dict : Arguments for processing calibration samples : int : How many calibration data samples to use seed : int : A value set as a seed for random selection add_dummy_conv: bool : Whether apply the dummy conv algorithm input_dtype: dict : Input data type for the quantized model output_dtype: dict : Output data type for the quantized model simulator : dict metric : string: The metric to be used for measurement threshold : float : The threshold value of the metric that determines agreement / disagreement input_data_path : string: The path to the dataset for model inference optimizer : dict skip_4_dim_conversion : bool : If true, it does not convert to 4 dimensions before optimization; if false, it proceeds with the conversion. overwrite_input_shapes : dict : Enter the input shape for models with undefined input shapes. custom_template_path : dict: Enter the templates to apply for optimization. Use the path of the Python file containing the template as the key, and the template within it as the value. enntools conversion When performing conversion, users can choose between using eht and elt depending on the mode. The detailed workflow is illustrated in the figure below. bash enntools conversion Introduction to Exynos AI Studio Exynos AI Studio is a software that provides optimization and conversion techniques for neural networks models. This software takes models as input and offers functionalities such as quantization, conversion, optimization, compilation to generate NNC models. System overview diagram Exynos AI Studio Documentation Exynos AI Studio References Exynos AI Studio Developer Guide This guide"
    ],
    "reference": "In the context of using enntools for model optimization and conversion, Gen-4 supports devices such as Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, and Gen-4Multi (only for fp16), while Gen-6 supports devices including Gen-5, Gen-5a, Gen-5b, and Gen-6. This indicates that Gen-6 has a broader range of newer devices compared to Gen-4, which includes older generations.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps involved in using the Exynos AI Studio for converting models, and how does the choice of hardware type, specifically Gen-5, affect the conversion process?",
    "reference_contexts": [
      "<1-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :",
      "<2-hop>\n\nstr : debug str for compiler - dequant_type : str : dequantiztion type - device : str : System on Chip type [Gen-3, Gen-3b, Gen-4, Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - enable_ofm_reuse : bool : Enable the reuse of OFM region for IMFM. - enable_stm : bool : Generate compile log including L1 tiling information - flc : bool : Enable featuremap lossless compression - fp16_swwa : bool : Enable NPU fp16 workaround with psum_init - input_conversion : str : Add a Tensor2Cell format converter node at start of network - mi : bool : multiple input compile - mo : bool : multiple output compile - multi_ncp : bool : generate multi-ncp(ucgo) custom op - multi_vc : bool : Introduce Multi-VC for OFM, IFM, and weight transfer - multicore : bool : Enable NPU multicore - optimization : str : Optimization choice [O1, O2, O3] - output_conversion : str : Add a Tensor2Cell format converter node at end of network - packed_ucgo : bool : true: generate one ucgo in nnc, false: generate multiple ucgos in nnc if there are multiple independent dsp OPs - preemption : bool : Setting priority of NNC while compiling - quant_type : str : quantiztion type - sync_npu_dsp : bool : simulator : dict - data_format : str : Indicate the position of channel of input [channel_first, channel_last] - use_randomdb : bool : Use randomdb to forward, just support single input - userdb : str : Simulation data set path (default path is {workspace}/DATA/data.txt) perf_estimator : dict - O2_enable : bool : O2 optimization (true or false) - O2_fm_forwarding : bool : feature-map forwarding (true or false) - SEG : bool : Set true if input model is Deeplab V3+ - SSD : bool : Set true if input model is SSD detection - bit_width_factor_FM : int : Select feature map bit width factor (1 or 2) - bit_width_factor_FP16 : bool : Set bit width factor as floating point (true or false) - bit_width_factor_weight : int : Select weight bit width factor (1 or 2) - core_num : int : 1 for single core, 2 for instance-1 - device : str : Select device type [Gen-3, Gen-4, Gen-5, Gen-5a] - json_report : bool : Enable report json format - nq_fold : bool : Speed-up by folding normalization parameters for input data (true: model input dtype is uint8, false: model input dtype is float32) profiler : dict - iter : int : This decides how many time the model inference will be processed. - mode : str : 1. Low power mode : Mode for low power consumption and low performance. 2. Balanced mode : Mode for medium power consumption and medium performance. 3. Performance mode : Mode for high power consumption and high performance. 4. Boost mode : Mode for very high power consumption and very high performance. [lowpower, balanced, performance, boost] - target : str : profiling target. [model, system] - test_type : str : ENN running mode [lib, service] - tv_threshold : float : The value is used for tolerance threshold of output match verification. - bitmatch_test : bool : if set true, visual profiler will compile nnc first - core_num : str : The number of NPU core. [single, multiple] - device : str : Target device. [Gen-3, Gen-4, Gen-5, Gen-5a, Gen-6, Gen-6npu, Gen-7] - device_id : str : id of the device connected to the server or PC running the enntools docker - remote_ssh_config_path : str : remote testing ssh config path (default path is {workspace}/DATA/remote_ssh_config.yaml) - ssh_bool : str : Connect to the device through ssh. [SSH_FALSE, SSH_TRUE] eht yaml file The optimization file contains optimization information. It allows you to set the locations of input and output models and detailed configurations for each module. Optimization is performed differently depending on the model type. Detailed explanation for eht yaml file model_type : string Specify the type of the input model. Optimization is performed differently depending on the model type, with details as follows CV LVM LLM quantizer : dict precision_weight : str : precision of weight(ex. int8, int16, fp16) precision_activation : str : precision of activation(ex. int8, int16, fp16) mpq_operator_dict : dict : When performing mixed precision quantization, input the operators and precision to be quantized with a precision different from the values specified in precision_weight and precision_activation above. alpha : float : smoothquant migration strength calibration_data_path : string : The path to the representative data to be used for calibration calibration_args : dict : Arguments for processing calibration samples : int : How many calibration data samples to use seed : int : A value set as a seed for random selection add_dummy_conv: bool : Whether apply the dummy conv algorithm input_dtype: dict : Input data type for the quantized model output_dtype: dict : Output data type for the quantized model simulator : dict metric : string: The metric to be used for measurement threshold : float : The threshold value of the metric that determines agreement / disagreement input_data_path : string: The path to the dataset for model inference optimizer : dict skip_4_dim_conversion : bool : If true, it does not convert to 4 dimensions before optimization; if false, it proceeds with the conversion. overwrite_input_shapes : dict : Enter the input shape for models with undefined input shapes. custom_template_path : dict: Enter the templates to apply for optimization. Use the path of the Python file containing the template as the key, and the template within it as the value. enntools conversion When performing conversion, users can choose between using eht and elt depending on the mode. The detailed workflow is illustrated in the figure below. bash enntools conversion Introduction to Exynos AI Studio Exynos AI Studio is a software that provides optimization and conversion techniques for neural networks models. This software takes models as input and offers functionalities such as quantization, conversion, optimization, compilation to generate NNC models. System overview diagram Exynos AI Studio Documentation Exynos AI Studio References Exynos AI Studio Developer Guide This guide"
    ],
    "reference": "To use the Exynos AI Studio for converting models, the first step is to choose the hardware type. The default option utilizes only the CPU and GPU, while selecting 'Accelerate' engages the NPU as an additional accelerator. However, it is important to note that the NPU does not support all models, and choosing the Accelerate option may lead to complications. After confirming the selections, users can initiate the conversion process by clicking 'Convert'. Once the conversion is complete, the 'NNC Download' button becomes enabled, allowing users to download the converted NNC model file. Additionally, if there are any conversion failures, users can download and examine the log files for troubleshooting. The choice of hardware type, particularly Gen-5, is crucial as it determines the compatibility and performance of the model during the conversion process.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How can I convert a TFLITE model to an NNC model using the Exynos AI Studio Service, and what are the necessary steps involved in this process?",
    "reference_contexts": [
      "<1-hop>\n\nTroubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For",
      "<2-hop>\n\nTools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)"
    ],
    "reference": "To convert a TFLITE model to an NNC model using the Exynos AI Studio Service, follow these steps: First, access the ENN Eco-system Portal. If you are a new user, sign up to create an account; if you are a returning user, log in. Next, navigate to the Exynos AI Studio Service page and enter a descriptive title for your project. Use the provided interface to upload your TFLITE model. Choose your hardware preferences, which can either be the default (utilizing only the CPU and GPU) or accelerate (engaging the NPU as an additional accelerator). Note that the NPU does not support all layers, and using unsupported layers may lead to complications. After confirming your selections, click 'Convert' to start the model conversion process. If the conversion is successful, the NNC Download button will be enabled, allowing you to download the converted NNC model.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How can I convert a TFLITE model to an NNC model using the Exynos AI Studio service, and what are the hardware preferences available during this process?",
    "reference_contexts": [
      "<1-hop>\n\nTroubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For",
      "<2-hop>\n\nTools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)"
    ],
    "reference": "To convert a TFLITE model to an NNC model using the Exynos AI Studio service, follow these steps: First, access the ENN Eco-system Portal by signing up or logging in. Then, navigate to the Exynos AI Studio service page and provide project information by entering a descriptive title for your project. You can upload your TFLITE model using the provided interface. During the conversion process, you have the option to choose hardware preferences: the default option utilizes only the CPU and GPU, while the 'Accelerate' option engages the NPU as an additional accelerator. However, be aware that the NPU does not support all layers, and using unsupported layers may lead to complications. After confirming your selections, click 'Convert' to start the model conversion process. If successful, the NNC Download button will be enabled, allowing you to download the converted NNC model.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How do I convert a TFLITE model to an NNC model using the Exynos AI Studio service, and what are the hardware preferences I can choose during this process?",
    "reference_contexts": [
      "<1-hop>\n\nTroubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For",
      "<2-hop>\n\nTools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)"
    ],
    "reference": "To convert a TFLITE model to an NNC model using the Exynos AI Studio service, you need to follow these steps: First, access the ENN Eco-system Portal by signing up if you are a new user or logging in if you are a returning user. Then, navigate to the Exynos AI Studio service page and enter a descriptive title for your project. You will upload your TFLITE model using the provided interface. When it comes to hardware preferences, you can choose between two options: Default, which utilizes only the CPU and GPU, or Accelerate, which engages the NPU as an additional accelerator. However, be cautious as the NPU does not support all layers, and using unsupported layers may lead to complications. After confirming your selections, click Convert to start the model conversion process. If the conversion is successful, the NNC Download button will be enabled, allowing you to download the NNC model.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What steps are involved in converting a TFLite model to an NNC model using the Exynos AI Studio Service?",
    "reference_contexts": [
      "<1-hop>\n\nTroubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For",
      "<2-hop>\n\nTools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)"
    ],
    "reference": "To convert a TFLite model to an NNC model using the Exynos AI Studio Service, follow these steps: 1. Access the ENN Eco-system: Sign up for a new account or log in if you are an existing user. 2. Navigate to the Exynos AI Studio service page. 3. Provide Project Information: Enter a descriptive title for your project and upload your TFLite model. 4. Choose Hardware Preferences: Select between default (using only CPU and GPU) or accelerate (engaging the NPU as an additional accelerator, noting that not all layers are supported). 5. Initiate the Conversion: Click Confirm to verify your selections and then click Convert to start the model conversion process. 6. Download the Converted Model: If the conversion is successful, the NNC Download button will be enabled, allowing you to download the NNC model.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:13:51.716612+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "Can you explain what EnnMetaTypeId is and how it relates to getting meta information in the ENN Framework?",
    "reference_contexts": [
      "Introduction EHT Converter Optimizer Quantizer Simulator Model Optimization Flow ELT Model Analyzer SCVT Quantizer NPU Simulator Performance Estimator Graph Generator Dataset and Model Quick Start System Requirements How to use ENN Framework Data Type References _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; API reference ENN Framework Data Type References _NumberOfBuffersInfo _ennBuffer _ennBufferInfo ENN Framwork API Functions Context initialize / deinitialize OpenModel / CloseModel related Memory Handling Setters and Getters for model Commit Buffer Execute Models Security, preference, get meta information.. title: _ennBuffer _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; Updated on 2023-08-11 at 16:24:05 +0900 title: _ennBufferInfo _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; Updated on 2023-08-11 at 16:24:05 +0900 title: _NumberOfBuffersInfo _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; Updated on 2023-08-11 at 16:24:05 +0900 title: Commit Buffer Commit Buffer Functions Name EnnReturn EnnBufferCommit (const EnnModelId model_id, const int session_id =0) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Functions Documentation function EnnBufferCommit EnnReturn EnnBufferCommit( const EnnModelId model_id, const int session_id =0 ) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Parameters: model_id [IN] model ID from load_model Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Context initialize / deinitialize Context initialize / deinitialize Functions Name EnnReturn EnnInitialize (void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. EnnReturn EnnDeinitialize (void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Functions Documentation function EnnInitialize EnnReturn EnnInitialize( void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. Return: EnnReturn result, 0 is success function EnnDeinitialize EnnReturn EnnDeinitialize( void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Security, preference, get meta information.. Security, preference, get meta information.. Functions Name EnnReturn EnnSetPreferencePresetId (const uint32_t val) Setting Preset ID for operation performance. EnnReturn EnnSetPreferencePerfConfigId (const uint32_t val) Setting PerfConfig ID for operation performance. EnnReturn EnnSetPreferencePerfMode (const uint32_t val) Setting Performance Mode. EnnReturn EnnSetPreferenceTimeOut (const uint32_t val) Setting Preset ID for time out. EnnReturn EnnSetPreferencePriority (const uint32_t val) Setting priority value for NPU. EnnReturn EnnSetPreferenceCoreAffinity (const uint32_t val) Setting affinity to set NPU core operation. EnnReturn EnnGetPreferencePresetId (uint32_t * val_ptr) Get current information for Preset ID. EnnReturn EnnGetPreferencePerfConfigId (uint32_t * val_ptr) Get current information for PerfConfig ID. EnnReturn EnnGetPreferencePerfMode (uint32_t * val_ptr) Get current information for Performance Mode. EnnReturn EnnGetPreferenceTimeOut (uint32_t * val_ptr) Get current information for Time Out. EnnReturn EnnGetPreferencePriority (uint32_t * val_ptr) Get current information for NPU Priority. EnnReturn EnnGetPreferenceCoreAffinity (uint32_t * val_ptr) Get current information for NPU Core affinity. EnnReturn EnnGetMetaInfo (const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]) Get Meta Information. EnnReturn EnnSetExecMsgAlwaysOn () Set frequency of execution message print. Functions Documentation function EnnSetPreferencePresetId EnnReturn EnnSetPreferencePresetId( const uint32_t val ) Setting Preset ID for operation performance. Parameters: val [IN] value to set preset ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfConfigId EnnReturn EnnSetPreferencePerfConfigId( const uint32_t val ) Setting PerfConfig ID for operation performance. Parameters: val [IN] value to set PerfConfig ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfMode EnnReturn EnnSetPreferencePerfMode( const uint32_t val ) Setting Performance Mode. Parameters: val [IN] value to set Performance Mode Return: EnnReturn result, 0 is success function EnnSetPreferenceTimeOut EnnReturn EnnSetPreferenceTimeOut( const uint32_t val ) Setting Preset ID for time out. Parameters: val [IN] value to set time out Return: EnnReturn result, 0 is success Note: in second function EnnSetPreferencePriority EnnReturn EnnSetPreferencePriority( const uint32_t val ) Setting priority value for NPU. Parameters: val [IN] value to set NPU job priority Return: EnnReturn result, 0 is success function EnnSetPreferenceCoreAffinity EnnReturn EnnSetPreferenceCoreAffinity( const uint32_t val ) Setting affinity to set NPU core operation. Parameters: val [IN] value to set affinity Return: EnnReturn result, 0 is success Note: in second function EnnGetPreferencePresetId EnnReturn EnnGetPreferencePresetId( uint32_t * val_ptr ) Get current information for Preset ID. Parameters: val [OUT] current value of Preset ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfConfigId EnnReturn EnnGetPreferencePerfConfigId( uint32_t * val_ptr ) Get current information for PerfConfig ID. Parameters: val [OUT] current value of PerfConfig ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfMode EnnReturn EnnGetPreferencePerfMode( uint32_t * val_ptr ) Get current information for Performance Mode. Parameters: val [OUT] current value of Performance Mode Return: EnnReturn result, 0 is success function EnnGetPreferenceTimeOut EnnReturn EnnGetPreferenceTimeOut( uint32_t * val_ptr ) Get current information for Time Out. Parameters: val [OUT] current value of Time Out Return: EnnReturn result, 0 is success function EnnGetPreferencePriority EnnReturn EnnGetPreferencePriority( uint32_t * val_ptr ) Get current information for NPU Priority. Parameters: val [OUT] current value of NPU Priority Return:"
    ],
    "reference": "EnnMetaTypeId is used in the ENN Framework to get meta information. The function EnnGetMetaInfo takes this type as a parameter along with the model ID to retrieve the relevant meta information.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the functions related to NPU performance settings in the ENN Framework?",
    "reference_contexts": [
      "Introduction EHT Converter Optimizer Quantizer Simulator Model Optimization Flow ELT Model Analyzer SCVT Quantizer NPU Simulator Performance Estimator Graph Generator Dataset and Model Quick Start System Requirements How to use ENN Framework Data Type References _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; API reference ENN Framework Data Type References _NumberOfBuffersInfo _ennBuffer _ennBufferInfo ENN Framwork API Functions Context initialize / deinitialize OpenModel / CloseModel related Memory Handling Setters and Getters for model Commit Buffer Execute Models Security, preference, get meta information.. title: _ennBuffer _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; Updated on 2023-08-11 at 16:24:05 +0900 title: _ennBufferInfo _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; Updated on 2023-08-11 at 16:24:05 +0900 title: _NumberOfBuffersInfo _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; Updated on 2023-08-11 at 16:24:05 +0900 title: Commit Buffer Commit Buffer Functions Name EnnReturn EnnBufferCommit (const EnnModelId model_id, const int session_id =0) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Functions Documentation function EnnBufferCommit EnnReturn EnnBufferCommit( const EnnModelId model_id, const int session_id =0 ) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Parameters: model_id [IN] model ID from load_model Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Context initialize / deinitialize Context initialize / deinitialize Functions Name EnnReturn EnnInitialize (void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. EnnReturn EnnDeinitialize (void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Functions Documentation function EnnInitialize EnnReturn EnnInitialize( void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. Return: EnnReturn result, 0 is success function EnnDeinitialize EnnReturn EnnDeinitialize( void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Security, preference, get meta information.. Security, preference, get meta information.. Functions Name EnnReturn EnnSetPreferencePresetId (const uint32_t val) Setting Preset ID for operation performance. EnnReturn EnnSetPreferencePerfConfigId (const uint32_t val) Setting PerfConfig ID for operation performance. EnnReturn EnnSetPreferencePerfMode (const uint32_t val) Setting Performance Mode. EnnReturn EnnSetPreferenceTimeOut (const uint32_t val) Setting Preset ID for time out. EnnReturn EnnSetPreferencePriority (const uint32_t val) Setting priority value for NPU. EnnReturn EnnSetPreferenceCoreAffinity (const uint32_t val) Setting affinity to set NPU core operation. EnnReturn EnnGetPreferencePresetId (uint32_t * val_ptr) Get current information for Preset ID. EnnReturn EnnGetPreferencePerfConfigId (uint32_t * val_ptr) Get current information for PerfConfig ID. EnnReturn EnnGetPreferencePerfMode (uint32_t * val_ptr) Get current information for Performance Mode. EnnReturn EnnGetPreferenceTimeOut (uint32_t * val_ptr) Get current information for Time Out. EnnReturn EnnGetPreferencePriority (uint32_t * val_ptr) Get current information for NPU Priority. EnnReturn EnnGetPreferenceCoreAffinity (uint32_t * val_ptr) Get current information for NPU Core affinity. EnnReturn EnnGetMetaInfo (const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]) Get Meta Information. EnnReturn EnnSetExecMsgAlwaysOn () Set frequency of execution message print. Functions Documentation function EnnSetPreferencePresetId EnnReturn EnnSetPreferencePresetId( const uint32_t val ) Setting Preset ID for operation performance. Parameters: val [IN] value to set preset ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfConfigId EnnReturn EnnSetPreferencePerfConfigId( const uint32_t val ) Setting PerfConfig ID for operation performance. Parameters: val [IN] value to set PerfConfig ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfMode EnnReturn EnnSetPreferencePerfMode( const uint32_t val ) Setting Performance Mode. Parameters: val [IN] value to set Performance Mode Return: EnnReturn result, 0 is success function EnnSetPreferenceTimeOut EnnReturn EnnSetPreferenceTimeOut( const uint32_t val ) Setting Preset ID for time out. Parameters: val [IN] value to set time out Return: EnnReturn result, 0 is success Note: in second function EnnSetPreferencePriority EnnReturn EnnSetPreferencePriority( const uint32_t val ) Setting priority value for NPU. Parameters: val [IN] value to set NPU job priority Return: EnnReturn result, 0 is success function EnnSetPreferenceCoreAffinity EnnReturn EnnSetPreferenceCoreAffinity( const uint32_t val ) Setting affinity to set NPU core operation. Parameters: val [IN] value to set affinity Return: EnnReturn result, 0 is success Note: in second function EnnGetPreferencePresetId EnnReturn EnnGetPreferencePresetId( uint32_t * val_ptr ) Get current information for Preset ID. Parameters: val [OUT] current value of Preset ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfConfigId EnnReturn EnnGetPreferencePerfConfigId( uint32_t * val_ptr ) Get current information for PerfConfig ID. Parameters: val [OUT] current value of PerfConfig ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfMode EnnReturn EnnGetPreferencePerfMode( uint32_t * val_ptr ) Get current information for Performance Mode. Parameters: val [OUT] current value of Performance Mode Return: EnnReturn result, 0 is success function EnnGetPreferenceTimeOut EnnReturn EnnGetPreferenceTimeOut( uint32_t * val_ptr ) Get current information for Time Out. Parameters: val [OUT] current value of Time Out Return: EnnReturn result, 0 is success function EnnGetPreferencePriority EnnReturn EnnGetPreferencePriority( uint32_t * val_ptr ) Get current information for NPU Priority. Parameters: val [OUT] current value of NPU Priority Return:"
    ],
    "reference": "The functions related to NPU performance settings in the ENN Framework include: EnnReturn EnnSetPreferencePresetId (const uint32_t val) for setting the Preset ID for operation performance; EnnReturn EnnSetPreferencePerfConfigId (const uint32_t val) for setting the PerfConfig ID for operation performance; EnnReturn EnnSetPreferencePerfMode (const uint32_t val) for setting the Performance Mode; EnnReturn EnnSetPreferenceTimeOut (const uint32_t val) for setting the time out; EnnReturn EnnSetPreferencePriority (const uint32_t val) for setting the priority value for NPU; and EnnReturn EnnSetPreferenceCoreAffinity (const uint32_t val) for setting the affinity to set NPU core operation.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is the role of the NPU in the context of the ENN Framework?",
    "reference_contexts": [
      "Introduction EHT Converter Optimizer Quantizer Simulator Model Optimization Flow ELT Model Analyzer SCVT Quantizer NPU Simulator Performance Estimator Graph Generator Dataset and Model Quick Start System Requirements How to use ENN Framework Data Type References _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; API reference ENN Framework Data Type References _NumberOfBuffersInfo _ennBuffer _ennBufferInfo ENN Framwork API Functions Context initialize / deinitialize OpenModel / CloseModel related Memory Handling Setters and Getters for model Commit Buffer Execute Models Security, preference, get meta information.. title: _ennBuffer _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; Updated on 2023-08-11 at 16:24:05 +0900 title: _ennBufferInfo _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; Updated on 2023-08-11 at 16:24:05 +0900 title: _NumberOfBuffersInfo _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; Updated on 2023-08-11 at 16:24:05 +0900 title: Commit Buffer Commit Buffer Functions Name EnnReturn EnnBufferCommit (const EnnModelId model_id, const int session_id =0) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Functions Documentation function EnnBufferCommit EnnReturn EnnBufferCommit( const EnnModelId model_id, const int session_id =0 ) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Parameters: model_id [IN] model ID from load_model Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Context initialize / deinitialize Context initialize / deinitialize Functions Name EnnReturn EnnInitialize (void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. EnnReturn EnnDeinitialize (void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Functions Documentation function EnnInitialize EnnReturn EnnInitialize( void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. Return: EnnReturn result, 0 is success function EnnDeinitialize EnnReturn EnnDeinitialize( void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Security, preference, get meta information.. Security, preference, get meta information.. Functions Name EnnReturn EnnSetPreferencePresetId (const uint32_t val) Setting Preset ID for operation performance. EnnReturn EnnSetPreferencePerfConfigId (const uint32_t val) Setting PerfConfig ID for operation performance. EnnReturn EnnSetPreferencePerfMode (const uint32_t val) Setting Performance Mode. EnnReturn EnnSetPreferenceTimeOut (const uint32_t val) Setting Preset ID for time out. EnnReturn EnnSetPreferencePriority (const uint32_t val) Setting priority value for NPU. EnnReturn EnnSetPreferenceCoreAffinity (const uint32_t val) Setting affinity to set NPU core operation. EnnReturn EnnGetPreferencePresetId (uint32_t * val_ptr) Get current information for Preset ID. EnnReturn EnnGetPreferencePerfConfigId (uint32_t * val_ptr) Get current information for PerfConfig ID. EnnReturn EnnGetPreferencePerfMode (uint32_t * val_ptr) Get current information for Performance Mode. EnnReturn EnnGetPreferenceTimeOut (uint32_t * val_ptr) Get current information for Time Out. EnnReturn EnnGetPreferencePriority (uint32_t * val_ptr) Get current information for NPU Priority. EnnReturn EnnGetPreferenceCoreAffinity (uint32_t * val_ptr) Get current information for NPU Core affinity. EnnReturn EnnGetMetaInfo (const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]) Get Meta Information. EnnReturn EnnSetExecMsgAlwaysOn () Set frequency of execution message print. Functions Documentation function EnnSetPreferencePresetId EnnReturn EnnSetPreferencePresetId( const uint32_t val ) Setting Preset ID for operation performance. Parameters: val [IN] value to set preset ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfConfigId EnnReturn EnnSetPreferencePerfConfigId( const uint32_t val ) Setting PerfConfig ID for operation performance. Parameters: val [IN] value to set PerfConfig ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfMode EnnReturn EnnSetPreferencePerfMode( const uint32_t val ) Setting Performance Mode. Parameters: val [IN] value to set Performance Mode Return: EnnReturn result, 0 is success function EnnSetPreferenceTimeOut EnnReturn EnnSetPreferenceTimeOut( const uint32_t val ) Setting Preset ID for time out. Parameters: val [IN] value to set time out Return: EnnReturn result, 0 is success Note: in second function EnnSetPreferencePriority EnnReturn EnnSetPreferencePriority( const uint32_t val ) Setting priority value for NPU. Parameters: val [IN] value to set NPU job priority Return: EnnReturn result, 0 is success function EnnSetPreferenceCoreAffinity EnnReturn EnnSetPreferenceCoreAffinity( const uint32_t val ) Setting affinity to set NPU core operation. Parameters: val [IN] value to set affinity Return: EnnReturn result, 0 is success Note: in second function EnnGetPreferencePresetId EnnReturn EnnGetPreferencePresetId( uint32_t * val_ptr ) Get current information for Preset ID. Parameters: val [OUT] current value of Preset ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfConfigId EnnReturn EnnGetPreferencePerfConfigId( uint32_t * val_ptr ) Get current information for PerfConfig ID. Parameters: val [OUT] current value of PerfConfig ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfMode EnnReturn EnnGetPreferencePerfMode( uint32_t * val_ptr ) Get current information for Performance Mode. Parameters: val [OUT] current value of Performance Mode Return: EnnReturn result, 0 is success function EnnGetPreferenceTimeOut EnnReturn EnnGetPreferenceTimeOut( uint32_t * val_ptr ) Get current information for Time Out. Parameters: val [OUT] current value of Time Out Return: EnnReturn result, 0 is success function EnnGetPreferencePriority EnnReturn EnnGetPreferencePriority( uint32_t * val_ptr ) Get current information for NPU Priority. Parameters: val [OUT] current value of NPU Priority Return:"
    ],
    "reference": "The NPU is involved in setting the priority value for operations and configuring core affinity for NPU core operation within the ENN Framework. It plays a crucial role in optimizing performance by allowing developers to set preferences related to operation performance.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps involved in using Exynos AI Studio for optimizing neural network models for Samsung hardware?",
    "reference_contexts": [
      "Index EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "Using Exynos AI Studio involves two main steps: first, the user converts neural network (NN) models to Neural Network Container (NNC) models, which can run efficiently on Samsung Exynos hardware. Second, the user executes the converted model for inference. The model conversion process includes preparing a pre-trained NN model, setting parameters for the tools, and executing the tools for conversion. For model execution, the user initializes the ENN framework, loads the converted model, allocates and commits necessary buffers, copies input data to input buffers, executes the model, and uses data from output buffers. This process can be repeated for multiple executions, followed by releasing buffers and unloading the model.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "Can you explain the workflow involved in using Exynos AI Studio for neural network model optimization and execution?",
    "reference_contexts": [
      "Index EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "Using Exynos AI Studio involves a two-step workflow: first, the user converts neural network (NN) models to Neural Network Container (NNC) models, which are optimized for efficient execution on Samsung Exynos hardware. Second, the user executes the converted model for inference using the ENN framework. The model conversion process requires preparing a pre-trained NN model, setting parameters for the conversion tools, and executing these tools. For model execution, the user initializes the ENN framework, loads the converted model, allocates necessary buffers, copies input data to these buffers, executes the model, and retrieves data from the output buffers. This process can be repeated for multiple executions, followed by releasing the allocated buffers and unloading the model.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "Wht is Exynos AI Studio?",
    "reference_contexts": [
      "Index EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "Exynos AI Studio is a software development kit that allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models and includes information about the ENN framework, model execution, and obtaining output.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How can I convert an ONNX model to CNNX using the Exynos AI Studio Service?",
    "reference_contexts": [
      "Troubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For"
    ],
    "reference": "To convert an ONNX model to CNNX using the Exynos AI Studio Service, you need to use the ONNX2CNNX Converter. This converter requires you to specify both the output model path and the output encodings path.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How can I convert a TFLITE model using Exynos AI Studio?",
    "reference_contexts": [
      "Troubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For"
    ],
    "reference": "You can use the Exynos AI Studio Service to convert trained TFLite models to NNC models by accessing the ENN Ecosystem Portal, creating a project, and following the provided steps for conversion.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "Can you explain how SmoothQuant works in the context of quantization methods for neural networks?",
    "reference_contexts": [
      "Troubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For"
    ],
    "reference": "SmoothQuant is an advanced quantization method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is SmoothQuant and how does it improve quantization in neural networks?",
    "reference_contexts": [
      "Troubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For"
    ],
    "reference": "SmoothQuant is an advanced quantization method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. This technique helps to reduce the performance degradation caused by quantization.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the implications of using the NPU as an accelerator in AI applications?",
    "reference_contexts": [
      "this guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "Engaging the NPU as an additional accelerator may lead to complications, as it does not support all models. Therefore, careful consideration is required when choosing the Accelerate option.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the key features of the Gen-5 system on chip type as mentioned in the context?",
    "reference_contexts": [
      "this guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "The Gen-5 system on chip type is referenced in the context as a device option for model analysis and conversion processes. It is included in the elt yaml file configuration values, where it is listed alongside other generations such as Gen-5a, Gen-5b, and Gen-6. The context indicates that Gen-5 supports various functionalities, including model analysis levels and device specifications for the conversion process.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What role does the NPU play in the hardware acceleration process?",
    "reference_contexts": [
      "this guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "The NPU engages as an additional accelerator when the Accelerate hardware type is chosen, allowing for enhanced performance. However, it is important to note that the NPU does not support all models, and selecting the Accelerate option may lead to complications.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What hardware does the guide utilize for processing?",
    "reference_contexts": [
      "this guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "The guide utilizes only the CPU and GPU by default, and can engage the NPU as an additional accelerator when the Accelerate hardware type is chosen.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What devices are supported by Gen-5a?",
    "reference_contexts": [
      "str : debug str for compiler - dequant_type : str : dequantiztion type - device : str : System on Chip type [Gen-3, Gen-3b, Gen-4, Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - enable_ofm_reuse : bool : Enable the reuse of OFM region for IMFM. - enable_stm : bool : Generate compile log including L1 tiling information - flc : bool : Enable featuremap lossless compression - fp16_swwa : bool : Enable NPU fp16 workaround with psum_init - input_conversion : str : Add a Tensor2Cell format converter node at start of network - mi : bool : multiple input compile - mo : bool : multiple output compile - multi_ncp : bool : generate multi-ncp(ucgo) custom op - multi_vc : bool : Introduce Multi-VC for OFM, IFM, and weight transfer - multicore : bool : Enable NPU multicore - optimization : str : Optimization choice [O1, O2, O3] - output_conversion : str : Add a Tensor2Cell format converter node at end of network - packed_ucgo : bool : true: generate one ucgo in nnc, false: generate multiple ucgos in nnc if there are multiple independent dsp OPs - preemption : bool : Setting priority of NNC while compiling - quant_type : str : quantiztion type - sync_npu_dsp : bool : simulator : dict - data_format : str : Indicate the position of channel of input [channel_first, channel_last] - use_randomdb : bool : Use randomdb to forward, just support single input - userdb : str : Simulation data set path (default path is {workspace}/DATA/data.txt) perf_estimator : dict - O2_enable : bool : O2 optimization (true or false) - O2_fm_forwarding : bool : feature-map forwarding (true or false) - SEG : bool : Set true if input model is Deeplab V3+ - SSD : bool : Set true if input model is SSD detection - bit_width_factor_FM : int : Select feature map bit width factor (1 or 2) - bit_width_factor_FP16 : bool : Set bit width factor as floating point (true or false) - bit_width_factor_weight : int : Select weight bit width factor (1 or 2) - core_num : int : 1 for single core, 2 for instance-1 - device : str : Select device type [Gen-3, Gen-4, Gen-5, Gen-5a] - json_report : bool : Enable report json format - nq_fold : bool : Speed-up by folding normalization parameters for input data (true: model input dtype is uint8, false: model input dtype is float32) profiler : dict - iter : int : This decides how many time the model inference will be processed. - mode : str : 1. Low power mode : Mode for low power consumption and low performance. 2. Balanced mode : Mode for medium power consumption and medium performance. 3. Performance mode : Mode for high power consumption and high performance. 4. Boost mode : Mode for very high power consumption and very high performance. [lowpower, balanced, performance, boost] - target : str : profiling target. [model, system] - test_type : str : ENN running mode [lib, service] - tv_threshold : float : The value is used for tolerance threshold of output match verification. - bitmatch_test : bool : if set true, visual profiler will compile nnc first - core_num : str : The number of NPU core. [single, multiple] - device : str : Target device. [Gen-3, Gen-4, Gen-5, Gen-5a, Gen-6, Gen-6npu, Gen-7] - device_id : str : id of the device connected to the server or PC running the enntools docker - remote_ssh_config_path : str : remote testing ssh config path (default path is {workspace}/DATA/remote_ssh_config.yaml) - ssh_bool : str : Connect to the device through ssh. [SSH_FALSE, SSH_TRUE] eht yaml file The optimization file contains optimization information. It allows you to set the locations of input and output models and detailed configurations for each module. Optimization is performed differently depending on the model type. Detailed explanation for eht yaml file model_type : string Specify the type of the input model. Optimization is performed differently depending on the model type, with details as follows CV LVM LLM quantizer : dict precision_weight : str : precision of weight(ex. int8, int16, fp16) precision_activation : str : precision of activation(ex. int8, int16, fp16) mpq_operator_dict : dict : When performing mixed precision quantization, input the operators and precision to be quantized with a precision different from the values specified in precision_weight and precision_activation above. alpha : float : smoothquant migration strength calibration_data_path : string : The path to the representative data to be used for calibration calibration_args : dict : Arguments for processing calibration samples : int : How many calibration data samples to use seed : int : A value set as a seed for random selection add_dummy_conv: bool : Whether apply the dummy conv algorithm input_dtype: dict : Input data type for the quantized model output_dtype: dict : Output data type for the quantized model simulator : dict metric : string: The metric to be used for measurement threshold : float : The threshold value of the metric that determines agreement / disagreement input_data_path : string: The path to the dataset for model inference optimizer : dict skip_4_dim_conversion : bool : If true, it does not convert to 4 dimensions before optimization; if false, it proceeds with the conversion. overwrite_input_shapes : dict : Enter the input shape for models with undefined input shapes. custom_template_path : dict: Enter the templates to apply for optimization. Use the path of the Python file containing the template as the key, and the template within it as the value. enntools conversion When performing conversion, users can choose between using eht and elt depending on the mode. The detailed workflow is illustrated in the figure below. bash enntools conversion Introduction to Exynos AI Studio Exynos AI Studio is a software that provides optimization and conversion techniques for neural networks models. This software takes models as input and offers functionalities such as quantization, conversion, optimization, compilation to generate NNC models. System overview diagram Exynos AI Studio Documentation Exynos AI Studio References Exynos AI Studio Developer Guide This guide"
    ],
    "reference": "Gen-5a is a type of System on Chip that is included in the list of supported devices, which also includes Gen-3, Gen-3b, Gen-4, Gen-5, Gen-5b, Gen-6, and Gen-7.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "how Exynos AI Studio help with converting NN models?",
    "reference_contexts": [
      "describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "Exynos AI Studio provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models, which is essential for executing models on Exynos devices.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How does Exynos AI Studio utilize GPU for neural network model execution?",
    "reference_contexts": [
      "describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "Exynos AI Studio supports GPU for executing neural network models, as indicated in the support matrix which lists GPU as one of the supported operators for various functions.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What models need to be quantized for NPU in Exynos AI Studio?",
    "reference_contexts": [
      "describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "For NPU, the models must be quantized.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What operating system is required for the Exynos AI Studio service?",
    "reference_contexts": [
      "Tools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)"
    ],
    "reference": "The Exynos AI Studio service requires Linux, which is based on Ubuntu 22.04.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What information can I find in the API Reference related to the Exynos AI Studio service?",
    "reference_contexts": [
      "Tools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)"
    ],
    "reference": "The API Reference provides information on the list of data types and API functions relevant to the Exynos AI Studio service. For more details, users are encouraged to refer to the API Reference.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is Exynos AI Stuido used for?",
    "reference_contexts": [
      "Tools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)"
    ],
    "reference": "Exynos AI Studio is an online platform designed to enable users to convert TFLite models into NNC models.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the minimum specifications for using the Intel Core i7 processor?",
    "reference_contexts": [
      "Tools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)"
    ],
    "reference": "The minimum specifications for using the Intel Core i7 processor include a 64-bit (x86-64) architecture, a minimum of a 2GHz dual-core processor, at least 8GB of RAM (16GB or more recommended), and sufficient disk space with a minimum of 100GB recommended.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps to convert Neural Network models to NNC models using Exynos AI Studio, and how does the CPU play a role in this process?",
    "reference_contexts": [
      "<1-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :",
      "<2-hop>\n\ndescribes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "To convert Neural Network models to NNC models using Exynos AI Studio, you first need to run a docker container and set up your workspace. After creating a folder and placing your ONNX file inside, you initialize the project with 'enntools init'. The conversion process is initiated by modifying the configuration file and setting the mode. The CPU plays a crucial role in this process as it is one of the supported devices for executing the models. The support matrix indicates that various operators can be executed on the CPU, ensuring compatibility with the models being converted.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps to convert a Neural Network model to an NNC model using Exynos AI Studio, and how does the CPU play a role in this process?",
    "reference_contexts": [
      "<1-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :",
      "<2-hop>\n\ndescribes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "To convert a Neural Network model to an NNC model using Exynos AI Studio, you first need to run a docker container and create a workspace. After initializing the project with the command 'enntools init', you will copy your ONNX file into the workspace. The next step involves modifying the configuration file to set the mode and then executing the conversion process. The CPU plays a crucial role in this process as it is one of the supported devices for executing the NNC models, alongside the NPU and GPU. The conversion process also allows for specific layers to be assigned to the CPU, ensuring optimal performance during model execution.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How can I utilize the GPU for model conversion and execution in Exynos AI Studio, and what are the necessary steps to ensure compatibility with the NPU?",
    "reference_contexts": [
      "<1-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :",
      "<2-hop>\n\ndescribes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "To utilize the GPU for model conversion and execution in Exynos AI Studio, you need to ensure that the 'gpu_enable' option is set to true in the elt yaml file configuration. This allows the model to be inferred on the GPU if available. Additionally, when converting models, it is important to note that the NPU does not support all models, so you should verify that your model is compatible with the NPU. The conversion process involves selecting the appropriate hardware type, which can include the CPU and GPU, and ensuring that the model is quantized if it is to be executed on the NPU. After confirming your selections, you can initiate the conversion process and download the converted NNC model file for further use.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What steps are involved in using the Exynos AI Studio Service to convert a TFLite model into an NNC model, and what limitations exist regarding project creation?",
    "reference_contexts": [
      "<1-hop>\n\nTools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)",
      "<2-hop>\n\nTroubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For"
    ],
    "reference": "To use the Exynos AI Studio Service for converting a TFLite model into an NNC model, follow these steps: First, access the ENN Eco-system by signing up for a new account or logging in as an existing user. Next, navigate to the Exynos AI Studio service page and provide project information by entering a descriptive title for your project. Then, upload your TFLite model and choose your hardware preferences, either utilizing only the CPU and GPU or engaging the NPU as an additional accelerator, keeping in mind that the NPU does not support all layers. After confirming your selections, click 'Convert' to initiate the model conversion process. If successful, you can download the converted NNC model. However, it is important to note that users can create a maximum of five projects within the Exynos AI Studio Service.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How do I convert a TFLITE model using the Exynos AI Studio Service?",
    "reference_contexts": [
      "<1-hop>\n\nTools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)",
      "<2-hop>\n\nTroubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For"
    ],
    "reference": "To convert a TFLITE model using the Exynos AI Studio Service, first, access the ENN Eco-system by signing up or logging in. Then, navigate to the Exynos AI Studio service page and enter a descriptive title for your project. Upload your TFLite model using the provided interface. Choose your hardware preferences, either default (using only CPU and GPU) or accelerate (engaging the NPU as an additional accelerator). After confirming your selections, click 'Convert' to start the model conversion process. If successful, the NNC Download button will be enabled, allowing you to download the converted NNC model.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What steps are involved in using the Exynos AI Studio Service to convert a TFLite model into an NNC model?",
    "reference_contexts": [
      "<1-hop>\n\nTools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)",
      "<2-hop>\n\nTroubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For"
    ],
    "reference": "To use the Exynos AI Studio Service for converting a TFLite model into an NNC model, follow these steps: First, access the ENN Eco-system by signing up for a new account or logging in if you are an existing user. Next, navigate to the Exynos AI Studio service page and provide project information by entering a descriptive title for your project. Then, upload your TFLite model using the provided interface. After that, choose your hardware preferences, either the default option which utilizes only the CPU and GPU, or the accelerate option which engages the NPU as an additional accelerator, keeping in mind that the NPU does not support all layers. Once your selections are confirmed, click 'Convert' to initiate the model conversion process. If the conversion is successful, the NNC Download button will be enabled, allowing you to download the converted NNC model.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps involved in converting a TensorFlow Lite model to an NNC model using the Exynos AI Studio, and how does the ENN framework facilitate the execution of the converted NNC model?",
    "reference_contexts": [
      "<1-hop>\n\nTools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)",
      "<2-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "To convert a TensorFlow Lite model to an NNC model using the Exynos AI Studio, the following steps are involved: First, the user must prepare a pre-trained NN model and set parameters for the conversion tools. Then, the user executes the tools for conversion, which includes uploading the TFLite model and selecting hardware preferences. Once the conversion is successful, the NNC model can be downloaded. After conversion, the execution of the NNC model is facilitated by the ENN framework. This involves initializing the ENN framework, loading the converted model, allocating and committing necessary buffers, copying input data to input buffers, executing the model, and using data from output buffers. This process can be repeated for multiple executions, and finally, the buffers are released, and the model is unloaded.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How do you convert a TensorFlow Lite model to an NNC model using the Exynos AI Studio service, and what are the requirements for the dataset format?",
    "reference_contexts": [
      "<1-hop>\n\nTools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)",
      "<2-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "To convert a TensorFlow Lite model to an NNC model using the Exynos AI Studio service, you need to follow these steps: First, access the ENN Eco-system by signing up or logging in. Then, navigate to the Exynos AI Studio service page and provide project information by entering a descriptive title and uploading your TFLite model. Choose your hardware preferences, either default (using only CPU and GPU) or accelerate (engaging the NPU as an additional accelerator). After confirming your selections, click 'Convert' to start the model conversion process. If successful, you can download the NNC model using the NNC Download button. Regarding the dataset format, it must use the .h5 file extension, and the keys in the h5 dataset should match the input names of the model for proper mapping. Additionally, the internal data within the h5 file must be in float32 format, and the h5 file should be placed directly under the DATA folder generated after executing the 'enntools init' command.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How can I convert a TensorFlow Lite model to an ONNX model using Exynos AI Studio, and what are the supported models and operators for this conversion?",
    "reference_contexts": [
      "<1-hop>\n\nTroubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For",
      "<2-hop>\n\ndescribes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "To convert a TensorFlow Lite model to an ONNX model using Exynos AI Studio, you can use the TF2ONNX Converter, which requires an ONNX opset for exporting. Exynos AI Studio supports TensorFlow Lite models, and it is recommended to convert TFLite models using MLIR version 1.14 or higher. The supported models include Tensors up to four dimensions and models with a maximum size of 1 GB. Additionally, for NPU, the models must be quantized. The supported operators in Exynos AI Studio include various operations such as ADD, CONVOLUTION, and MAXPOOL, among others, which are compatible with the conversion process.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How can I convert a TensorFlow Lite model to an ONNX model using Exynos AI Studio, and what are the supported models for this conversion?",
    "reference_contexts": [
      "<1-hop>\n\nTroubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For",
      "<2-hop>\n\ndescribes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "To convert a TensorFlow Lite model to an ONNX model using Exynos AI Studio, you can use the TF2ONNX Converter, which requires an ONNX opset for exporting. Exynos AI Studio supports TensorFlow Lite models, and it is recommended to convert TFLite models using MLIR version 1.14 or higher. Additionally, the supported models must have tensors up to four dimensions and a maximum size of 1 GB. For NPU, the models must also be quantized.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps to convert a TensorFlow Lite model to a Neural Network Container (NNC) model using Exynos AI Studio, and how does the Exynos AI Studio support TensorFlow models?",
    "reference_contexts": [
      "<1-hop>\n\nTroubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For",
      "<2-hop>\n\ndescribes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "To convert a TensorFlow Lite model to a Neural Network Container (NNC) model using Exynos AI Studio, follow these steps: First, access the ENN Ecosystem Portal and create an account if you are a new user. Then, navigate to the Exynos AI Studio Service page and enter a descriptive title for your model. You can use the Exynos AI Studio service to convert trained TFLite models by utilizing the conversion tools provided. The Exynos AI Studio supports TensorFlow models by allowing users to convert TensorFlow (TF) or TensorFlow Lite (TFLITE) models to ONNX format using the TF2ONNX Converter. This converter requires an ONNX opset for exporting. Additionally, Exynos AI Studio supports models with a maximum size of 1 GB and recommends using MLIR version 1.14 or higher for converting TFLite models.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the key functions of the ENN framework as described in the context, and how do they relate to the usage of Exynos AI Studio for model conversion?",
    "reference_contexts": [
      "<1-hop>\n\nIntroduction EHT Converter Optimizer Quantizer Simulator Model Optimization Flow ELT Model Analyzer SCVT Quantizer NPU Simulator Performance Estimator Graph Generator Dataset and Model Quick Start System Requirements How to use ENN Framework Data Type References _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; API reference ENN Framework Data Type References _NumberOfBuffersInfo _ennBuffer _ennBufferInfo ENN Framwork API Functions Context initialize / deinitialize OpenModel / CloseModel related Memory Handling Setters and Getters for model Commit Buffer Execute Models Security, preference, get meta information.. title: _ennBuffer _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; Updated on 2023-08-11 at 16:24:05 +0900 title: _ennBufferInfo _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; Updated on 2023-08-11 at 16:24:05 +0900 title: _NumberOfBuffersInfo _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; Updated on 2023-08-11 at 16:24:05 +0900 title: Commit Buffer Commit Buffer Functions Name EnnReturn EnnBufferCommit (const EnnModelId model_id, const int session_id =0) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Functions Documentation function EnnBufferCommit EnnReturn EnnBufferCommit( const EnnModelId model_id, const int session_id =0 ) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Parameters: model_id [IN] model ID from load_model Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Context initialize / deinitialize Context initialize / deinitialize Functions Name EnnReturn EnnInitialize (void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. EnnReturn EnnDeinitialize (void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Functions Documentation function EnnInitialize EnnReturn EnnInitialize( void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. Return: EnnReturn result, 0 is success function EnnDeinitialize EnnReturn EnnDeinitialize( void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Security, preference, get meta information.. Security, preference, get meta information.. Functions Name EnnReturn EnnSetPreferencePresetId (const uint32_t val) Setting Preset ID for operation performance. EnnReturn EnnSetPreferencePerfConfigId (const uint32_t val) Setting PerfConfig ID for operation performance. EnnReturn EnnSetPreferencePerfMode (const uint32_t val) Setting Performance Mode. EnnReturn EnnSetPreferenceTimeOut (const uint32_t val) Setting Preset ID for time out. EnnReturn EnnSetPreferencePriority (const uint32_t val) Setting priority value for NPU. EnnReturn EnnSetPreferenceCoreAffinity (const uint32_t val) Setting affinity to set NPU core operation. EnnReturn EnnGetPreferencePresetId (uint32_t * val_ptr) Get current information for Preset ID. EnnReturn EnnGetPreferencePerfConfigId (uint32_t * val_ptr) Get current information for PerfConfig ID. EnnReturn EnnGetPreferencePerfMode (uint32_t * val_ptr) Get current information for Performance Mode. EnnReturn EnnGetPreferenceTimeOut (uint32_t * val_ptr) Get current information for Time Out. EnnReturn EnnGetPreferencePriority (uint32_t * val_ptr) Get current information for NPU Priority. EnnReturn EnnGetPreferenceCoreAffinity (uint32_t * val_ptr) Get current information for NPU Core affinity. EnnReturn EnnGetMetaInfo (const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]) Get Meta Information. EnnReturn EnnSetExecMsgAlwaysOn () Set frequency of execution message print. Functions Documentation function EnnSetPreferencePresetId EnnReturn EnnSetPreferencePresetId( const uint32_t val ) Setting Preset ID for operation performance. Parameters: val [IN] value to set preset ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfConfigId EnnReturn EnnSetPreferencePerfConfigId( const uint32_t val ) Setting PerfConfig ID for operation performance. Parameters: val [IN] value to set PerfConfig ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfMode EnnReturn EnnSetPreferencePerfMode( const uint32_t val ) Setting Performance Mode. Parameters: val [IN] value to set Performance Mode Return: EnnReturn result, 0 is success function EnnSetPreferenceTimeOut EnnReturn EnnSetPreferenceTimeOut( const uint32_t val ) Setting Preset ID for time out. Parameters: val [IN] value to set time out Return: EnnReturn result, 0 is success Note: in second function EnnSetPreferencePriority EnnReturn EnnSetPreferencePriority( const uint32_t val ) Setting priority value for NPU. Parameters: val [IN] value to set NPU job priority Return: EnnReturn result, 0 is success function EnnSetPreferenceCoreAffinity EnnReturn EnnSetPreferenceCoreAffinity( const uint32_t val ) Setting affinity to set NPU core operation. Parameters: val [IN] value to set affinity Return: EnnReturn result, 0 is success Note: in second function EnnGetPreferencePresetId EnnReturn EnnGetPreferencePresetId( uint32_t * val_ptr ) Get current information for Preset ID. Parameters: val [OUT] current value of Preset ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfConfigId EnnReturn EnnGetPreferencePerfConfigId( uint32_t * val_ptr ) Get current information for PerfConfig ID. Parameters: val [OUT] current value of PerfConfig ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfMode EnnReturn EnnGetPreferencePerfMode( uint32_t * val_ptr ) Get current information for Performance Mode. Parameters: val [OUT] current value of Performance Mode Return: EnnReturn result, 0 is success function EnnGetPreferenceTimeOut EnnReturn EnnGetPreferenceTimeOut( uint32_t * val_ptr ) Get current information for Time Out. Parameters: val [OUT] current value of Time Out Return: EnnReturn result, 0 is success function EnnGetPreferencePriority EnnReturn EnnGetPreferencePriority( uint32_t * val_ptr ) Get current information for NPU Priority. Parameters: val [OUT] current value of NPU Priority Return:",
      "<2-hop>\n\ndescribes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "The ENN framework provides several key functions including context initialization and deinitialization, model execution, and buffer management. Specifically, functions like EnnInitialize and EnnDeinitialize are used to manage the framework's context within a caller's process. Additionally, the framework includes functions for committing buffers to the service core, which is essential for executing models. This relates to the usage of Exynos AI Studio, as the studio provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models, which can then be executed using the ENN framework. The integration of these functions ensures that models are efficiently processed and optimized for deployment on Exynos hardware.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the key functions of the ENN framework as described in the context, and how do they relate to the Exynos Neural Network Software Development Kit?",
    "reference_contexts": [
      "<1-hop>\n\nIntroduction EHT Converter Optimizer Quantizer Simulator Model Optimization Flow ELT Model Analyzer SCVT Quantizer NPU Simulator Performance Estimator Graph Generator Dataset and Model Quick Start System Requirements How to use ENN Framework Data Type References _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; API reference ENN Framework Data Type References _NumberOfBuffersInfo _ennBuffer _ennBufferInfo ENN Framwork API Functions Context initialize / deinitialize OpenModel / CloseModel related Memory Handling Setters and Getters for model Commit Buffer Execute Models Security, preference, get meta information.. title: _ennBuffer _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; Updated on 2023-08-11 at 16:24:05 +0900 title: _ennBufferInfo _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; Updated on 2023-08-11 at 16:24:05 +0900 title: _NumberOfBuffersInfo _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; Updated on 2023-08-11 at 16:24:05 +0900 title: Commit Buffer Commit Buffer Functions Name EnnReturn EnnBufferCommit (const EnnModelId model_id, const int session_id =0) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Functions Documentation function EnnBufferCommit EnnReturn EnnBufferCommit( const EnnModelId model_id, const int session_id =0 ) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Parameters: model_id [IN] model ID from load_model Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Context initialize / deinitialize Context initialize / deinitialize Functions Name EnnReturn EnnInitialize (void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. EnnReturn EnnDeinitialize (void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Functions Documentation function EnnInitialize EnnReturn EnnInitialize( void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. Return: EnnReturn result, 0 is success function EnnDeinitialize EnnReturn EnnDeinitialize( void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Security, preference, get meta information.. Security, preference, get meta information.. Functions Name EnnReturn EnnSetPreferencePresetId (const uint32_t val) Setting Preset ID for operation performance. EnnReturn EnnSetPreferencePerfConfigId (const uint32_t val) Setting PerfConfig ID for operation performance. EnnReturn EnnSetPreferencePerfMode (const uint32_t val) Setting Performance Mode. EnnReturn EnnSetPreferenceTimeOut (const uint32_t val) Setting Preset ID for time out. EnnReturn EnnSetPreferencePriority (const uint32_t val) Setting priority value for NPU. EnnReturn EnnSetPreferenceCoreAffinity (const uint32_t val) Setting affinity to set NPU core operation. EnnReturn EnnGetPreferencePresetId (uint32_t * val_ptr) Get current information for Preset ID. EnnReturn EnnGetPreferencePerfConfigId (uint32_t * val_ptr) Get current information for PerfConfig ID. EnnReturn EnnGetPreferencePerfMode (uint32_t * val_ptr) Get current information for Performance Mode. EnnReturn EnnGetPreferenceTimeOut (uint32_t * val_ptr) Get current information for Time Out. EnnReturn EnnGetPreferencePriority (uint32_t * val_ptr) Get current information for NPU Priority. EnnReturn EnnGetPreferenceCoreAffinity (uint32_t * val_ptr) Get current information for NPU Core affinity. EnnReturn EnnGetMetaInfo (const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]) Get Meta Information. EnnReturn EnnSetExecMsgAlwaysOn () Set frequency of execution message print. Functions Documentation function EnnSetPreferencePresetId EnnReturn EnnSetPreferencePresetId( const uint32_t val ) Setting Preset ID for operation performance. Parameters: val [IN] value to set preset ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfConfigId EnnReturn EnnSetPreferencePerfConfigId( const uint32_t val ) Setting PerfConfig ID for operation performance. Parameters: val [IN] value to set PerfConfig ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfMode EnnReturn EnnSetPreferencePerfMode( const uint32_t val ) Setting Performance Mode. Parameters: val [IN] value to set Performance Mode Return: EnnReturn result, 0 is success function EnnSetPreferenceTimeOut EnnReturn EnnSetPreferenceTimeOut( const uint32_t val ) Setting Preset ID for time out. Parameters: val [IN] value to set time out Return: EnnReturn result, 0 is success Note: in second function EnnSetPreferencePriority EnnReturn EnnSetPreferencePriority( const uint32_t val ) Setting priority value for NPU. Parameters: val [IN] value to set NPU job priority Return: EnnReturn result, 0 is success function EnnSetPreferenceCoreAffinity EnnReturn EnnSetPreferenceCoreAffinity( const uint32_t val ) Setting affinity to set NPU core operation. Parameters: val [IN] value to set affinity Return: EnnReturn result, 0 is success Note: in second function EnnGetPreferencePresetId EnnReturn EnnGetPreferencePresetId( uint32_t * val_ptr ) Get current information for Preset ID. Parameters: val [OUT] current value of Preset ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfConfigId EnnReturn EnnGetPreferencePerfConfigId( uint32_t * val_ptr ) Get current information for PerfConfig ID. Parameters: val [OUT] current value of PerfConfig ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfMode EnnReturn EnnGetPreferencePerfMode( uint32_t * val_ptr ) Get current information for Performance Mode. Parameters: val [OUT] current value of Performance Mode Return: EnnReturn result, 0 is success function EnnGetPreferenceTimeOut EnnReturn EnnGetPreferenceTimeOut( uint32_t * val_ptr ) Get current information for Time Out. Parameters: val [OUT] current value of Time Out Return: EnnReturn result, 0 is success function EnnGetPreferencePriority EnnReturn EnnGetPreferencePriority( uint32_t * val_ptr ) Get current information for NPU Priority. Parameters: val [OUT] current value of NPU Priority Return:",
      "<2-hop>\n\ndescribes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "The ENN framework provides several key functions including context initialization and deinitialization, model execution, and buffer management. Specifically, functions like EnnInitialize and EnnDeinitialize are used to manage the framework's context within a caller's process. Additionally, the framework includes functions for committing buffers to the service core, such as EnnBufferCommit, which sends a buffer-set to the service core for processing. These functions are essential for optimizing neural network models for deployment on Exynos hardware, as they facilitate the conversion of Neural Network (NN) models to Neural Network Container (NNC) models, enabling efficient execution on Exynos devices.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is the role of the ENN framework in converting Neural Network models to NNC models and how does it relate to the Exynos AI Studio?",
    "reference_contexts": [
      "<1-hop>\n\nIntroduction EHT Converter Optimizer Quantizer Simulator Model Optimization Flow ELT Model Analyzer SCVT Quantizer NPU Simulator Performance Estimator Graph Generator Dataset and Model Quick Start System Requirements How to use ENN Framework Data Type References _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; API reference ENN Framework Data Type References _NumberOfBuffersInfo _ennBuffer _ennBufferInfo ENN Framwork API Functions Context initialize / deinitialize OpenModel / CloseModel related Memory Handling Setters and Getters for model Commit Buffer Execute Models Security, preference, get meta information.. title: _ennBuffer _ennBuffer Public Attributes Name void * va uint32_t size uint32_t offset Public Attributes Documentation variable va cpp void * va; variable size cpp uint32_t size; variable offset cpp uint32_t offset; Updated on 2023-08-11 at 16:24:05 +0900 title: _ennBufferInfo _ennBufferInfo Public Attributes Name bool is_able_to_update uint32_t n uint32_t width uint32_t height uint32_t channel uint32_t size uint32_t buffer_type const char * label Public Attributes Documentation variable is_able_to_update cpp bool is_able_to_update; variable n cpp uint32_t n; variable width cpp uint32_t width; variable height cpp uint32_t height; variable channel cpp uint32_t channel; variable size cpp uint32_t size; variable buffer_type cpp uint32_t buffer_type; variable label cpp const char * label; Updated on 2023-08-11 at 16:24:05 +0900 title: _NumberOfBuffersInfo _NumberOfBuffersInfo Public Attributes Name uint32_t n_in_buf uint32_t n_out_buf Public Attributes Documentation variable n_in_buf cpp uint32_t n_in_buf; variable n_out_buf cpp uint32_t n_out_buf; Updated on 2023-08-11 at 16:24:05 +0900 title: Commit Buffer Commit Buffer Functions Name EnnReturn EnnBufferCommit (const EnnModelId model_id, const int session_id =0) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Functions Documentation function EnnBufferCommit EnnReturn EnnBufferCommit( const EnnModelId model_id, const int session_id =0 ) Send buffer-set to service core. session_id indicates which buffer space should be sent. The committed buffers are released if a caller calls CloseModel() Parameters: model_id [IN] model ID from load_model Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Context initialize / deinitialize Context initialize / deinitialize Functions Name EnnReturn EnnInitialize (void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. EnnReturn EnnDeinitialize (void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Functions Documentation function EnnInitialize EnnReturn EnnInitialize( void ) Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair. Return: EnnReturn result, 0 is success function EnnDeinitialize EnnReturn EnnDeinitialize( void ) Deinitialize Enn Framework. Framework degenerates context in a caller's process. Return: EnnReturn result, 0 is success Updated on 2023-08-11 at 16:24:05 +0900 title: Security, preference, get meta information.. Security, preference, get meta information.. Functions Name EnnReturn EnnSetPreferencePresetId (const uint32_t val) Setting Preset ID for operation performance. EnnReturn EnnSetPreferencePerfConfigId (const uint32_t val) Setting PerfConfig ID for operation performance. EnnReturn EnnSetPreferencePerfMode (const uint32_t val) Setting Performance Mode. EnnReturn EnnSetPreferenceTimeOut (const uint32_t val) Setting Preset ID for time out. EnnReturn EnnSetPreferencePriority (const uint32_t val) Setting priority value for NPU. EnnReturn EnnSetPreferenceCoreAffinity (const uint32_t val) Setting affinity to set NPU core operation. EnnReturn EnnGetPreferencePresetId (uint32_t * val_ptr) Get current information for Preset ID. EnnReturn EnnGetPreferencePerfConfigId (uint32_t * val_ptr) Get current information for PerfConfig ID. EnnReturn EnnGetPreferencePerfMode (uint32_t * val_ptr) Get current information for Performance Mode. EnnReturn EnnGetPreferenceTimeOut (uint32_t * val_ptr) Get current information for Time Out. EnnReturn EnnGetPreferencePriority (uint32_t * val_ptr) Get current information for NPU Priority. EnnReturn EnnGetPreferenceCoreAffinity (uint32_t * val_ptr) Get current information for NPU Core affinity. EnnReturn EnnGetMetaInfo (const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX]) Get Meta Information. EnnReturn EnnSetExecMsgAlwaysOn () Set frequency of execution message print. Functions Documentation function EnnSetPreferencePresetId EnnReturn EnnSetPreferencePresetId( const uint32_t val ) Setting Preset ID for operation performance. Parameters: val [IN] value to set preset ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfConfigId EnnReturn EnnSetPreferencePerfConfigId( const uint32_t val ) Setting PerfConfig ID for operation performance. Parameters: val [IN] value to set PerfConfig ID Return: EnnReturn result, 0 is success function EnnSetPreferencePerfMode EnnReturn EnnSetPreferencePerfMode( const uint32_t val ) Setting Performance Mode. Parameters: val [IN] value to set Performance Mode Return: EnnReturn result, 0 is success function EnnSetPreferenceTimeOut EnnReturn EnnSetPreferenceTimeOut( const uint32_t val ) Setting Preset ID for time out. Parameters: val [IN] value to set time out Return: EnnReturn result, 0 is success Note: in second function EnnSetPreferencePriority EnnReturn EnnSetPreferencePriority( const uint32_t val ) Setting priority value for NPU. Parameters: val [IN] value to set NPU job priority Return: EnnReturn result, 0 is success function EnnSetPreferenceCoreAffinity EnnReturn EnnSetPreferenceCoreAffinity( const uint32_t val ) Setting affinity to set NPU core operation. Parameters: val [IN] value to set affinity Return: EnnReturn result, 0 is success Note: in second function EnnGetPreferencePresetId EnnReturn EnnGetPreferencePresetId( uint32_t * val_ptr ) Get current information for Preset ID. Parameters: val [OUT] current value of Preset ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfConfigId EnnReturn EnnGetPreferencePerfConfigId( uint32_t * val_ptr ) Get current information for PerfConfig ID. Parameters: val [OUT] current value of PerfConfig ID Return: EnnReturn result, 0 is success function EnnGetPreferencePerfMode EnnReturn EnnGetPreferencePerfMode( uint32_t * val_ptr ) Get current information for Performance Mode. Parameters: val [OUT] current value of Performance Mode Return: EnnReturn result, 0 is success function EnnGetPreferenceTimeOut EnnReturn EnnGetPreferenceTimeOut( uint32_t * val_ptr ) Get current information for Time Out. Parameters: val [OUT] current value of Time Out Return: EnnReturn result, 0 is success function EnnGetPreferencePriority EnnReturn EnnGetPreferencePriority( uint32_t * val_ptr ) Get current information for NPU Priority. Parameters: val [OUT] current value of NPU Priority Return:",
      "<2-hop>\n\ndescribes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "The ENN framework plays a crucial role in converting Neural Network (NN) models to Neural Network Container (NNC) models as described in the Exynos AI Studio documentation. It provides the necessary instructions for model conversion, execution, and output retrieval. The framework includes various functions and data types that facilitate the optimization and performance testing of neural network models on Samsung Exynos hardware.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How can I convert a TensorFlow Lite model to an NNC model using Exynos AI Studio, and what are the requirements for the model format?",
    "reference_contexts": [
      "<1-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3.",
      "<2-hop>\n\nTroubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For"
    ],
    "reference": "To convert a TensorFlow Lite model to an NNC model using Exynos AI Studio, you need to prepare a pre-trained NN model in ONNX format, as the model should be optimized for efficient execution on Samsung Exynos hardware. The conversion process involves using the tools provided by Exynos AI Studio, where you set parameters for the tools and execute them for conversion. Additionally, the model must adhere to the requirements that it supports TensorFlow Lite operations and can handle tensors with up to four dimensions, with a maximum model size of 1 GB.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How can I convert a TensorFlow Lite model to an NNC model using Exynos AI Studio, and what are the key features of TensorFlow Lite that support this process?",
    "reference_contexts": [
      "<1-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3.",
      "<2-hop>\n\nTroubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For"
    ],
    "reference": "To convert a TensorFlow Lite model to an NNC model using Exynos AI Studio, you need to prepare a pre-trained NN model in TensorFlow Lite format. The conversion process involves setting parameters for the tools provided by Exynos AI Studio and executing these tools for conversion. Key features of TensorFlow Lite that support this process include its compatibility with models that have tensors up to four dimensions and a maximum size of 1 GB. Additionally, Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models efficiently for deployment on Samsung Exynos hardware.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps involved in converting TensorFlow Lite models to NNC models using Exynos AI Studio, and how does this process relate to the optimization techniques provided by the EHT?",
    "reference_contexts": [
      "<1-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3.",
      "<2-hop>\n\nTroubleshooting FAQs Following are the responses to some of the most frequently asked questions: 1. How do I use Exynos AI Studio Service? The Exynos AI Studio service section provides detailed information on using the Exynos AI Studio service. 2. How many projects can I create in Exynos AI Studio Service? Users can create a maximum of five projects with the Exynos AI Studio service. 3. Is Exynos AI Studio service a paid service? The Exynos AI Studio service is currently free. Reporting Exynos AI Studio Issues We encourage you to share general questions, feedbacks, or suspected bugs related to the Exynos AI Studio on our forums for public discussion. If you prefer a more direct approach or need personalized assistance, submit your concerns to our 1:1 Support. Introduction to Exynos AI High-Level Toolchain (EHT) EHT is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models. System overview diagram Converter Optimizer Quantizer Simulator Model Optimization Flow Converter Converter enables model conversion between various intermediate representations (IRs). Through this converter, users can apply the EHT module's optimization features to their PyTorch and TensorFlow models, and ultimately convert their optimized CNNX model into an SNC model. This module consists of the four parts below: - PyTorch2ONNX Converter - Converts (exports) a PyTorch model to ONNX. - Requires input dimension information: - Input shapes to export the model. - Dynamic axes to remain dynamic after the export. - Requires an ONNX opset for exporting. - TF2ONNX Converter - Converts a TF (or TFLITE) model to ONNX. - Requires an ONNX opset for exporting. - ONNX2CNNX Converter - Converts an ONNX model to CNNX. - Requires specification of both the output model path and the output encodings path. - CNNX2SNC Converter - Converts a CNNX model to SNC. Example codes to use them: ```python from converter import api cnnx_to_snc_params = api.Cnnx2SncParameters( input_model_path = \"/path/to/model.onnx\", input_encodings_path = \"/path/to/model.encodings\", output_model_path = \"/output/path/for/model.snc\" ) api.Converter.cnnx_to_snc(cnnx_to_snc_params) tflite_to_onnx_params = api.TfLite2OnnxParameters( input_model_path = \"/path/to/model.tflite\", output_model_path = \"./output/path/for/model.onnx\", ) api.Converter.tflite_to_onnx(tflite_to_onnx_params) ``` Quantizer Quantizer is a module in EHT that applies basic and advanced quantization methods to input models. Basic methods include fixed precision quantization and mixed precision quantization (MPQ). Users can apply mixed precision quantization to models by specifying activation names or operators. Quantizer also provides a MPQ debug API. Basic Quantization Methods Fixed Precision Quantization Applies static uniform quantization to both weights and activations. Users can specify the precision (bit-width) for weights and activations. The entire model is quantized to the specified precision. Mixed Precision Quantization Allows different parts of the model to use different precisions Two approaches are supported: - Mixed precision by name: Users specify precisions for specific activation or weight names - Mixed precision by operator: Users define precisions for different types of operators. For example, if set the Add operator to INT4 then all outputs of the Add operators are quantized to INT4 Advanced Quantization Methods Currently supported advanced quantization methods are as follows. Softmax Bias Correction (https://arxiv.org/abs/2309.01729) A method that add a float bias to Softmax output layer to reduce the performance degradation caused by quantization SmoothQuant (https://arxiv.org/abs/2211.10438) A method that smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation Cross Layer Equalization (https://arxiv.org/abs/1906.04721) A method that tune the weights range of the channels in one tensor to reduce quantization error Dummy Convolution for the Concat Operator The dummy conv algorithm can be applied during the fixed or mixed precision quantization process. It adds a dummy convolution operator to the long residual input activations of the concat operator by a specific algorithm to maintain the long residual input activation scales. After applying this method, the model perfromance could be improved. Debug API Layer-wise mixed precision quantiztion debug API The quantizer provides a mixed precision quantization debug API. Users can select different activations and weights and assign them varying levels of precision. The input includes a quantized CNNX model and a debug dictionary. Simulator Simulator conducts inference on a CNNX model, which contains an ONNX model file and a quantization information encodings file. It can check output and intermediate tensors during execution. Quantized inference is performed using simulated quantization. Simulated quantization provides the capability to mimic the effects of quantized operations. Floating point values are clipped and divided into several ranges, and values within each range are converted to the same value, simulating the quantization process. Simulator includes the following features(with example codes): - get_quantization_sim: Attain a CNNX quantization simulation session for manual run. python from simulator imoprt api output_names = [LIST_OF_ONNX_OUTPUTS] input_dict = {\"input_0\": NUMPY_ARRAY_0, \"input_1\": NUMPY_ARRAY_1] quantsim = api.get_quantization_sim(params) inference_session = quantsim.session result = session.run(output_names, input_dict) print(result) load_input_data: Load the input data from the given dataset path. run_inference: Conduct inference on a CNNX model. compare_model_by_inference: Compare two CNNX models on their inference outputs. Currently, the SNR value is used for the comparison metric. compare_model_by_layer : Compare two CNNX models on intermediate tensors for each layer. ```python from simulator imoprt api params = api.LayerwiseCheckerParameters( input_data_path = \"/path/to/input/data/, input_model_path_0 = \"/path/to/model/0.onnx\", input_encodings_path_0 = \"/path/to/model/0.encodings\", input_model_path_1 = \"/path/to/model/0.onnx\", input_encodings_path_1 = \"/path/to/model/0.encodings\", metric = \"snr\", threshold = 100, use_cuda = False, layer_match_info = [(\"featuremap_0\", \"featuremap_0\"), (\"featuremap_1\",\"featuremap_1\")] export_featuremap = True, export_featuremap_path = \"/path/to/save/exported/featuremap\" ) res = api.compare_model_by_layer(params) print(\"Layerwise check\") for input_name, result in res.items(): _, result_dict = result for output_name, snr_value in result_dict.items(): print(f\"{output_name}: {snr_value}\") ``` Exynos AI Studio Exynos AI Studio Service Use the Exynos AI Studio Service to convert trained TFLite models to NNC models by performing the following steps. Preparation Access the ENN Ecosystem Portal If you are a new user, sign up to create an account. If you are a returning user, log in to ENN eco-system Download Inception v4 .tflite model from here. Project Creation Navigate to the Exynos AI Studio Service page. Enter a descriptive title for your model. For"
    ],
    "reference": "To convert TensorFlow Lite models to NNC models using Exynos AI Studio, users must follow these steps: First, prepare a pre-trained neural network model in TensorFlow Lite format. Next, set the parameters for the conversion tools provided by Exynos AI Studio. Finally, execute the tools to perform the conversion. This process is closely related to the optimization techniques provided by the EHT, which includes functionalities such as quantization and model optimization. The EHT allows users to apply various optimization techniques to their models before conversion, ensuring that the resulting NNC models run efficiently on Samsung Exynos hardware.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps involved in converting a neural network model to NNC format and how does the NPU core affinity affect the execution of the model?",
    "reference_contexts": [
      "<1-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :",
      "<2-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "To convert a neural network model to NNC format, the user must first prepare a pre-trained NN model and set parameters for the conversion tools. After executing the tools for conversion, the converted NNC model can be downloaded. The NPU core affinity affects the execution of the model by determining which cores are utilized during the model's execution. The function EnnGetPreferenceCoreAffinity retrieves the current information for NPU Core affinity, which is crucial for optimizing the model's performance on Samsung Exynos hardware.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps to convert a Neural Network (NN) model to a Neural Network Container (NNC) model and what is the significance of the NNC format in relation to Samsung Exynos hardware?",
    "reference_contexts": [
      "<1-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :",
      "<2-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "To convert a Neural Network (NN) model to a Neural Network Container (NNC) model, the following steps are involved: First, prepare a pre-trained NN model. Next, set parameters for the conversion tools provided by Exynos AI Studio. Finally, execute the tools for conversion. The significance of the NNC format is that it allows NN models to run efficiently on Samsung Exynos hardware, optimizing performance during inference.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps involved in converting NN models to NNC models and how does the ENN framework facilitate model execution?",
    "reference_contexts": [
      "<1-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :",
      "<2-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3."
    ],
    "reference": "To convert NN models to NNC models, the user must first prepare a pre-trained NN model, set parameters for the conversion tools, and then execute these tools for conversion. Once the model is converted to the NNC format, the ENN framework facilitates model execution by initializing the framework, loading the converted model, allocating and committing necessary buffers, copying input data to these buffers, executing the model, and finally using the data from the output buffers. After execution, the user must uncommit and release the buffers, unload the model, and deinitialize the ENN framework.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What steps must be taken to convert a TFLite model into an NNC model using the Exynos AI Studio service, and what considerations should be made regarding hardware preferences?",
    "reference_contexts": [
      "<1-hop>\n\nTools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)",
      "<2-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "To convert a TFLite model into an NNC model using the Exynos AI Studio service, follow these steps: First, access the ENN Eco-system by signing up or logging in. Then, navigate to the Exynos AI Studio service page and provide project information by entering a descriptive title and uploading your TFLite model. Next, choose your hardware preferences: the default option utilizes only the CPU and GPU, while the 'Accelerate' option engages the NPU as an additional accelerator. However, it is important to note that the NPU does not support all layers, and using unsupported layers may lead to complications. After confirming your selections, click 'Convert' to initiate the conversion process. If successful, the NNC Download button will be enabled, allowing you to download the converted NNC model.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How do you convert TFLite models into NNC models using the Exynos AI Studio service, and what should you be aware of regarding the NPU's support for layers?",
    "reference_contexts": [
      "<1-hop>\n\nTools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)",
      "<2-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "To convert TFLite models into NNC models using the Exynos AI Studio service, you first need to access the ENN Eco-system by signing up or logging in. Then, navigate to the Exynos AI Studio service page and provide project information by entering a descriptive title and uploading your TFLite model. You can choose hardware preferences, either the default option which utilizes only the CPU and GPU, or the 'Accelerate' option which engages the NPU as an additional accelerator. However, it is important to note that the NPU does not support all layers, and using unsupported layers may lead to complications. After confirming your selections, you can initiate the conversion process by clicking 'Convert'. Once the conversion is successful, the NNC Download button will be enabled, allowing you to download the converted NNC model.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What steps are involved in converting a TFLite model to an NNC model using the Exynos AI Studio service, and what considerations should be made regarding hardware preferences?",
    "reference_contexts": [
      "<1-hop>\n\nTools Exynos AI Studio Service The Exynos AI Studio service is an online platform designed to enable users to convert TFLite models into NNC models. To utilize this service: Access the ENN Eco-system: If you are a new user, sign up to create an account. If you are an existing user, log in to ENN eco-system. Navigate to the Service: Visit the Exynos AI Studio service page. Provide Project Information: Enter a descriptive title for your project. Use the provided interface to upload your TFLite model. Choose Hardware Preferences: Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warning: The NPU does not support all layers. Using unsupported layers may lead to complications. For more information on the detailed list, refer to Support Matix. Initiate the Conversion: Click Confirm to verify your selections. Click Convert to start the model conversion process. Download the Converted Model: If the conversion is successful, the NNC Download button is enabled. Click NNC Download to download the NNC model. Integrate the downloaded NNC model into your application required. 4. ENN tensor Framework APIs Data Type References For more information on the list of data types, refer to API Reference. API Functions For more information on the list of API functions, refer to API Reference. 5. Advanced Topics Model Design Tips Channel Alignment Maintaining a channel number that aligns with the specified architecture ensures effective utilization of NPU resources. Architecture Channel Alignment Gen-4 32 Bilinear Resize Parameters To ensure optimal image resizing using bilinear interpolation, configure the following settings: Option A: Aligned corner: False Half pixel centers: False Performance: High speed Option B: Aligned corner: True Half pixel centers: False Compatibility: Gen-4 and later NPUs Performance: Medium speed Option C: Aligned corner: False Half pixel centers: True Note: Requires workaround Performance: Reduced speed Data Processing Procedures Pre-processing and Post-processing: Implement these tasks outside the main model computation. For efficient execution, it is recommended to use parallel processing on GPU or CPU. Memory Alignment: Includes data transformation operations such as split and reshape during the pre-processing phase to ensure proper data alignment. Layer Adjustments To enhance performance, it is recommended to exclude the dropout layer. PReLU Use the PReLU activation function for optimal performance. Although LeakyReLU is functional, it may not provide the same level of efficiency. Sharing IFM and OFM Recursively Merge successive concat layers that share the same Input Feature Maps (IFM) and Output Feature Maps (OFM). 6. or Intel Core i7 processor with 4 cores or more Architecture: 64-bit (x86-64) Minimum specifications: 2GHz dual-core processor Architecture: 64-bit (x86-64) Memory Minimum 8GB RAM (16GB or more recommended) Sufficient disk space (minimum 100GB recommended) Software OS : Linux (based on Ubuntu 22.04) NVIDIA driver version : 450.80.02 or later Docker : 19.03 or later (with NVIDIA Container Toolkit support) NVIDIA Container Toolkit (nvidia-docker2)",
      "<2-hop>\n\nthis guide, choose Accelerate hardware type. Default: Utilizes only the CPU and GPU. Accelerate: Engages the NPU as an additional accelerator. > Warnig: NPU does nto support all the models. Choosing the Accelerate option may lead to complications. After confirming the selections, the subsequent screen appears: Conversion Select Convert to initiate the conversion process. After the completion of conversion process, the NNC Download button is enabled. Download Model Click NNC Download to obtain the converted NNC model file. To view the logs for the conversion that has failed, click Log Download. You can download and examine the log files. Copy the downloaded model to ${APP_ROOT}/app/src/main/assets. Compiling Using NDK This section describes the method to use NDK to compile the native program. The process comprises of two main steps such as setting up the Makefile and initiating the build process with NDK. Creating the Makefile The Makefile is a crucial component in the build process. It instructs the compiler on how to build the program. The Makefile for this project is divided into two parts such as Android.mk and Application.mk. Android.mk The Android.mk file defines the module and its properties. (example): ```cmake LOCAL_PATH := $(call my-dir) include $(CLEAR_VARS) LOCAL_MODULE := enn_public_api_ndk_v1 LOCAL_SRC_FILES := ${LOCAL_PATH}/lib64/libenn_public_api_ndk_v1.so include $(PREBUILT_SHARED_LIBRARY) include $(CLEAR_VARS) LOCAL_MODULE := enn_nnc_model_tester LOCAL_C_INCLUDES += \\ ${LOCAL_PATH} \\ ${LOCAL_PATH}/include LOCAL_LDLIBS := -llog LOCAL_CFLAGS += -Wall -std=c++14 -O3 LOCAL_CPPFLAGS += -fexceptions -frtti LOCAL_SRC_FILES := enn_nnc_model_tester.cpp LOCAL_SHARED_LIBRARIES := enn_public_api_ndk_v1 include $(BUILD_EXECUTABLE) ``` Application.mk: The Application.mk file specifies the Application Binary Interface (ABI) and the Standard Template Library (STL) variant that must be used. (example): cmake APP_ABI := arm64-v8a APP_STL := c++_static Build Using NDK After the Makefile is set up, the build process with NDK can be initiated. shell export NDK_PROJECT_PATH=$(pwd) $ANDROID_NDK_HOME/ndk-build -C . This command instructs NDK to start the build process in the current directory. The NDK_PROJECT_PATH environment variable is set to the current directory. Verifying the Build After the build process is complete, the compiled program can be verified by checking the libs directory: shell ls libs/arm64-v8a/ The compiled program is visible in the output. Troubleshooting If you encounter any issues during the build process, ensure the following: The NDK_PROJECT_PATH environment variable is correctly set. The ANDROID_NDK_HOME environment variable points to the correct location of the NDK installation. The paths in the Android.mk file are correct. How to use enntools initialization To run enntools, you first need to create a folder and place an ONNX file with the same name as the folder inside it. Then you must initialize the project. When you initialize the project, the necessary files for using enntools commands will be generated inside the project directory. bash enntools init elt yaml file These are the configuration values used in various modules of elt. Detailed explanation for elt yaml file model_analyzer : dict - check : bool : Check the op support status - device : str : System on Chip type [Gen-5, Gen-5a, Gen-5b, Gen-6] - level : int : level for analyze model : 0 (OpCount), 1 (OpCount + Attribute), 2 (OpCount + Attribute + In/Out Shape), 3 (OpCount + Attribute + In/Out Shape + Mem Footprint) - snc_input : bool : analyze the snc model, false: analyze the original model database_gen : dict - database_spec : str : genration dataset spec path (default path is {workspace}/DATA/db_spec.yaml) converter : dict - device : str : Soc type [Gen-2, Gen-2a, Gen-3, Gen-3DSP, Gen-4, Gen-4DSP, Gen-4Multi(only for fp16), Gen-5, Gen-5a, Gen-5b, Gen-6, Gen-7] - do_quantize : bool : Enable quantization - eltsum_cvt : bool : enable eltwise conversion process; eltsum -> concat + conv - graph_opt : str : Graph Optimizer Model Type, Model Name for Graph Optimizer. - mean : str : Mean value(s), for multiple channel \"128, 128, 128, ...\" / y=x-MEAN - onnx_simplify : bool : enable onnx_simplify process - optimize : bool : Use graph optimization - quantize_type : str : Select quantization type, quantized model (include caffeQAT) is \"qat\" - scale : str : Scale value(s), for multiple channel \"128, 128, 128, ...\" / ex> y=x/SCALE - bw_ofm : int : Bitwidth of intermediate feature map(A). - data_format : str : [channel_first, channel_last] - debug : bool : dump layerwise sqnr between new snc and hw quantized snc. - gpu_enable : bool : enable infer model on gpu if gpu is available - gpu_id : int : gpu id for quantization profiling - input_dtype : str : [float32, float16, uint8, uint16, none] You can set model input datatpye as float32, uint8(only Asym)(if none take from model). - mode : str : [elt, eht_cnnx, eht_snc] - output_dtype : str : You can set model output datatpye as float32, uint8(only Asym)(if none take from model). [float32, float16, uint8, uint16, none] - profile_batchsize : int : Batchsize for profile (value 100 is recommened). - skip_old_snc_optimizer : bool : true, skip old snc optimizer in old2new - snc_converter : bool : True, convert old snc to new snc, set it to false when input is new snc - test_vector_gen : bool : Enable testvector geneartion after quantization. - tv_input : str : Input data file path for testvector generation (default path is {workspace}/DATA/database.h5) - use_randomdb : bool : Use randomdb for profiling data set - userdb : str : Profling data set path (default path is {workspace}/DATA/database.txt) compiler : dict - assign_cpu : str : Assign specific layer to cpu device - assign_dsp : str : Assign specific layer to dsp device - assign_gpu : str : Assign specific layer to gpu device - best_fit_generalized : bool : Control whether generalized best fit allocation is to be used. - cast_in : str : Type casting fp32 to fp16 for nnc input data - cast_out : str : Type casting fp16 to fp32 for nnc output data - cfs : bool : Enable cfifo sync - compiler : str : Compiler option - datalayout_conversion_in : str : Data layout(NHWC) conversion for nnc input data - datalayout_conversion_out : str : Data layout(NHWC) conversion for nnc output data - debug_str :"
    ],
    "reference": "To convert a TFLite model to an NNC model using the Exynos AI Studio service, follow these steps: First, access the ENN Eco-system by signing up or logging in. Then, navigate to the Exynos AI Studio service page and provide project information by entering a descriptive title and uploading your TFLite model. Next, choose your hardware preferences: the default option utilizes only the CPU and GPU, while the 'Accelerate' option engages the NPU as an additional accelerator. However, it is important to note that the NPU does not support all layers, and using unsupported layers may lead to complications. After confirming your selections, click 'Convert' to initiate the conversion process. If successful, the NNC Download button will be enabled, allowing you to download the converted NNC model.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the steps involved in executing a model using the ENN framework as described in the Exynos AI Studio documentation?",
    "reference_contexts": [
      "<1-hop>\n\nIndex EnnReturn result, 0 is success function EnnGetPreferenceCoreAffinity EnnReturn EnnGetPreferenceCoreAffinity( uint32_t * val_ptr ) Get current information for NPU Core affinity. Parameters: val [OUT] current value of NPU Core affinity Return: EnnReturn result, 0 is success function EnnGetMetaInfo EnnReturn EnnGetMetaInfo( const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX] ) Get Meta Information. Parameters: info_id info_id can be below: currently, ENN_META_VERSION_FRAMEWORK, ENN_META_VERSION_COMMIT, ENN_META_VERSION_MODEL_COMPILER_NPU is done. cpp ENN_META_VERSION_FRAMEWORK ENN_META_VERSION_COMMIT ENN_META_VERSION_MODEL_COMPILER_NNC ENN_META_VERSION_MODEL_COMPILER_NPU ENN_META_VERSION_MODEL_COMPILER_DSP ENN_META_VERSION_MODEL_SCHEMA ENN_META_VERSION_MODEL_VERSION ENN_META_VERSION_DD ENN_META_VERSION_UNIFIED_FW ENN_META_VERSION_NPU_FW ENN_META_VERSION_DSP_FW * model_id * output_str Return: EnnReturn result, 0 is success This API includes loaded model information as well as framework information function EnnSetExecMsgAlwaysOn EnnReturn EnnSetExecMsgAlwaysOn() Set frequency of execution message print. Parameters: rate if rate is N, the exe msg shows every {1, N+1, 2N+1..} times. Return: EnnReturn Updated on 2023-08-11 at 16:24:05 +0900 Dataset preparation This is a guideline for preparing the input dataset. Dataset format The dataset format must use the .h5 file extension. It is important to ensure that the keys in the h5 dataset match the input names of the model for proper mapping to occur. Additionally, as the Exynos AI Studio currently only accepts float32 data types, the internal data within the h5 file must be in fp32 format. The h5 file should be placed directly under the DATA folder, which is generated after executing the 'enntools init' command in the workspace. Model Requirements and Constraints model format Model should be prepared in ONNX format to start optimization. opset version EHT currently support ONNX opset version 13 ~ 17. Exynos AI Studio Developer Guide Abstract This guide describes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. 1. Introduction Exynos AI Studio allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware. This guide is applicable for users who want to test or construct an application to run inference on Exynos AI Studio. Structure of Documentation Chapter 1 introduces Exynos AI Studio and its eco-system. Chapter 2 provides information on the features of Exynos AI Studio. Chapter 3 provides information on tools provided with Exynos AI Studio. Chapter 4 provides information on ENN framework API. The subsequent chapters provide additional information on Exynos AI Studio. Samples The list of samples for Exynos AI Studio is available in Exynos AI Studio Samples. Support Support materials including forums, FAQs, and others are available at the Exynos Eco-system web page. Reporting Bugs To report a bug or issue, follow the instructions described in the Reporting Exynos AI Studio Issues. 2. Features This chapter provides a general overview of the features that are provided by Exynos AI Studio. Workflow of Exynos AI Studio Using Exynos AI Studio involves the following two steps: 1. The user converts NN models to NNC models. NNC is an NN model format that can run efficiently in Samsung Exynos hardware. 1. The user executes the converted model for inference. Model Conversion Use one of the tools that is provided to convert NN models. To convert a model: 1. Prepare a pre-trained NN model. 1. Set parameters for tools. 1. Execute tools for conversion. Model Execution Executing converted models is performed by the ENN framework. When using the ENN framework: 1. Initialize ENN framework. 1. Load the converted model to ENN framework. 1. Allocate and commit all the necessary buffers for the model. Then: 1. Copy input data to input buffers. 2. Execute model on ENN framework. 3. Use data on output buffers. To execute the model multiple times, repeat this process. Finally, perform the following steps: 1. Un commit and release buffers allocated to the model. 2. Unload the model. 3. De initialize ENN framework.. ENN framework APIs support language binding for C++. Supported Neural Network Models Exynos AI Studio supports the following NN models: - TensorFlow Lite - Tensors with up to four dimensions - Models with a maximum size of 1 GB For more information on supported TensorFlow Lite operations, refer to the Support Matrix. 3.",
      "<2-hop>\n\ndescribes the method to use Exynos Neural Network Software Development Kit (Exynos AI Studio). It provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models. It also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output. API References This API reference documentation provides a list of data types and functions of Exynos AI Studio. Support Matrix These support matrices provide information on the supported platforms and operators of Exynos AI Studio. Exynos AI Studio Usage Guide Quick Start Guide This Quick Start Guide provides basic instructions for using Exynos AI Studio. This document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices. Exynos AI Studio Samples This guide provides a list of samples of Exynos AI Studio and their explanation. Getting Started With Android Samples This guide provides a comprehensive overview of developing an image classification Android application using Exynos AI Studio. Getting Started with Native Samples This guide provides a walkthrough for developing a native program using Exynos AI Studio. Last Updated: 2023-11-21 0321 UTC Quick Start Run docker container bash docker run -it --gpus all --name exynos_ai_studio_container \\ -v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\ exynos_ai_studio:0.1.0 RUN tutorial bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/my_workspace.onnx my_workspace/ cd my_workspace enntools init bash mkdir my_workspace cp {CONTAINER_DATASET_DIR}/dataset.h5 my_workspace/DATA/ modify config file and set mode bash enntools conversion Support Matrix The support matrices provide information about the models that are compatible with Exynos AI Studio. Supported Models Exynos AI Studio supports the following models: - TensorFlow Lite - It is recommended to convert TFLite models using MLIR version 1.14 or higher. - Tensors up to four dimensions - Models with a maximum size of 1 GB Additionally, for NPU, the models must be quantized. Supported Operators Exynos AI Studio supports the following operators | Index | Operator_Name | TFLite | NPU | DSP | GPU | CPU | | --------- | ------------------------------ | ----------------------- | ------- | ------- | ------- | ------- | | 1 | ABS | ABS | | O | O | O | | 2 | ADD | ADD | | | O | O | | 3 | ARGMAX | ARG_MAX | | O | O | O | | 4 | ARGMIN | ARG_MIN | | O | O | O | | 5 | AVGPOOL | AVERAGE_POOL_2D | O | O | O | O | | 6 | BATCH_NORMALIZATION | - | O | | | | | 7 | CEIL | CEIL | O | O | O | O | | 8 | CONCATENATION | CONCATENATION | O | O | O | O | | 9 | CONSTANT | - | O | | | | | 10 | CONVOLUTION | CONV_2D | O | O | O | O | | 11 | COS | COS | | | O | O | | 12 | CROP | STRIDED_SLICE | | O | O | O | | 13 | DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 14 | DEPTH_TO_SPACE | DEPTH_TO_SPACE | O | O | O | O | | 15 | DEPTHWISE_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 16 | DEPTHWISE_DECONVOLUTION | TRANSPOSE_CONV | O | O | O | O | | 17 | DEPTHWISE_DILATION_CONVOLUTION | DEPTHWISE_CONV_2D | O | O | O | O | | 18 | DEQUANTIZE | DEQUANTIZE | O | O | O | O | | 19 | DILATION_CONVOLUTION | CONV_2D | O | O | O | O | | 20 | DIV | DIV | | | O | O | | 21 | DYNAMIC_CONVOLUTION | - | O | | | | | 22 | DYNAMIC_FC | - | O | | | | | 23 | ELEMENTWISE_DIV | DIV | | O | O | O | | 24 | ELEMENTWISE_MUL | MUL | O | O | O | O | | 25 | ELEMENTWISE_SUB | SUB | O | O | O | O | | 26 | ELEMENTWISE_SUM | ADD | O | O | O | O | | 27 | EXP | EXP | | O | O | O | | 28 | UNSQUEEZE | EXPAND_DIMS | | | | O | | 29 | FLATTEN | - | O | O | O | O | | 30 | FLOOR | FLOOR | | O | O | O | | 31 | FLOOR_DIV | FLOOR_DIV | | O | | O | | 32 | FULLY_CONNECTED | FULLY_CONNECTED | O | O | O | O | | 33 | GATHER | GATHER | | | O | O | | 34 | GLOBAL_AVGPOOL | - | O | | O | O | | 35 | GLOBAL_MAXPOOL | - | O | O | O | O | | 36 | GROUP_CONVOLUTION | CONV_2D | O | O | O | O | | 37 | HARD_SWISH | HARD_SWISH | O | O | O | O | | 38 | L2_NORMALIZATION | L2_NORMALIZATION | | O | O | O | | 39 | LEAKY_RELU | LEAKY_RELU | O | O | O | O | | 40 | LOG | LOG | | | O | O | | 41 | LOGISTIC | LOGISTIC | O | O | O | O | | 42 | MATMUL | - | | | | | | 43 | MAXIMUM | MAXIMUM | | O | O | O | | 44 | MAXPOOL | MAX_POOL_2D | O | O | O | O | | 45 | MEAN | MEAN | | O | O | O | | 46 | MINIMUM | MINIMUM | | O | O | O | | 47 | MIRROR_PAD | MIRROR_PAD | O | O | O | O | |"
    ],
    "reference": "To execute a model using the ENN framework, the following steps are involved: 1. Initialize the ENN framework. 2. Load the converted model into the ENN framework. 3. Allocate and commit all necessary buffers for the model. 4. Copy input data to the input buffers. 5. Execute the model on the ENN framework. 6. Use data from the output buffers. To execute the model multiple times, repeat this process. Finally, perform the following cleanup steps: 1. Uncommit and release buffers allocated to the model. 2. Unload the model. 3. Deinitialize the ENN framework.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:15:38.735101+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "How does Samsung Electronics contribute to mobile application development for image processing?",
    "reference_contexts": [
      "This document explains how a simple Android Sample Application operates using the [Detr\\_resnet50\\_dc5](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/a631921e-dc8b-46cb-ac17-d23c5a54db26) model optimized for Exynos hardware. **1\\. Functionality** This Sample Application identifies objects in images either from stored image files or input via the camera. Detected objects are highlighted with bounding boxes, and the label and score of each object are displayed. Additionally, the inference time is shown at the bottom of the application interface. ![](images/sample_application/phone_1.png) **2\\. Prerequisites** * To download [Android Studio](https://developer.android.com/studio), go to the official website and download it. * To run the Sample Application, clone the following repository. ```sh git clone https://github.com/exynos-eco/enn-sdk-samples-9925.git ``` * If there is no device available to run the application, you can use a real device provided in the Device Farm. For guidance on connecting a device in Android Studio, refer to the ADB proxy guide. **3. Starting Sample Application** * Open the cmd window and check if adb is connected. ```sh adb device List of devices attached 000011b58d246013 device ``` * Push a sample image for testing to the following path using adb push. ```sh adb push sample.jpg /storage/emulated/0/Pictures/ ``` * If the sample image is not visible after running the Sample Application, the media scanner may not have been updated, preventing the app from detecting the file. In this case, run the following command in adb shell to trigger media scanning. ```sh adb adb shell am broadcast -a android.intent.action.MEDIA_SCANNER_SCAN_FILE -d file:///storage/emulated/0/Pictures/sample.jpg ``` * After confirming a proper connection with Device Farm, select Tools \\-\\> Device Manager in Android Studio. ![](images/sample_application/device_manager_1.png) * After that check if the physical device is properly connected. ![](images/sample_application/samsung_electronics_1.png) * Run the Object Detection project from the Sample Applications obtained via git clone in Android Studio. ![](images/sample_application/object_detection_1.png) * When you select the object-detection project Android Studio will automatically perform a gradle sync. * The application in the selected project checks the connected device and runs when you click the Run button. ![](images/sample_application/run_1.png) * Select image mode and provide the data to be used for inference. ![](images/sample_application/camera_1.png) **4\\. I/O Specs** | INPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | input | An RGB image | \\[1, 3, 480, 480\\] | float32 | | OUTPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | output | Class score value | \\[1, 100, 92\\] | float32 | | 4363 | Show bounding box | \\[1, 100, 4\\] | Float32 | **5\\."
    ],
    "reference": "Samsung Electronics provides hardware optimized for applications like the Android Sample Application that utilizes the Detr_resnet50_dc5 model. This application identifies objects in images, highlighting them with bounding boxes and displaying labels and scores, which is crucial for enhancing image processing capabilities, especially in low-light conditions.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation\\documentation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:25:51.314092+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the purpose of the ennDeinitialize function in the object detection application workflow?",
    "reference_contexts": [
      "Object Detection Application Workflow** The Object Detection Sample Application follows these steps to produce results. 1. The model execution preparation sequence is as follows. 1-1. Initialize the framework using the ennInitialize function. 1-2. Load the ML model into the framework using the ennOpenModel function. 1-3. Allocate and commit the necessary buffers using the ennAllocateAllBuffers function. 2. The model execution sequence is as follows. 2-1. Set the input data as parameters. 2-2. Call the ennExecute function. 2-3. Receive the execution results. 3. The method for releasing the framework initialization is as follows. 3-1. Release the allocated memory of the buffers using the ennReleaseBuffers function. 3-2. Close the model and release other resources using the ennCloseModel function. 3-3. Deinitialize the framework using the ennDeinitialize function. ※ If you want to learn more about the detailed functions, please refer to the [Framework API](#bookmark=id.3o7alnk) at the bottom of the document. **6\\. Checklist when replacing model** 1\\. Model Filename (MODEL\\_NAME) * Replace the model file (Detr\\_resnet50\\_dc5) by adding it to the assets directory and entering the new model filename in the ModelConstants.kt file. ![](images/sample_application/android_assets_1.png) ![](images/sample_application/modelconstants_1.png) ```kotlin const val MODEL_NAME = \"detr_resnet50_dc5.nnc\" ``` 2. Change the input data type in ModelConstants.kt (INPUT_DATA_TYPE, INPUT_DATA_LAYER) * Other models may accept FLOAT32 format input, so verification is required. Check whether the input format is NHWC, HWC, or CHW before making changes. ```kotlin val INPUT_DATA_TYPE = DataType.FLOAT32 // Change it to the data type required by the model val INPUT_DATA_LAYER = LayerType.CHW // Change it to the layer required by the model ``` 3. Modify the input resolution (INPUT_SIZE_W, INPUT_SIZE_H, INPUT_SIZE_C) in ModelConstants.kt. * You must reflect the input resolution used by the model. ```kotlin const val INPUT_SIZE_W = 320 const val INPUT_SIZE_H = 320 const val INPUT_SIZE_C = 3 // If 3(RGB) ``` 4. Change the normalization method in ModelConstants.kt (INPUT\\_CONVERSION\\_SCALE, INPUT\\_CONVERSION\\_OFFSET) * You need to check and modify the normalization method used by the model. * Input normalization formula normalized_input = (raw_input−INPUT_CONVERSION_OFFSET)/INPUT_CONVERSION_SCALE ```kotlin const val INPUT_CONVERSION_SCALE = 255.0F // Scale the input data by 255 times const val INPUT_CONVERSION_OFFSET = 0.0F // Since the normalization formula is normalized\\_input \\= (raw\\_input−0.0)/255 any original pixel value, such as 0, 128, or 255, will always be normalized within the 0 to 1 range. ``` Deep learning models are designed to accept input data in the range of 0 to 1 or \\-1 to 1\\. 5\\. Output size in ModelConstants.kt (OUTPUT\\_SIZE\\_W, OUTPUT\\_SIZE\\_H) * Check and modify the output size of the model accordingly. ```kotlin const val OUTPUT_SIZE_W = 8400 const val OUTPUT_SIZE_H = 84 ``` 6\\. Class label file in ModelConstants.kt (LABEL_FILE) * If the model uses different class labels, a new .txt file must be provided in the assets directory. ```kotlin const val LABEL_FILE = \"detr_resnet50_dc5.txt\" ``` 7\\. Modify the preProcess() and postProcess() functions in the ModelExecutor.kt file located at the following path ![](images/sample_application/modelExecutor_1.png) * If the model's input or output data differs from the predefined format, modify the preProcess() and postProcess() functions accordingly. ```kotlin Class ModelExecutor -> preProcess(), postProcess() ``` **7\\. Compatible AI Models** * [Detr\\_resnet101\\_dc5](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/9eb3e0fd-4478-49e4-b631-5941ce62d16c) * Face\\_det\\_lite * Foot\\_track\\_net * Yolov5"
    ],
    "reference": "The ennDeinitialize function is used to deinitialize the framework, releasing the resources that were allocated during the model execution preparation and execution sequences.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation\\documentation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:25:51.314092+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is Exynos used for in mobile applications?",
    "reference_contexts": [
      "This document explains how a simple Android Sample Application operates using the [Densenet121](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/118f8cc6-f251-43b7-b8c2-ec77a3c50fda) model optimized for Exynos hardware. **1\\. Functionality** This application classifies objects in images either from stored image files or captured via the camera. The classified items, corresponding scores, and inference time are displayed at the bottom of the application interface. ![](images/sample_application/mouse.png) **2\\. Prerequisites** * To download [Android Studio](https://developer.android.com/studio), go to the official website and download it. * To run the Sample Application, clone the following repository. ```sh git clone https://github.com/exynos-eco/enn-sdk-samples-9945.git ``` * If there is no device available to run the application, you can use a real device provided in the Device Farm. For guidance on connecting a device in Android Studio, refer to the ADB proxy guide. **3\\. Starting Sample Application** * Open the cmd window and check if adb is connected. ```sh adb device List of devices attached 000011b58d246013 device ``` * Push a sample image for testing to the following path using adb push. ```sh adb push sample.jpg /storage/emulated/0/Pictures/ ``` * If the sample image is not visible after running the Sample Application, the media scanner may not have been updated, preventing the app from detecting the file. In this case, run the following command in adb shell to trigger media scanning. ```sh adb adb shell am broadcast -a android.intent.action.MEDIA_SCANNER_SCAN_FILE -d file:///storage/emulated/0/Pictures/sample.jpg ``` * After confirming a proper connection with Device Farm, select Tools \\-\\> Device Manager in Android Studio. ![](images/sample_application/device_manager_1.png) * After that check if the physical device is properly connected. ![](images/sample_application/samsung_electronics_1.png) * Run the Image Classification project from the Sample Applications obtained via git clone in Android Studio. ![](images/sample_application/image_classification_1.png) * When you select the image-classification project Android Studio will automatically perform a gradle sync. * The application in the selected project checks the connected device and runs when you click the Run button. ![](images/sample_application/run_1.png) * Select image mode and provide the data to be used for inference. ![](images/sample_application/camera_1.png) **4\\."
    ],
    "reference": "Exynos is used in the context of a simple Android Sample Application that operates using the Densenet121 model optimized for Exynos hardware, allowing the application to classify objects in images.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation\\documentation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:25:51.314092+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the input shape required for Resnet34_v1_7?",
    "reference_contexts": [
      "I/O Specs** | INPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | Image | An RGB image | \\[1, 3, 224, 224\\] | float32 | | OUTPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | Classes | [Dataset Classes](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/assets/labels1001.txt) | \\[1, 1000\\] | float32 | **5\\. Image Classification** **Application Workflow** The Image Classification Sample Application follows these steps to produce results. 1. The model execution preparation sequence is as follows. 1-1. Initialize the framework using the ennInitialize function. 1-2. Load the ML model into the framework using the ennOpenModel function. 1-3. Allocate and commit the necessary buffers using the ennAllocateAllBuffers function. 2. The model execution sequence is as follows. 2-1. Set the input data as parameters. 2-2. Call the ennExecute function. 2-3. Receive the execution results. 3. The method for releasing the framework initialization is as follows. 3-1. Release the allocated memory of the buffers using the ennReleaseBuffers function. 3-2. Close the model and release other resources using the ennCloseModel function. 3-3. Deinitialize the framework using the ennDeinitialize function. ※ If you want to learn more about the detailed functions, please refer to the [Framework API](#bookmark=id.3o7alnk) at the bottom of the document. **6\\. Checklist when replacing model** 1\\. Model Filename (Densenet121) * Replace the model file (densenet121) by adding it to the assets directory and entering the new model filename in the ModelConstants.kt file. ![](images/sample_application/modelconstants_directory.png) ![](images/sample_application/modelconstants_2.png) ```kotlin const val MODEL_NAME = \"densenet121.nnc\" ``` 2\\. Change the input data type in ModelConstants.kt (INPUT_DATA_TYPE, INPUT_DATA_LAYER) * Other models may accept FLOAT32 format input, so verification is required. * Check whether the input format is NHWC, HWC, or CHW before making changes. ```kotlin val INPUT_DATA_TYPE = DataType.FLOAT32 // Change it to the data type required by the model val INPUT_DATA_LAYER = LayerType.CHW // Change it to the layer required by the model ``` 3\\. Modify the input resolution (INPUT_SIZE_W, INPUT_SIZE_H, INPUT_SIZE_C) in ModelConstants.kt * You must reflect the input resolution used by the model. ```kotlin const val INPUT_SIZE_W = 224 // If the model uses **224x224**, modify it accordingly const val INPUT_SIZE_H = 224 const val INPUT_SIZE_C = 3 // If 3(RGB) ``` 4\\. Change the normalization method in ModelConstants.kt (INPUT\\_CONVERSION\\_SCALE, INPUT\\_CONVERSION\\_OFFSET) * You need to check and modify the normalization method used by the model. * Input normalization formula normalized\\_input \\= (raw\\_input−INPUT\\_CONVERSION\\_OFFSET)/INPUT\\_CONVERSION\\_SCALE ```kotlin const val INPUT_CONVERSION_SCALE = 1/255F // Scale the input data by 255 times const val INPUT_CONVERSION_OFFSET = 0F // No data is subtracted from the input. Since the normalization formula is normalized\\_input \\= (raw\\_input–0)/(1/255) this is equivalent to raw\\_input×255. Therefore, the original pixel values in the range 0–255 are scaled up by 255 times, effectively keeping the 0–255 values unchanged. ``` 5\\. Modify the output size in ModelConstants.kt (OUTPUT\\_DATA\\_TYPE, OUTPUT\\_CONVERSION\\_SCALE, OUTPUT\\_CONVERSION\\_OFFSET) * You need to check and modify the model is data type and normalization method accordingly. * Output normalization formula final\\_output=(model\\_output×OUTPUT\\_CONVERSION\\_SCALE)+OUTPUT\\_CONVERSION\\_OFFSET ```kotlin val OUTPUT_DATA_TYPE = DataType.FLOAT32 const val OUTPUT_CONVERSION_SCALE = 1F // The output data is kept in its original form without scaling const val OUTPUT_CONVERSION_OFFSET = 0F // Since the formula is (model_output × 1) + 0 the output data remains unchanged and is used as is without any transformation ``` 6\\. Class label file in ModelConstants.kt (LABEL\\_FILE) * If the model uses different class labels a new .txt file must be provided in the assets directory. ```kotlin const val LABEL_FILE = \"densenet121.txt\" ``` 7\\. Modify the preProcess() and postProcess() functions in the ModelExecutor.kt file located at the following path. ![](images/sample_application/image_classification_modelExecutor.png) * If the model's input or output data differs from the predefined format, modify the preProcess() and postProcess() functions accordingly. ```kotlin Class ModelExecutor -> preProcess(), postProcess() ``` **7\\. Compatible AI Models** * [EfficientNet-B4](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/9d310aaa-d2f0-43d8-bdb1-0c31413da46e) * [MobileNet-v2](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/0c031a1e-0eed-442d-9691-421d416a5556) * [ResNet18](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/4c29e543-f74f-4bc3-a373-bc993c7ac7df) * [Resnet34\\_v1\\_7](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/df74a1bf-b048-4648-9396-31231b6fed49) * [ResNet50](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/27b58ffc-c760-4c87-ab60-533aba27ffa6) * [ResNet101](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/311c216e-f50c-4fee-a400-952b1fb96506) * [SqueezeNet1\\_1](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/546abf23-be6c-4a1a-9d65-edb48e94eb3a) * [Densenet121](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/118f8cc6-f251-43b7-b8c2-ec77a3c50fda) * [EfficientNet-B0](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/21ed28ef-d958-4cec-8d29-2d13efaf0468) * [MnasNet05](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/34efd7b3-8f3d-44fa-9440-34365277ff5f)"
    ],
    "reference": "The input shape required for Resnet34_v1_7 is [1, 3, 224, 224], which corresponds to an RGB image.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation\\documentation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:25:51.314092+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Wht is the main functionality of an Android Sample Applcation that uses the DeeplabV3 model?",
    "reference_contexts": [
      "This document explains how a simple Android Sample Application operates using the [DeeplabV3](https://prd.ai-studio-farm.com/kr/solution/ai/models/detail/ca3dc3fa-d687-474c-9679-dcee50da8b52) model optimized for Exynos hardware. **1\\. Functionality** This Application provides segmentation results for images either from stored image files or captured via the camera. Each pixel of the segmented object is overlaid with a color corresponding to its label, offering a visual representation of the classification. ![](images/sample_application/deepable.png) **2\\. Prerequisites** * To download [Android Studio](https://developer.android.com/studio), go to the official website and download it. * To run the Sample Application, clone the following repository. ```sh git clone https://github.com/exynos-eco/enn-sdk-samples-9945.git ``` * If there is no device available to run the application, you can use a real device provided in the Device Farm. For guidance on connecting a device in Android Studio, refer to the ADB proxy guide. **3\\. Starting Sample Application** * Open the cmd window and check if adb is connected. ```sh adb device List of devices attached 000011b58d246013 device ``` * Push a sample image for testing to the following path using adb push. ```sh adb push sample.jpg /storage/emulated/0/Pictures/ ``` * If the sample image is not visible after running the Sample Application, the media scanner may not have been updated, preventing the app from detecting the file. In this case, run the following command in adb shell to trigger media scanning. ```sh adb adb shell am broadcast -a android.intent.action.MEDIA_SCANNER_SCAN_FILE -d file:///storage/emulated/0/Pictures/sample.jpg ``` * After confirming a proper connection with Device Farm, select Tools -> Device Manager in Android Studio. ![](images/sample_application/device_manager_1.png) * After that check if the physical device is properly connected. ![](images/sample_application/samsung_electronics_1.png) * Run the Segmentation project from the Sample Applications obtained via git clone in Android Studio. ![](images/sample_application/segmentation.png) * When you select the segmentation project Android Studio will automatically perform a gradle sync. * The application in the selected project checks the connected device and runs when you click the Run button. ![](images/sample_application/run_1.png) * Select image mode and provide the data to be used for inference. ![](images/sample_application/camera_1.png) **4\\."
    ],
    "reference": "The main functionality of the Android Sample Application is to provide segmentation results for images either from stored image files or captured via the camera. Each pixel of the segmented object is overlaid with a color corresponding to its label, offering a visual representation of the classification.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation\\documentation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:25:51.314092+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the DataType used for input images in the Segmentation Sample Application?",
    "reference_contexts": [
      "I/O Specs** | INPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | Image | An RGB image | \\[1,257,257,3\\] | float32 | | OUTPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | Classes | Dataset Classes | \\[1,257,257,21\\] | float32 | **5\\. Segmentation** **Application Workflow** The Segmentation Sample Application follows these steps to produce results. 1. The model execution preparation sequence is as follows. 1-1. Initialize the framework using the ennInitialize function. 1-2. Load the ML model into the framework using the ennOpenModel function. 1-3. Allocate and commit the necessary buffers using the ennAllocateAllBuffers function. 2. The model execution sequence is as follows. 2-1. Set the input data as parameters. 2-2. Call the ennExecute function. 2-3. Receive the execution results. 3. The method for releasing the framework initialization is as follows. 3-1. Release the allocated memory of the buffers using the ennReleaseBuffers function. 3-2. Close the model and release other resources using the ennCloseModel function. 3-3. Deinitialize the framework using the ennDeinitialize function. ※ If you want to learn more about the detailed functions, please refer to the [Framework API](#bookmark=id.3o7alnk) at the bottom of the document. **6\\. Checklist when replacing model** 1\\. Model Filename (DeepLabV3) * Replace the model file (deeplabv3) by adding it to the assets directory and entering the new model filename in the ModelConstants.kt file. ![](images/sample_application/deepable_directory.png) ![](images/sample_application/deepable_modelconstants.png) ```kotlin const val MODEL_NAME = \"deeplabv3.nnc\" ``` 2\\. Change the input data type in ModelConstants.kt (INPUT\\_DATA\\_TYPE, INPUT\\_DATA\\_LAYER) * Other models may accept FLOAT32 format input, so verification is required. * Check whether the input format is NHWC, HWC, or CHW before making changes. ```kotlin val INPUT_DATA_TYPE = DataType.FLOAT32 // Change it to the data type required by the model val INPUT_DATA_LAYER = LayerType.HWC // 모델이 요구하는 레이어 형식으로 변경 ``` 3\\. Modify the input resolution (INPUT\\_SIZE\\_W, INPUT\\_SIZE\\_H, INPUT\\_SIZE\\_C) in ModelConstants.kt * You must reflect the input resolution used by the model. ```kotlin const val INPUT_SIZE_W = 257 // If the model uses 257x257, modify it accordingly const val INPUT_SIZE_H = 257 const val INPUT_SIZE_C = 3 // If 3(RGB) ``` 4\\. Change the normalization method in ModelConstants.kt (INPUT\\_CONVERSION\\_SCALE, INPUT\\_CONVERSION\\_OFFSET) * You need to check and modify the normalization method used by the model. * Input normalization formula normalized\\_input \\= (raw\\_input−INPUT\\_CONVERSION\\_OFFSET)/INPUT\\_CONVERSION\\_SCALE ```kotlin const val INPUT_CONVERSION_SCALE = 127.5F // Scale the input data by 127.5 times const val INPUT_CONVERSION_OFFSET = 127.5F // If the input pixel values range from 0\\~255, this technique is used to symmetrically normalize the data around the central value 127.5. By subtracting 127.5 from the input data (raw_input), the values are centered around 0, a commonly used technique in deep learning ``` 5\\. Output size in ModelConstants.kt (OUTPUT\\_DATA\\_TYPE, OUTPUT\\_SIZE\\_W, OUTPUT\\_SIZE\\_H, OUTPUT\\_SIZE\\_C) * You need to check and modify the model's data type and output size accordingly. The output size should be the same as the input size. * The DeepLabV3 model performs segmentation with 21 classes. For each pixel 21 probability values are output. ```kotlin val OUTPUT_DATA_TYPE = DataType.FLOAT32 const val OUTPUT_SIZE_H = INPUT_SIZE_H const val OUTPUT_SIZE_W = INPUT_SIZE_W const val OUTPUT_SIZE_C = 21 ``` 6\\. Output normalization in ModelConstants.kt (OUTPUT\\_CONVERSION\\_SCALE, OUTPUT\\_CONVERSION\\_OFFSET) * You need to check and modify the normalization method used by the model. * Output normalization formula final_output=(model_output×OUTPUT_CONVERSION_SCALE)+OUTPUT_CONVERSION_OFFSET ```kotlin const val OUTPUT_CONVERSION_SCALE = 1F // Use the output scale without conversion const val OUTPUT_CONVERSION_OFFSET = 0F // Since the formula is (model_output × 1) + 0, the output remains unchanged and is used as is without any transformation ``` 7\\. Modify the preProcess() and postProcess() functions in the ModelExecutor.kt file located at the following path. ![](images/sample_application/deepable_modelexecutor.png) * If the model is input or output data differs from the predefined format, modify the preProcess() and postProcess() functions accordingly. ```kotlin Class ModelExecutor -> preProcess(), postProcess() ``` **7\\. Compatible AI Models** * [DDRNet23\\_Slim](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/994cb06f-b886-4fb6-b8e9-8b4efdc8baee)"
    ],
    "reference": "The DataType used for input images in the Segmentation Sample Application is float32.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation\\documentation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:25:51.314092+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How does Posenet function in the Android Sample Application?",
    "reference_contexts": [
      "This document explains how a simple Android Sample Application operates using the [Posenet](https://prd.ai-studio-farm.com/kr/solution/ai/models/detail/f40473cb-a6e6-42a3-a50a-daf428273eab) model optimized for Exynos hardware **1\\. Functionality** This Application detects key points in images from stored image files or those captured via the camera and automatically measures joint positions. Additionally, the inference time is displayed at the bottom of the application interface. ![](images/sample_application/pose_estimation.png) **2\\. Prerequisites** * To download [Android Studio](https://developer.android.com/studio), go to the official website and download it. * To run the Sample Application, clone the following repository. ```sh git clone https://github.com/exynos-eco/enn-sdk-samples-9945.git ``` * If there is no device available to run the application, you can use a real device provided in the Device Farm. For guidance on connecting a device in Android Studio, refer to the ADB proxy guide. **3\\. Starting Sample Application** * Open the cmd window and check if adb is connected. ```sh adb device List of devices attached 000011b58d246013 device ``` * Push a sample image for testing to the following path using adb push. ```sh adb push sample.jpg /storage/emulated/0/Pictures/ ``` * If the sample image is not visible after running the Sample Application, the media scanner may not have been updated, preventing the app from detecting the file. In this case, run the following command in adb shell to trigger media scanning. ```sh adb adb shell am broadcast -a android.intent.action.MEDIA_SCANNER_SCAN_FILE -d file:///storage/emulated/0/Pictures/sample.jpg ``` * After confirming a proper connection with Device Farm, select Tools -> Device Manager in Android Studio. ![](images/sample_application/device_manager_1.png) * After that check if the physical device is properly connected. ![](images/sample_application/samsung_electronics_1.png) * Run the Pose Estimation project from the Sample Applications obtained via git clone in Android Studio. ![](images/sample_application/pose_estimation_android.png) * When you select the pose-estimation project Android Studio will automatically perform a gradle sync. * The application in the selected project checks the connected device and runs when you click the Run button. ![](images/sample_application/run_1.png) * Select image mode and provide the data to be used for inference. ![](images/sample_application/camera_1.png) **4\\. I/O Specs** | INPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | Image | An RGB image | \\[1, 3, 513, 257\\] | float32 | | OUTPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | Classes | Human Body Part | \\[1, 17, 33, 17\\] | float32 | **5\\."
    ],
    "reference": "Posenet detects key points in images from stored image files or those captured via the camera and automatically measures joint positions. The inference time is displayed at the bottom of the application interface.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation\\documentation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:25:51.314092+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Can you explain what happens when you use the ennDeinitialize function in the context of the Pose Estimation Sample Application?",
    "reference_contexts": [
      "Post Estimation Application Workflow** The Pose Estimation Sample Application follows these steps to produce results. 1. The model execution preparation sequence is as follows. 1-1. Initialize the framework using the ennInitialize function. 1-2. Load the ML model into the framework using the ennOpenModel function. 1-3. Allocate and commit the necessary buffers using the ennAllocateAllBuffers function. 2. The model execution sequence is as follows. 2-1. Set the input data as parameters. 2-2. Call the ennExecute function. 2-3. Receive the execution results. 3. The method for releasing the framework initialization is as follows. 3-1. Release the allocated memory of the buffers using the ennReleaseBuffers function. 3-2. Close the model and release other resources using the ennCloseModel function. 3-3. Deinitialize the framework using the ennDeinitialize function. ※ If you want to learn more about the detailed functions, please refer to the [Framework API](#bookmark=id.3o7alnk) at the bottom of the document. **6\\. Checklist when replacing model** 1\\. Model Filename (PoseNet) * Replace the model file (float32\\_pose) by adding it to the assets directory and entering the new model filename in the ModelConstants.kt file. ![](images/sample_application/pose_estimation_directory.png) ![](images/sample_application/pose_estimation_modelconstants.png) ```koltin const val MODEL_NAME = \"float32_pose.nnc\" ``` 2\\. Change the input data type in ModelConstants.kt (INPUT_DATA_TYPE, INPUT_DATA_LAYER) * Other models may accept FLOAT32 format input, so verification is required. * Check whether the input format is NHWC, HWC, or CHW before making changes. ```kotlin val INPUT_DATA_TYPE = DataType.FLOAT32 // Change it to the data type required by the model val INPUT_DATA_LAYER = LayerType.HWC // Change it to the layer required by the model ``` 3\\.Modify the input resolution in ModelConstants.kt (INPUT\\_SIZE\\_W, INPUT\\_SIZE\\_H, INPUT\\_SIZE\\_C) * You must reflect the input resolution used by the model. ```kotlin const val INPUT_SIZE_W = 257 // If the model uses 257x257, modify it accordingly const val INPUT_SIZE_H = 257 const val INPUT_SIZE_C = 3 // If 3(RGB) ``` 4\\. Change the normalization method in ModelConstants.kt (INPUT\\_CONVERSION\\_SCALE, INPUT\\_CONVERSION\\_OFFSET) * You need to check and modify the normalization method used by the model. * Input normalization formula normalized\\_input \\= (raw\\_input−INPUT\\_CONVERSION\\_OFFSET)/INPUT\\_CONVERSION\\_SCALE ```kotlin const val INPUT_CONVERSION_SCALE = 127.5F // Scale the input data by 127.5 times const val INPUT_CONVERSION_OFFSET = 127.5F // This is a technique commonly used in deep learning to symmetrically normalize data based on the central value of 127.5 when the input pixel values range from 0 to 255\\. By subtracting 127.5 from the input data (raw\\_input), the values are adjusted to center around zero ``` 5\\. Modify the Heatmap data type in ModelConstants.kt (HEATMAP\\_DATA\\_TYPE) * You need to check and modify the model's data type and output size accordingly. ```kotlin val HEATMAP_DATA_TYPE = DataType.FLOAT32 ``` 6\\. Modify the Heatmap output size in ModelConstants.kt (HEATMAP\\_SIZE\\_W, HEATMAP\\_SIZE\\_H, HEATMAP\\_SIZE\\_C) * he Heatmap generates an output of size 9×9×17, where the 17 channels represent the probability distribution for 17 body parts, including the nose, shoulders, elbows, wrists, hips, knees, and ankles. ```kotlin const val HEATMAP_SIZE_W = 9 // Width of heatmap (9x9) const val HEATMAP_SIZE_H = 9 // Height of heatmap const val HEATMAP_SIZE_C = 17 // Number of body parts (PoseNet uses 17 key points) ``` 7\\. Offset data type in ModelConstants.kt (OFFSET\\_DATA\\_TYPE) * You need to check and modify the model is output size accordingly. ```kotlin val OFFSET_DATA_TYPE = DataType.FLOAT32 ``` 8\\. Modify the Offset output size in ModelConstants.kt (OFFSET\\_SIZE\\_W, OFFSET\\_SIZE\\_H, OFFSET\\_SIZE\\_C) * The Offset is used to refine the exact position of each body part within the Heatmap grid. ```kotlin const val OFFSET_SIZE_W = 9 // Width of offset const val OFFSET_SIZE_H = 9 // Height of offset const val OFFSET_SIZE_C = 34 // Twice the body part (17 \\* 2 \\= 34\\) ``` 9\\. Modify the preProcess() and postProcess() functions in the ModelExecutor.kt file located at the following path. ![](images/sample_application/pose_estimation_modelexecutor.png) * If the model's input or output data differs from the predefined format, modify the preProcess() and postProcess() functions accordingly. ```kotlin Class ModelExecutor -> preProcess(), postProcess() ``` **7\\. Compatible AI Models** * Facemap\\_3dmm"
    ],
    "reference": "The ennDeinitialize function is used to release the framework initialization in the Pose Estimation Sample Application. This process involves closing the model and releasing other resources after the allocated memory of the buffers has been released using the ennReleaseBuffers function and the model has been closed with the ennCloseModel function.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation\\documentation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:25:51.314092+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What are the steps involved in model execution preparation for the Android Sample Application that enhances low-light images, and how does this relate to the overall functionality of the application?",
    "reference_contexts": [
      "<1-hop>\n\nThis document explains how a simple Android Sample Application operates using the [Real\\_ESRGAN\\_General\\_x4v3](https://prd.ai-studio-farm.com/global/solution/ai/models/detail/36ad7134-5621-48b2-8ddf-e4889417f6ef) model optimized for Exynos hardware. **1\\. Functionality** This Application enhances the low-light quality of images from stored image files or those captured via the camera. Additionally, the inference time is displayed at the bottom of the application interface. ![](images/sample_application/image_enhance.png) **2\\. Prerequisites** * To download [Android Studio](https://developer.android.com/studio), go to the official website and download it. * To run the Sample Application, clone the following repository. ```sh git clone https://github.com/exynos-eco/enn-sdk-samples-9945.git ``` * If there is no device available to run the application, you can use a real device provided in the Device Farm. For guidance on connecting a device in Android Studio, refer to the ADB proxy guide. **3\\. Starting Sample Application** * Open the cmd window and check if adb is connected. ```sh adb device List of devices attached 000011b58d246013 device ``` * Push a sample image for testing to the following path using adb push. ```sh adb push sample.jpg /storage/emulated/0/Pictures/ ``` * If the sample image is not visible after running the Sample Application, the media scanner may not have been updated, preventing the app from detecting the file. In this case, run the following command in adb shell to trigger media scanning. ```sh adb adb shell am broadcast -a android.intent.action.MEDIA_SCANNER_SCAN_FILE -d file:///storage/emulated/0/Pictures/sample.jpg ``` * After confirming a proper connection with Device Farm, select Tools -> Device Manager in Android Studio. ![](images/sample_application/device_manager_1.png) * After that check if the physical device is properly connected. ![](images/sample_application/samsung_electronics_1.png) * Run the Image Enhance project from the Sample Applications obtained via git clone in Android Studio. ![](images/sample_application/image_enhance_android.png) * When you select the image-enhance project Android Studio will automatically perform a gradle sync. * The application in the selected project checks the connected device and runs when you click the Run button. ![](images/sample_application/samsung_electronics_1.png) * Select image mode and provide the data to be used for inference. ![](images/sample_application/camera_1.png) **4\\. I/O Specs** | INPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | Image | An RGB image | \\[1, 3, 128, 128\\] | float32 | | OUTPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | Classes | Enhanced Image | \\[1, 3, 512, 512\\] | float32 | **5\\.",
      "<2-hop>\n\nImage Enhance Application Workflow** The Image Enhance Sample Application follows these steps to produce results. 1. The model execution preparation sequence is as follows. 1-1. Initialize the framework using the ennInitialize function. 1-2. Load the ML model into the framework using the ennOpenModel function. 1-3. Allocate and commit the necessary buffers using the ennAllocateAllBuffers function. 2. The model execution sequence is as follows. 2-1. Set the input data as parameters. 2-2. Call the ennExecute function. 2-3. Receive the execution results. 3. The method for releasing the framework initialization is as follows. 3-1. Release the allocated memory of the buffers using the ennReleaseBuffers function. 3-2. Close the model and release other resources using the ennCloseModel function. 3-3. Deinitialize the framework using the ennDeinitialize function. ※ If you want to learn more about the detailed functions, please refer to the [Framework API](#bookmark=id.3o7alnk) at the bottom of the document. **6\\. Checklist when replacing model** 1\\. Model Filename (Real\\_esrgan\\_general\\_x4v3) * Replace the model file (real\\_esrgan\\_general\\_x4v3) by adding it to the assets directory and entering the new model filename in the ModelConstants.kt file. ![](images/sample_application/image_enhance_directory.png) ![](images/sample_application/image_enhance_modelconstants.png) ```kotlin const val MODEL_NAME = \"real_esrgan_general_x4v3.nnc\" ``` 2\\. Change the input data type in ModelConstants.kt (INPUT\\_DATA\\_TYPE, INPUT\\_DATA\\_LAYER) * Other models may accept FLOAT32 format input, so verification is required. * Check whether the input format is NHWC, HWC, or CHW before making changes. ```kotlin val INPUT_DATA_TYPE = DataType.FLOAT32 // Change it to the data type required by the model val INPUT_DATA_LAYER = LayerType.CHW // Change it to the layer required by the model ``` 3\\. Modify the input resolution (INPUT\\_SIZE\\_W, INPUT\\_SIZE\\_H, INPUT\\_SIZE\\_C) * You must reflect the input resolution used by the model. ```kotlin const val INPUT_SIZE_W = 128 // If the model uses **128x128**, modify it accordingly const val INPUT_SIZE_H = 128 const val INPUT_SIZE_C = 3 // If 3(RGB) ``` 4\\. Change the normalization method in ModelConstants.kt (INPUT\\_CONVERSION\\_SCALE, INPUT\\_CONVERSION\\_OFFSET) * You need to check and modify the normalization method used by the model. * Input normalization formula normalized\\_input \\= (raw\\_input−INPUT\\_CONVERSION\\_OFFSET)/INPUT\\_CONVERSION\\_SCALE ```kotlin const val INPUT_CONVERSION_SCALE = 256F // Scale the input data by 256 times const val INPUT_CONVERSION_OFFSET \\= 0.0F // By setting the offset to 0, the maximum pixel value of the image (255) is divided by INPUT_CONVERSION_SCALE, resulting in a transformed value of 0.996 ``` 5\\. Set the output data format in ModelConstants.kt (OUTPUT\\_DATA\\_TYPE, OUTPUT\\_DATA\\_LAYER) * Other models may accept FLOAT32 format input, so verification is required. * Check whether the input format is NHWC, HWC, or CHW before making changes. ```kotlin val OUTPUT_DATA_TYPE = DataType.FLOAT32 //Change it to the data type required by the model val OUTPUT_DATA_LAYER = LayerType.CHW // Change it to the layer required by the model ``` 6\\. Set the output image size in ModelConstants.kt (OUTPUT\\_SIZE\\_W, OUTPUT\\_SIZE\\_H, OUTPUT\\_SIZE\\_C) * You must reflect the output resolution used by the model. ```kotlin const val OUTPUT_SIZE_W = 512 const val OUTPUT_SIZE_H = 512 const val OUTPUT_SIZE_C = 3 ``` 7\\. Set the output normalization method in ModelConstants.kt (OUTPUT\\_CONVERSION\\_SCALE, OUTPUT\\_CONVERSION\\_OFFSET) * You need to check and modify the normalization method used by the model. * Output normalization formula final\\_output=(model\\_output×OUTPUT\\_CONVERSION\\_SCALE)+OUTPUT\\_CONVERSION\\_OFFSET ```kotlin const val OUTPUT_CONVERSION_SCALE = 256F // Scale the input data by 256 times const val OUTPUT_CONVERSION_OFFSET = 0F // By setting the offset to 0, the maximum pixel value of the image (255) is divided by OUTPUT_CONVERSION_SCALE, resulting in a transformed value of 0.996 ``` 8\\. Modify the preProcess() and postProcess() functions in the ModelExecutor.kt file located at the following path. ![](images/sample_application/image_enhance_modelexecutor.png) * If the model's input or output data differs from the predefined format, modify the preProcess() and postProcess() functions accordingly. ```kotlin Class ModelExecutor -> preProcess(), postProcess() ``` **7\\. Compatible AI Models** * SESR-M5 * Zero\\_Dce",
      "<3-hop>\n\nThis document explains how a simple Android Sample Application operates using the [Posenet](https://prd.ai-studio-farm.com/kr/solution/ai/models/detail/f40473cb-a6e6-42a3-a50a-daf428273eab) model optimized for Exynos hardware **1\\. Functionality** This Application detects key points in images from stored image files or those captured via the camera and automatically measures joint positions. Additionally, the inference time is displayed at the bottom of the application interface. ![](images/sample_application/pose_estimation.png) **2\\. Prerequisites** * To download [Android Studio](https://developer.android.com/studio), go to the official website and download it. * To run the Sample Application, clone the following repository. ```sh git clone https://github.com/exynos-eco/enn-sdk-samples-9945.git ``` * If there is no device available to run the application, you can use a real device provided in the Device Farm. For guidance on connecting a device in Android Studio, refer to the ADB proxy guide. **3\\. Starting Sample Application** * Open the cmd window and check if adb is connected. ```sh adb device List of devices attached 000011b58d246013 device ``` * Push a sample image for testing to the following path using adb push. ```sh adb push sample.jpg /storage/emulated/0/Pictures/ ``` * If the sample image is not visible after running the Sample Application, the media scanner may not have been updated, preventing the app from detecting the file. In this case, run the following command in adb shell to trigger media scanning. ```sh adb adb shell am broadcast -a android.intent.action.MEDIA_SCANNER_SCAN_FILE -d file:///storage/emulated/0/Pictures/sample.jpg ``` * After confirming a proper connection with Device Farm, select Tools -> Device Manager in Android Studio. ![](images/sample_application/device_manager_1.png) * After that check if the physical device is properly connected. ![](images/sample_application/samsung_electronics_1.png) * Run the Pose Estimation project from the Sample Applications obtained via git clone in Android Studio. ![](images/sample_application/pose_estimation_android.png) * When you select the pose-estimation project Android Studio will automatically perform a gradle sync. * The application in the selected project checks the connected device and runs when you click the Run button. ![](images/sample_application/run_1.png) * Select image mode and provide the data to be used for inference. ![](images/sample_application/camera_1.png) **4\\. I/O Specs** | INPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | Image | An RGB image | \\[1, 3, 513, 257\\] | float32 | | OUTPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | Classes | Human Body Part | \\[1, 17, 33, 17\\] | float32 | **5\\.",
      "<4-hop>\n\nPost Estimation Application Workflow** The Pose Estimation Sample Application follows these steps to produce results. 1. The model execution preparation sequence is as follows. 1-1. Initialize the framework using the ennInitialize function. 1-2. Load the ML model into the framework using the ennOpenModel function. 1-3. Allocate and commit the necessary buffers using the ennAllocateAllBuffers function. 2. The model execution sequence is as follows. 2-1. Set the input data as parameters. 2-2. Call the ennExecute function. 2-3. Receive the execution results. 3. The method for releasing the framework initialization is as follows. 3-1. Release the allocated memory of the buffers using the ennReleaseBuffers function. 3-2. Close the model and release other resources using the ennCloseModel function. 3-3. Deinitialize the framework using the ennDeinitialize function. ※ If you want to learn more about the detailed functions, please refer to the [Framework API](#bookmark=id.3o7alnk) at the bottom of the document. **6\\. Checklist when replacing model** 1\\. Model Filename (PoseNet) * Replace the model file (float32\\_pose) by adding it to the assets directory and entering the new model filename in the ModelConstants.kt file. ![](images/sample_application/pose_estimation_directory.png) ![](images/sample_application/pose_estimation_modelconstants.png) ```koltin const val MODEL_NAME = \"float32_pose.nnc\" ``` 2\\. Change the input data type in ModelConstants.kt (INPUT_DATA_TYPE, INPUT_DATA_LAYER) * Other models may accept FLOAT32 format input, so verification is required. * Check whether the input format is NHWC, HWC, or CHW before making changes. ```kotlin val INPUT_DATA_TYPE = DataType.FLOAT32 // Change it to the data type required by the model val INPUT_DATA_LAYER = LayerType.HWC // Change it to the layer required by the model ``` 3\\.Modify the input resolution in ModelConstants.kt (INPUT\\_SIZE\\_W, INPUT\\_SIZE\\_H, INPUT\\_SIZE\\_C) * You must reflect the input resolution used by the model. ```kotlin const val INPUT_SIZE_W = 257 // If the model uses 257x257, modify it accordingly const val INPUT_SIZE_H = 257 const val INPUT_SIZE_C = 3 // If 3(RGB) ``` 4\\. Change the normalization method in ModelConstants.kt (INPUT\\_CONVERSION\\_SCALE, INPUT\\_CONVERSION\\_OFFSET) * You need to check and modify the normalization method used by the model. * Input normalization formula normalized\\_input \\= (raw\\_input−INPUT\\_CONVERSION\\_OFFSET)/INPUT\\_CONVERSION\\_SCALE ```kotlin const val INPUT_CONVERSION_SCALE = 127.5F // Scale the input data by 127.5 times const val INPUT_CONVERSION_OFFSET = 127.5F // This is a technique commonly used in deep learning to symmetrically normalize data based on the central value of 127.5 when the input pixel values range from 0 to 255\\. By subtracting 127.5 from the input data (raw\\_input), the values are adjusted to center around zero ``` 5\\. Modify the Heatmap data type in ModelConstants.kt (HEATMAP\\_DATA\\_TYPE) * You need to check and modify the model's data type and output size accordingly. ```kotlin val HEATMAP_DATA_TYPE = DataType.FLOAT32 ``` 6\\. Modify the Heatmap output size in ModelConstants.kt (HEATMAP\\_SIZE\\_W, HEATMAP\\_SIZE\\_H, HEATMAP\\_SIZE\\_C) * he Heatmap generates an output of size 9×9×17, where the 17 channels represent the probability distribution for 17 body parts, including the nose, shoulders, elbows, wrists, hips, knees, and ankles. ```kotlin const val HEATMAP_SIZE_W = 9 // Width of heatmap (9x9) const val HEATMAP_SIZE_H = 9 // Height of heatmap const val HEATMAP_SIZE_C = 17 // Number of body parts (PoseNet uses 17 key points) ``` 7\\. Offset data type in ModelConstants.kt (OFFSET\\_DATA\\_TYPE) * You need to check and modify the model is output size accordingly. ```kotlin val OFFSET_DATA_TYPE = DataType.FLOAT32 ``` 8\\. Modify the Offset output size in ModelConstants.kt (OFFSET\\_SIZE\\_W, OFFSET\\_SIZE\\_H, OFFSET\\_SIZE\\_C) * The Offset is used to refine the exact position of each body part within the Heatmap grid. ```kotlin const val OFFSET_SIZE_W = 9 // Width of offset const val OFFSET_SIZE_H = 9 // Height of offset const val OFFSET_SIZE_C = 34 // Twice the body part (17 \\* 2 \\= 34\\) ``` 9\\. Modify the preProcess() and postProcess() functions in the ModelExecutor.kt file located at the following path. ![](images/sample_application/pose_estimation_modelexecutor.png) * If the model's input or output data differs from the predefined format, modify the preProcess() and postProcess() functions accordingly. ```kotlin Class ModelExecutor -> preProcess(), postProcess() ``` **7\\. Compatible AI Models** * Facemap\\_3dmm"
    ],
    "reference": "The steps involved in model execution preparation for the Android Sample Application that enhances low-light images are as follows: 1. Initialize the framework using the ennInitialize function. 2. Load the ML model into the framework using the ennOpenModel function. 3. Allocate and commit the necessary buffers using the ennAllocateAllBuffers function. This preparation is crucial as it sets up the application to enhance the low-light quality of images from stored files or those captured via the camera, ensuring that the model is ready to process the input data effectively.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation\\documentation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:25:51.314092+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What are the steps to run the segmentation project in Android Studio after cloning the repository, and how does it compare to running the pose estimation project?",
    "reference_contexts": [
      "<1-hop>\n\nThis document explains how a simple Android Sample Application operates using the [DeeplabV3](https://prd.ai-studio-farm.com/kr/solution/ai/models/detail/ca3dc3fa-d687-474c-9679-dcee50da8b52) model optimized for Exynos hardware. **1\\. Functionality** This Application provides segmentation results for images either from stored image files or captured via the camera. Each pixel of the segmented object is overlaid with a color corresponding to its label, offering a visual representation of the classification. ![](images/sample_application/deepable.png) **2\\. Prerequisites** * To download [Android Studio](https://developer.android.com/studio), go to the official website and download it. * To run the Sample Application, clone the following repository. ```sh git clone https://github.com/exynos-eco/enn-sdk-samples-9945.git ``` * If there is no device available to run the application, you can use a real device provided in the Device Farm. For guidance on connecting a device in Android Studio, refer to the ADB proxy guide. **3\\. Starting Sample Application** * Open the cmd window and check if adb is connected. ```sh adb device List of devices attached 000011b58d246013 device ``` * Push a sample image for testing to the following path using adb push. ```sh adb push sample.jpg /storage/emulated/0/Pictures/ ``` * If the sample image is not visible after running the Sample Application, the media scanner may not have been updated, preventing the app from detecting the file. In this case, run the following command in adb shell to trigger media scanning. ```sh adb adb shell am broadcast -a android.intent.action.MEDIA_SCANNER_SCAN_FILE -d file:///storage/emulated/0/Pictures/sample.jpg ``` * After confirming a proper connection with Device Farm, select Tools -> Device Manager in Android Studio. ![](images/sample_application/device_manager_1.png) * After that check if the physical device is properly connected. ![](images/sample_application/samsung_electronics_1.png) * Run the Segmentation project from the Sample Applications obtained via git clone in Android Studio. ![](images/sample_application/segmentation.png) * When you select the segmentation project Android Studio will automatically perform a gradle sync. * The application in the selected project checks the connected device and runs when you click the Run button. ![](images/sample_application/run_1.png) * Select image mode and provide the data to be used for inference. ![](images/sample_application/camera_1.png) **4\\.",
      "<2-hop>\n\nThis document explains how a simple Android Sample Application operates using the [Posenet](https://prd.ai-studio-farm.com/kr/solution/ai/models/detail/f40473cb-a6e6-42a3-a50a-daf428273eab) model optimized for Exynos hardware **1\\. Functionality** This Application detects key points in images from stored image files or those captured via the camera and automatically measures joint positions. Additionally, the inference time is displayed at the bottom of the application interface. ![](images/sample_application/pose_estimation.png) **2\\. Prerequisites** * To download [Android Studio](https://developer.android.com/studio), go to the official website and download it. * To run the Sample Application, clone the following repository. ```sh git clone https://github.com/exynos-eco/enn-sdk-samples-9945.git ``` * If there is no device available to run the application, you can use a real device provided in the Device Farm. For guidance on connecting a device in Android Studio, refer to the ADB proxy guide. **3\\. Starting Sample Application** * Open the cmd window and check if adb is connected. ```sh adb device List of devices attached 000011b58d246013 device ``` * Push a sample image for testing to the following path using adb push. ```sh adb push sample.jpg /storage/emulated/0/Pictures/ ``` * If the sample image is not visible after running the Sample Application, the media scanner may not have been updated, preventing the app from detecting the file. In this case, run the following command in adb shell to trigger media scanning. ```sh adb adb shell am broadcast -a android.intent.action.MEDIA_SCANNER_SCAN_FILE -d file:///storage/emulated/0/Pictures/sample.jpg ``` * After confirming a proper connection with Device Farm, select Tools -> Device Manager in Android Studio. ![](images/sample_application/device_manager_1.png) * After that check if the physical device is properly connected. ![](images/sample_application/samsung_electronics_1.png) * Run the Pose Estimation project from the Sample Applications obtained via git clone in Android Studio. ![](images/sample_application/pose_estimation_android.png) * When you select the pose-estimation project Android Studio will automatically perform a gradle sync. * The application in the selected project checks the connected device and runs when you click the Run button. ![](images/sample_application/run_1.png) * Select image mode and provide the data to be used for inference. ![](images/sample_application/camera_1.png) **4\\. I/O Specs** | INPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | Image | An RGB image | \\[1, 3, 513, 257\\] | float32 | | OUTPUT | Description | Shape | Data Type | | :---: | :---: | :---: | :---: | | Classes | Human Body Part | \\[1, 17, 33, 17\\] | float32 | **5\\."
    ],
    "reference": "To run the segmentation project in Android Studio after cloning the repository, follow these steps: First, ensure that you have downloaded Android Studio and cloned the repository using the command `git clone https://github.com/exynos-eco/enn-sdk-samples-9945.git`. Next, open the command window and check if adb is connected by running `adb device`. Then, push a sample image for testing to the specified path using `adb push sample.jpg /storage/emulated/0/Pictures/`. If the sample image is not visible, trigger media scanning with the command `adb shell am broadcast -a android.intent.action.MEDIA_SCANNER_SCAN_FILE -d file:///storage/emulated/0/Pictures/sample.jpg`. After confirming a proper connection with Device Farm, select Tools -> Device Manager in Android Studio to check if the physical device is connected. Finally, run the Segmentation project from the Sample Applications in Android Studio, which will automatically perform a gradle sync and check the connected device when you click the Run button. In comparison, the steps to run the pose estimation project are similar: you also need to clone the repository, check adb connection, push a sample image, and ensure the device is connected. The main difference lies in the functionality of the applications, where the segmentation project overlays colors on segmented objects, while the pose estimation project detects key points and measures joint positions.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation\\documentation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:25:51.314092+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What EnnDeinitialize do?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnInitialize](#function-enninitialize)**(void )<br>Initialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair.  |\n| EnnReturn | **[EnnDeinitialize](#function-enndeinitialize)**(void )<br>Deinitialize Enn Framework. Framework degenerates context in a caller's process.  |"
    ],
    "reference": "EnnDeinitialize is used to deinitialize the Enn Framework, which degenerates context in a caller's process.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:26:33.175655+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How Enn Framework initialize and what it return?",
    "reference_contexts": [
      "```cpp\nEnnReturn EnnInitialize(\nvoid\n)\n```  \nInitialize Enn Framework. Framework generates context in a caller's process. Context counts initialize/deinitialize pair.  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "Enn Framework initialize by calling the function EnnInitialize, which generates context in a caller's process. The context counts the initialize/deinitialize pair, and it returns an EnnReturn result, where 0 indicates success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:26:33.175655+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What EnnReturn do when deinitialize?",
    "reference_contexts": [
      "```cpp\nEnnReturn EnnDeinitialize(\nvoid\n)\n```  \nDeinitialize Enn Framework. Framework degenerates context in a caller's process.  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "EnnReturn is the result of deinitializing the Enn Framework, where 0 indicates success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:26:33.175655+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Can you explain what EnnReturn is and its significance in the context of model management?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnOpenModel](#function-ennopenmodel)**(const char * model_file, EnnModelId * model_id)<br>OpenModel with model file.  |\n| EnnReturn | **[EnnOpenModelFromMemory](#function-ennopenmodelfrommemory)**(const char * va, const uint32_t size, EnnModelId * model_id)<br>OpenModel from memory buffer.  |\n| EnnReturn | **[EnnCloseModel](#function-ennclosemodel)**(const EnnModelId model_id);<br>Close model and free all resources in OpenModel()|"
    ],
    "reference": "EnnReturn is a return type associated with various functions in model management, such as EnnOpenModel, EnnOpenModelFromMemory, and EnnCloseModel. It indicates the success or failure of operations related to opening and closing models, as well as managing resources in the context of model frameworks.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:26:33.175655+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What does the EnnOpenModel function return upon success?",
    "reference_contexts": [
      "```cpp\nEnnReturn EnnOpenModel(\nconst char * model_file,\nEnnModelId * model_id\n)\n```  \nOpenModel with model file.  \n**Parameters**:  \n* **model_file** [IN] model_file, output from graph-gen. A caller should access the file.\n* **model_id** [OUT] model_id, 64 bit unsigned int  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "The EnnOpenModel function returns an EnnReturn result, where 0 indicates success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:26:33.175655+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What does the EnnReturn type represent in the context of the EnnOpenModelFromMemory function?",
    "reference_contexts": [
      "```cpp\nEnnReturn EnnOpenModelFromMemory(\nconst char * va,\nconst uint32_t size,\nEnnModelId * model_id\n)\n```  \nOpenModel from memory buffer.  \n**Parameters**:  \n* **va** [IN] address which a model loaded from\n* **size** [IN] size of the buffer\n* **model_id** [OUT] model_id, 64 bit unsigned int  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "In the context of the EnnOpenModelFromMemory function, EnnReturn represents the result of the operation, where a return value of 0 indicates success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:26:33.175655+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What does the EnnReturn function signify in the context of closing a model?",
    "reference_contexts": [
      "```cpp\nEnnReturn EnnCloseModel(\nconst EnnModelId model_id\n);\n```  \nClose model and free all resources in OpenModel().  \n**Parameters**:  \n* **model_id** [IN] model_id from OpenModel()  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "The EnnReturn function signifies the result of closing a model and freeing all resources in OpenModel(). It returns an EnnReturn result, where 0 indicates success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:26:33.175655+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the role of NumberOfBuffersInfo in buffer allocation?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnCreateBuffer](#function-enncreatebuffer)**([EnnBufferPtr](api-reference/enn-framework-data-type-references/#_ennbuffer) * out, const uint32_t req_size, const bool is_cached =true)<br>Create Buffer with request size support ION or dmabufheap which can be used in a device(DSP/CPU/NPU/GPU)  |\n| EnnReturn | **[EnnAllocateAllBuffers](#function-ennallocateallbuffers)**(const EnnModelId model_id, [EnnBufferPtr](api-reference/enn-framework-data-type-references/#_ennbuffer) ** out_buffers, [NumberOfBuffersInfo](api-reference/enn-framework-data-type-references/#_numberofbuffersinfo) * buf_info, const int session_id =0, const bool do_commit =true)<br>Allocate all buffers which a caller should allocate.  |\n| EnnReturn | **[EnnReleaseBuffers](#function-ennreleasebuffers)**([EnnBufferPtr](api-reference/enn-framework-data-type-references/#_ennbuffer) * buffers, const int32_t numOfBuffers)<br>Release buffer array from EnnAllocatedAllBuffers() This API includes releasing all elements in the array.  |\n| EnnReturn | **[EnnReleaseBuffer](#function-ennreleasebuffer)**([EnnBufferPtr](api-reference/enn-framework-data-type-references/#_ennbuffer) buffer)<br>release buffer from [EnnCreateBuffer()](#function-enncreatebuffer) |"
    ],
    "reference": "NumberOfBuffersInfo is used in the function EnnAllocateAllBuffers to provide information about the buffers that a caller should allocate.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:26:33.175655+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How does the function EnnOpenModelFromMemory utilize the memory buffer, and what role does EnnSetPreferencePerfMode play in optimizing performance during model execution?",
    "reference_contexts": [
      "<1-hop>\n\n```cpp\nEnnReturn EnnOpenModelFromMemory(\nconst char * va,\nconst uint32_t size,\nEnnModelId * model_id\n)\n```  \nOpenModel from memory buffer.  \n**Parameters**:  \n* **va** [IN] address which a model loaded from\n* **size** [IN] size of the buffer\n* **model_id** [OUT] model_id, 64 bit unsigned int  \n**Return**: EnnReturn result, 0 is success",
      "<2-hop>\n\n|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnSetPreferencePresetId](#function-ennsetpreferencepresetid)**(const uint32_t val)<br>Setting Preset ID for operation performance.  |\n| EnnReturn | **[EnnSetPreferencePerfConfigId](#function-ennsetpreferenceperfconfigid)**(const uint32_t val)<br>Setting PerfConfig ID for operation performance.  |\n| EnnReturn | **[EnnSetPreferencePerfMode](#function-ennsetpreferenceperfmode)**(const uint32_t val)<br>Setting Performance Mode.  |\n| EnnReturn | **[EnnSetPreferenceTimeOut](#function-ennsetpreferencetimeout)**(const uint32_t val)<br>Setting Preset ID for time out.  |\n| EnnReturn | **[EnnSetPreferencePriority](#function-ennsetpreferencepriority)**(const uint32_t val)<br>Setting priority value for NPU.  |\n| EnnReturn | **[EnnSetPreferenceCoreAffinity](#function-ennsetpreferencecoreaffinity)**(const uint32_t val)<br>Setting affinity to set NPU core operation.  |\n| EnnReturn | **[EnnGetPreferencePresetId](#function-enngetpreferencepresetid)**(uint32_t * val_ptr)<br>Get current information for Preset ID.  |\n| EnnReturn | **[EnnGetPreferencePerfConfigId](#function-enngetpreferenceperfconfigid)**(uint32_t * val_ptr)<br>Get current information for PerfConfig ID.  |\n| EnnReturn | **[EnnGetPreferencePerfMode](#function-enngetpreferenceperfmode)**(uint32_t * val_ptr)<br>Get current information for Performance Mode.  |\n| EnnReturn | **[EnnGetPreferenceTimeOut](#function-enngetpreferencetimeout)**(uint32_t * val_ptr)<br>Get current information for Time Out.  |\n| EnnReturn | **[EnnGetPreferencePriority](#function-enngetpreferencepriority)**(uint32_t * val_ptr)<br>Get current information for NPU Priority.  |\n| EnnReturn | **[EnnGetPreferenceCoreAffinity](#function-enngetpreferencecoreaffinity)**(uint32_t * val_ptr)<br>Get current information for NPU Core affinity.  |\n| EnnReturn | **[EnnGetMetaInfo](#function-enngetmetainfo)**(const EnnMetaTypeId info_id, const EnnModelId model_id, char output_str[ENN_INFO_GRAPH_STR_LENGTH_MAX])<br>Get Meta Information.  |\n| EnnReturn | **[EnnSetExecMsgAlwaysOn](#function-ennsetexecmsgalwayson)**()<br>Set frequency of execution message print.  |"
    ],
    "reference": "The function EnnOpenModelFromMemory utilizes the memory buffer by taking an address from which a model is loaded and the size of that buffer as parameters. This allows the model to be opened directly from memory, facilitating efficient memory management. On the other hand, EnnSetPreferencePerfMode plays a crucial role in optimizing performance during model execution by allowing the user to set a specific performance mode, which can enhance the operational efficiency of the model being executed.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:26:33.175655+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the role of the parameter 'val' in the functions EnnSetPreferencePerfMode and EnnGetPreferencePresetId?",
    "reference_contexts": [
      "<1-hop>\n\n```cpp\nEnnReturn EnnSetPreferencePerfMode(\nconst uint32_t val\n)\n```  \nSetting Performance Mode.  \n**Parameters**:  \n* **val** [IN] value to set Performance Mode  \n**Return**: EnnReturn result, 0 is success",
      "<2-hop>\n\n```cpp\nEnnReturn EnnGetPreferencePresetId(\nuint32_t * val_ptr\n)\n```  \nGet current information for Preset ID.  \n**Parameters**:  \n* **val** [OUT] current value of Preset ID  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "In the function EnnSetPreferencePerfMode, the parameter 'val' is used as an input to set the Performance Mode, while in the function EnnGetPreferencePresetId, 'val' is used as an output parameter to retrieve the current value of the Preset ID.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\enn-framework-api-functions.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:26:33.175655+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Can you explain what the EnnExecuteModel function does in the context of executing machine learning models?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnExecuteModel](Modules/group__api__execute.md#function-ennexecutemodel)**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model with commited buffers.  |\n| EnnReturn | **[EnnExecuteModelAsync](Modules/group__api__execute.md#function-ennexecutemodelasync)**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model in background asynchronously.  |\n| EnnReturn | **[EnnExecuteModelWait](Modules/group__api__execute.md#function-ennexecutemodelwait)**(const EnnModelId model_id, const int session_id =0)<br>Wait result of calling [EnnExecuteModelAsync()](Modules/group__api__execute.md#function-ennexecutemodelasync) If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished.  |"
    ],
    "reference": "The EnnExecuteModel function requests the service core to execute a model with committed buffers, using the specified model_id and an optional session_id.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:01.098327+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Can you explain the functionality of EnnExecuteModel in the context of executing machine learning models?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnExecuteModel](Modules/group__api__execute.md#function-ennexecutemodel)**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model with commited buffers.  |\n| EnnReturn | **[EnnExecuteModelAsync](Modules/group__api__execute.md#function-ennexecutemodelasync)**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model in background asynchronously.  |\n| EnnReturn | **[EnnExecuteModelWait](Modules/group__api__execute.md#function-ennexecutemodelwait)**(const EnnModelId model_id, const int session_id =0)<br>Wait result of calling [EnnExecuteModelAsync()](Modules/group__api__execute.md#function-ennexecutemodelasync) If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished.  |"
    ],
    "reference": "EnnExecuteModel is a function that requests the service core to execute a model with committed buffers. It is designed to facilitate the execution of machine learning models by providing a direct call to the core service.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:01.098327+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Could you elaborate on the functionality and parameters of the EnnExecuteModel function as described in the context?",
    "reference_contexts": [
      "```\nEnnReturn EnnExecuteModel(\nconst EnnModelId model_id,\nconst int session_id =0\n)\n```  \nRequest to service core to execute model with commited buffers.  \n**Parameters**:  \n* **model_id** [IN] model ID from load_model\n* **session_id** [IN] session ID  \n**Return**: EnnReturn result, 0 is success  \n**Note**: this function runs in block mode"
    ],
    "reference": "The EnnExecuteModel function is a request to the service core to execute a model using committed buffers. It takes two parameters: model_id, which is the model ID obtained from the load_model function, and session_id, which is an optional parameter with a default value of 0. The function returns an EnnReturn result, where a return value of 0 indicates success. It is important to note that this function operates in block mode.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:01.098327+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the purpose of model_id in the EnnExecuteModel function?",
    "reference_contexts": [
      "```\nEnnReturn EnnExecuteModel(\nconst EnnModelId model_id,\nconst int session_id =0\n)\n```  \nRequest to service core to execute model with commited buffers.  \n**Parameters**:  \n* **model_id** [IN] model ID from load_model\n* **session_id** [IN] session ID  \n**Return**: EnnReturn result, 0 is success  \n**Note**: this function runs in block mode"
    ],
    "reference": "The model_id is an input parameter that represents the model ID from the load_model function, which is used to execute the model with committed buffers.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:01.098327+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What does the EnnReturn type signify in the context of the EnnExecuteModelAsync function, and what are its parameters?",
    "reference_contexts": [
      "```\nEnnReturn EnnExecuteModelAsync(\nconst EnnModelId model_id,\nconst int session_id =0\n)\n```  \nRequest to service core to execute model in background asynchronously.  \n**Parameters**:  \n* **model_id** [IN] model ID from load_model\n* **session_id** [IN] session ID  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "In the context of the EnnExecuteModelAsync function, EnnReturn signifies the result of the request to the service core to execute a model in the background asynchronously. The function has two parameters: model_id, which is the model ID obtained from load_model, and session_id, which is an optional session ID. A return value of 0 indicates success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:01.098327+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Can you tell me what EnnExecuteModelAsync do and what it need for work?",
    "reference_contexts": [
      "```\nEnnReturn EnnExecuteModelAsync(\nconst EnnModelId model_id,\nconst int session_id =0\n)\n```  \nRequest to service core to execute model in background asynchronously.  \n**Parameters**:  \n* **model_id** [IN] model ID from load_model\n* **session_id** [IN] session ID  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "EnnExecuteModelAsync is a request to service core to execute a model in the background asynchronously. It requires two parameters: model_id, which is the model ID from load_model, and session_id, which is the session ID. The function returns an EnnReturn result, where 0 indicates success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:01.098327+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the purpose of the EnnExecuteModelWait function in machine learning model execution?",
    "reference_contexts": [
      "```\nEnnReturn EnnExecuteModelWait(\nconst EnnModelId model_id,\nconst int session_id =0\n)\n```  \nWait result of calling [EnnExecuteModelAsync()](Modules/group__api__execute.md#function-ennexecutemodelasync) If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished.  \n**Parameters**:  \n* **model_id** [IN] model ID from load_model\n* **session_id** [IN] session ID  \n**Return**: EnnReturn result, 0 is success  \n-------------------------------  \nUpdated on 2023-08-11 at 16:24:05 +0900"
    ],
    "reference": "The EnnExecuteModelWait function is used to wait for the result of calling EnnExecuteModelAsync. If the execution is finished, this function returns immediately; if not, it blocks until the execution is completed. It takes parameters such as model_id, which is the model ID from load_model, and session_id, which is the session ID. The function returns an EnnReturn result, where 0 indicates success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:01.098327+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What EnnExecuteModelWait do?",
    "reference_contexts": [
      "```\nEnnReturn EnnExecuteModelWait(\nconst EnnModelId model_id,\nconst int session_id =0\n)\n```  \nWait result of calling [EnnExecuteModelAsync()](Modules/group__api__execute.md#function-ennexecutemodelasync) If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished.  \n**Parameters**:  \n* **model_id** [IN] model ID from load_model\n* **session_id** [IN] session ID  \n**Return**: EnnReturn result, 0 is success  \n-------------------------------  \nUpdated on 2023-08-11 at 16:24:05 +0900"
    ],
    "reference": "EnnExecuteModelWait waits for the result of calling EnnExecuteModelAsync. If execution is finished, it returns immediately; if not, it blocks until execution is finished.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:01.098327+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the difference between the EnnExecuteModelWait function and the EnnExecuteModelAsync function in terms of execution behavior?",
    "reference_contexts": [
      "<1-hop>\n\n```\nEnnReturn EnnExecuteModelWait(\nconst EnnModelId model_id,\nconst int session_id =0\n)\n```  \nWait result of calling [EnnExecuteModelAsync()](Modules/group__api__execute.md#function-ennexecutemodelasync) If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished.  \n**Parameters**:  \n* **model_id** [IN] model ID from load_model\n* **session_id** [IN] session ID  \n**Return**: EnnReturn result, 0 is success  \n-------------------------------  \nUpdated on 2023-08-11 at 16:24:05 +0900",
      "<2-hop>\n\n|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnExecuteModel](Modules/group__api__execute.md#function-ennexecutemodel)**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model with commited buffers.  |\n| EnnReturn | **[EnnExecuteModelAsync](Modules/group__api__execute.md#function-ennexecutemodelasync)**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model in background asynchronously.  |\n| EnnReturn | **[EnnExecuteModelWait](Modules/group__api__execute.md#function-ennexecutemodelwait)**(const EnnModelId model_id, const int session_id =0)<br>Wait result of calling [EnnExecuteModelAsync()](Modules/group__api__execute.md#function-ennexecutemodelasync) If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished.  |"
    ],
    "reference": "The EnnExecuteModelWait function waits for the result of calling the EnnExecuteModelAsync function. If the execution is finished, EnnExecuteModelWait returns immediately; if not, it blocks until the execution is complete. In contrast, the EnnExecuteModelAsync function requests the service core to execute the model in the background asynchronously, allowing other operations to continue without waiting for the execution to finish.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:01.098327+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the difference between EnnExecuteModelAsync and EnnExecuteModelWait in terms of execution and result handling?",
    "reference_contexts": [
      "<1-hop>\n\n|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnExecuteModel](Modules/group__api__execute.md#function-ennexecutemodel)**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model with commited buffers.  |\n| EnnReturn | **[EnnExecuteModelAsync](Modules/group__api__execute.md#function-ennexecutemodelasync)**(const EnnModelId model_id, const int session_id =0)<br>Request to service core to execute model in background asynchronously.  |\n| EnnReturn | **[EnnExecuteModelWait](Modules/group__api__execute.md#function-ennexecutemodelwait)**(const EnnModelId model_id, const int session_id =0)<br>Wait result of calling [EnnExecuteModelAsync()](Modules/group__api__execute.md#function-ennexecutemodelasync) If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished.  |",
      "<2-hop>\n\n```\nEnnReturn EnnExecuteModelWait(\nconst EnnModelId model_id,\nconst int session_id =0\n)\n```  \nWait result of calling [EnnExecuteModelAsync()](Modules/group__api__execute.md#function-ennexecutemodelasync) If execution is finished, this function is returned intermediatelly If not, this function would be blocked until the execution finished.  \n**Parameters**:  \n* **model_id** [IN] model ID from load_model\n* **session_id** [IN] session ID  \n**Return**: EnnReturn result, 0 is success  \n-------------------------------  \nUpdated on 2023-08-11 at 16:24:05 +0900"
    ],
    "reference": "EnnExecuteModelAsync requests the service core to execute a model in the background asynchronously, allowing other processes to continue without waiting for the model execution to complete. In contrast, EnnExecuteModelWait waits for the result of the EnnExecuteModelAsync call. If the execution is finished, it returns immediately; if not, it blocks until the execution is complete. This means that EnnExecuteModelWait is used when the user needs to ensure that the model execution has finished before proceeding.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__execute.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:01.098327+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the role of EnnBufferPtr in memory management operations?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnCreateBuffer](Modules/group__api__memory.md#function-enncreatebuffer)**([EnnBufferPtr](Classes/struct__enn_buffer.md) * out, const uint32_t req_size, const bool is_cached =true)<br>Create Buffer with request size support ION or dmabufheap which can be used in a device(DSP/CPU/NPU/GPU)  |\n| EnnReturn | **[EnnAllocateAllBuffers](Modules/group__api__memory.md#function-ennallocateallbuffers)**(const EnnModelId model_id, [EnnBufferPtr](Classes/struct__enn_buffer.md) ** out_buffers, [NumberOfBuffersInfo](Classes/struct___number_of_buffers_info.md) * buf_info, const int session_id =0, const bool do_commit =true)<br>Allocate all buffers which a caller should allocate.  |\n| EnnReturn | **[EnnReleaseBuffers](Modules/group__api__memory.md#function-ennreleasebuffers)**([EnnBufferPtr](Classes/struct__enn_buffer.md) * buffers, const int32_t numOfBuffers)<br>Release buffer array from EnnAllocatedAllBuffers() This API includes releasing all elements in the array.  |\n| EnnReturn | **[EnnReleaseBuffer](Modules/group__api__memory.md#function-ennreleasebuffer)**([EnnBufferPtr](Classes/struct__enn_buffer.md) buffer)<br>release buffer from [EnnCreateBuffer()](Modules/group__api__memory.md#function-enncreatebuffer) |"
    ],
    "reference": "EnnBufferPtr is used in various functions related to memory management, such as creating buffers with the EnnCreateBuffer function, allocating all buffers with EnnAllocateAllBuffers, and releasing buffers with EnnReleaseBuffers and EnnReleaseBuffer.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__memory.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:21.706831+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How can I create a buffer for GPU using the provided function?",
    "reference_contexts": [
      "```\nEnnReturn EnnCreateBuffer(\nEnnBufferPtr * out,\nconst uint32_t req_size,\nconst bool is_cached =true\n)\n```  \nCreate Buffer with request size support ION or dmabufheap which can be used in a device(DSP/CPU/NPU/GPU)  \n**Parameters**:  \n* **out** [OUT] output buffer pointer. User can get va, size, offset through *out\n* **req_size** [IN] request size\n* **is_cached** [IN] flag, the buffer uses cache or not  \n**Return**: EnnReturn"
    ],
    "reference": "To create a buffer for GPU using the provided function, you can call `EnnCreateBuffer` with the appropriate parameters. You need to pass an output buffer pointer `out`, specify the required size `req_size`, and indicate whether the buffer should use caching by setting the `is_cached` flag to true or false.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__memory.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:21.706831+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What are the parameters and return value of the EnnCreateBuffer function?",
    "reference_contexts": [
      "```\nEnnReturn EnnCreateBuffer(\nEnnBufferPtr * out,\nconst uint32_t req_size,\nconst bool is_cached =true\n)\n```  \nCreate Buffer with request size support ION or dmabufheap which can be used in a device(DSP/CPU/NPU/GPU)  \n**Parameters**:  \n* **out** [OUT] output buffer pointer. User can get va, size, offset through *out\n* **req_size** [IN] request size\n* **is_cached** [IN] flag, the buffer uses cache or not  \n**Return**: EnnReturn"
    ],
    "reference": "The EnnCreateBuffer function has three parameters: 'out', which is an output buffer pointer that allows the user to get the virtual address, size, and offset; 'req_size', which is the requested size of the buffer; and 'is_cached', which is a flag indicating whether the buffer uses cache or not. The function returns an EnnReturn value.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__memory.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:21.706831+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What buf_info do in EnnAllocateAllBuffers?",
    "reference_contexts": [
      "```\nEnnReturn EnnAllocateAllBuffers(\nconst EnnModelId model_id,\nEnnBufferPtr ** out_buffers,\nNumberOfBuffersInfo * buf_info,\nconst int session_id =0,\nconst bool do_commit =true\n)\n```  \nAllocate all buffers which a caller should allocate.  \n**Parameters**:  \n* **model_id** model_id from OpenModel()\n* **out_buffers** [OUT] pointer of EnnBuffer array\n* **buf_info** [OUT] size of the array\n* **session_id** [IN] after generate buffer space, user can set this field if session_id > 0\n* **do_commit** [IN] if true, the framework tries to commit after buffer allocation  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "buf_info is an output parameter that provides the size of the array of buffers that the caller should allocate.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__memory.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:21.706831+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Can you explain the function of EnnBufferPtr in the context of the EnnReleaseBuffers API, including its parameters and return value?",
    "reference_contexts": [
      "```\nEnnReturn EnnReleaseBuffers(\nEnnBufferPtr * buffers,\nconst int32_t numOfBuffers\n)\n```  \nRelease buffer array from EnnAllocatedAllBuffers() This API includes releasing all elements in the array.  \n**Parameters**:  \n* **buffers** [IN] pointer of buffer array\n* **numOfBuffers** [IN] size of bufefr array  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "The EnnBufferPtr is used as a pointer to a buffer array in the EnnReleaseBuffers API. This API is responsible for releasing all elements in the buffer array that was allocated by EnnAllocatedAllBuffers(). The parameters include 'buffers', which is an [IN] pointer to the buffer array, and 'numOfBuffers', which is an [IN] parameter indicating the size of the buffer array. The function returns an EnnReturn result, where a return value of 0 indicates success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__memory.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:21.706831+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the function of EnnAllocatedAllBuffers in memory management?",
    "reference_contexts": [
      "```\nEnnReturn EnnReleaseBuffers(\nEnnBufferPtr * buffers,\nconst int32_t numOfBuffers\n)\n```  \nRelease buffer array from EnnAllocatedAllBuffers() This API includes releasing all elements in the array.  \n**Parameters**:  \n* **buffers** [IN] pointer of buffer array\n* **numOfBuffers** [IN] size of bufefr array  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "EnnAllocatedAllBuffers is responsible for releasing the buffer array, which includes releasing all elements in the array.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__memory.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:21.706831+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the process for buffer creation and allocation using EnnReturn in the context of memory management?",
    "reference_contexts": [
      "<1-hop>\n\n```\nEnnReturn EnnAllocateAllBuffers(\nconst EnnModelId model_id,\nEnnBufferPtr ** out_buffers,\nNumberOfBuffersInfo * buf_info,\nconst int session_id =0,\nconst bool do_commit =true\n)\n```  \nAllocate all buffers which a caller should allocate.  \n**Parameters**:  \n* **model_id** model_id from OpenModel()\n* **out_buffers** [OUT] pointer of EnnBuffer array\n* **buf_info** [OUT] size of the array\n* **session_id** [IN] after generate buffer space, user can set this field if session_id > 0\n* **do_commit** [IN] if true, the framework tries to commit after buffer allocation  \n**Return**: EnnReturn result, 0 is success",
      "<2-hop>\n\n|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnCreateBuffer](Modules/group__api__memory.md#function-enncreatebuffer)**([EnnBufferPtr](Classes/struct__enn_buffer.md) * out, const uint32_t req_size, const bool is_cached =true)<br>Create Buffer with request size support ION or dmabufheap which can be used in a device(DSP/CPU/NPU/GPU)  |\n| EnnReturn | **[EnnAllocateAllBuffers](Modules/group__api__memory.md#function-ennallocateallbuffers)**(const EnnModelId model_id, [EnnBufferPtr](Classes/struct__enn_buffer.md) ** out_buffers, [NumberOfBuffersInfo](Classes/struct___number_of_buffers_info.md) * buf_info, const int session_id =0, const bool do_commit =true)<br>Allocate all buffers which a caller should allocate.  |\n| EnnReturn | **[EnnReleaseBuffers](Modules/group__api__memory.md#function-ennreleasebuffers)**([EnnBufferPtr](Classes/struct__enn_buffer.md) * buffers, const int32_t numOfBuffers)<br>Release buffer array from EnnAllocatedAllBuffers() This API includes releasing all elements in the array.  |\n| EnnReturn | **[EnnReleaseBuffer](Modules/group__api__memory.md#function-ennreleasebuffer)**([EnnBufferPtr](Classes/struct__enn_buffer.md) buffer)<br>release buffer from [EnnCreateBuffer()](Modules/group__api__memory.md#function-enncreatebuffer) |"
    ],
    "reference": "The process for buffer creation and allocation using EnnReturn involves two main functions: EnnCreateBuffer and EnnAllocateAllBuffers. EnnCreateBuffer is used to create a buffer with a requested size, which supports ION or dmabufheap for use in devices like DSP, CPU, NPU, or GPU. On the other hand, EnnAllocateAllBuffers allocates all buffers that a caller should allocate, taking parameters such as model_id, out_buffers, buf_info, session_id, and do_commit. The function returns an EnnReturn result, where 0 indicates success.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__memory.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:21.706831+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the process for creating and releasing buffers using EnnBufferPtr in the context of memory management?",
    "reference_contexts": [
      "<1-hop>\n\n```\nEnnReturn EnnReleaseBuffers(\nEnnBufferPtr * buffers,\nconst int32_t numOfBuffers\n)\n```  \nRelease buffer array from EnnAllocatedAllBuffers() This API includes releasing all elements in the array.  \n**Parameters**:  \n* **buffers** [IN] pointer of buffer array\n* **numOfBuffers** [IN] size of bufefr array  \n**Return**: EnnReturn result, 0 is success",
      "<2-hop>\n\n```\nEnnReturn EnnCreateBuffer(\nEnnBufferPtr * out,\nconst uint32_t req_size,\nconst bool is_cached =true\n)\n```  \nCreate Buffer with request size support ION or dmabufheap which can be used in a device(DSP/CPU/NPU/GPU)  \n**Parameters**:  \n* **out** [OUT] output buffer pointer. User can get va, size, offset through *out\n* **req_size** [IN] request size\n* **is_cached** [IN] flag, the buffer uses cache or not  \n**Return**: EnnReturn"
    ],
    "reference": "The process for creating and releasing buffers using EnnBufferPtr involves two main APIs. First, the `EnnCreateBuffer` function is used to create a buffer with a specified request size, which can be utilized by various devices such as DSP, CPU, NPU, or GPU. This function takes parameters including an output pointer for the buffer, the requested size, and a flag indicating whether the buffer should be cached. After buffers are created, the `EnnReleaseBuffers` function is called to release the buffer array, which includes all elements in the array allocated by `EnnAllocatedAllBuffers`. This function requires a pointer to the buffer array and the number of buffers to be released, returning a result indicating success or failure.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__memory.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:21.706831+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the role of enn_buf_dir_e in the EnnGetBufferInfoByIndex function?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnGetBuffersInfo](Modules/group__api__model.md#function-enngetbuffersinfo)**([NumberOfBuffersInfo](Classes/struct___number_of_buffers_info.md) * buffers_info, const EnnModelId model_id)<br>Get buffers information from loaded model.  |\n| EnnReturn | **[EnnGetBufferInfoByIndex](Modules/group__api__model.md#function-enngetbufferinfobyindex)**([EnnBufferInfo](Classes/struct__enn_buffer_info.md) * out_buf_info, const EnnModelId model_id, const enn_buf_dir_e direction, const uint32_t index)<br>Get one buffer information from loaded model.  |\n| EnnReturn | **[EnnGetBufferInfoByLabel](Modules/group__api__model.md#function-enngetbufferinfobylabel)**([EnnBufferInfo](Classes/struct__enn_buffer_info.md) * out_buf_info, const EnnModelId model_id, const char * label)<br>Get one buffer information from loaded model.  |\n| EnnReturn | **[EnnSetBufferByIndex](Modules/group__api__model.md#function-ennsetbufferbyindex)**(const EnnModelId model_id, const enn_buf_dir_e direction, const uint32_t index, [EnnBufferPtr](Classes/struct__enn_buffer.md) buf, const int session_id =0)<br>Set memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core.  |\n| EnnReturn | **[EnnSetBufferByLabel](Modules/group__api__model.md#function-ennsetbufferbylabel)**(const EnnModelId model_id, const char * label, [EnnBufferPtr](Classes/struct__enn_buffer.md) buf, const int session_id =0)<br>Set memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core.  |"
    ],
    "reference": "The enn_buf_dir_e parameter in the EnnGetBufferInfoByIndex function specifies the direction of the buffer information being retrieved from the loaded model.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:51.108069+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What EnnGetBuffersInfo do?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnGetBuffersInfo](Modules/group__api__model.md#function-enngetbuffersinfo)**([NumberOfBuffersInfo](Classes/struct___number_of_buffers_info.md) * buffers_info, const EnnModelId model_id)<br>Get buffers information from loaded model.  |\n| EnnReturn | **[EnnGetBufferInfoByIndex](Modules/group__api__model.md#function-enngetbufferinfobyindex)**([EnnBufferInfo](Classes/struct__enn_buffer_info.md) * out_buf_info, const EnnModelId model_id, const enn_buf_dir_e direction, const uint32_t index)<br>Get one buffer information from loaded model.  |\n| EnnReturn | **[EnnGetBufferInfoByLabel](Modules/group__api__model.md#function-enngetbufferinfobylabel)**([EnnBufferInfo](Classes/struct__enn_buffer_info.md) * out_buf_info, const EnnModelId model_id, const char * label)<br>Get one buffer information from loaded model.  |\n| EnnReturn | **[EnnSetBufferByIndex](Modules/group__api__model.md#function-ennsetbufferbyindex)**(const EnnModelId model_id, const enn_buf_dir_e direction, const uint32_t index, [EnnBufferPtr](Classes/struct__enn_buffer.md) buf, const int session_id =0)<br>Set memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core.  |\n| EnnReturn | **[EnnSetBufferByLabel](Modules/group__api__model.md#function-ennsetbufferbylabel)**(const EnnModelId model_id, const char * label, [EnnBufferPtr](Classes/struct__enn_buffer.md) buf, const int session_id =0)<br>Set memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core.  |"
    ],
    "reference": "EnnGetBuffersInfo gets buffers information from loaded model.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:51.108069+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What are the parameters and return value of the EnnGetBuffersInfo function in OpenModel?",
    "reference_contexts": [
      "```\nEnnReturn EnnGetBuffersInfo(\nNumberOfBuffersInfo * buffers_info,\nconst EnnModelId model_id\n)\n```  \nGet buffers information from loaded model.  \n**Parameters**:  \n* **buffers_info** [OUT] number of in / out buffer which caller should commit.\n* **model_id** [IN] model id from OpenModel()  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "The EnnGetBuffersInfo function retrieves buffers information from a loaded model. It has two parameters: 'buffers_info', which is an output parameter indicating the number of input/output buffers that the caller should commit, and 'model_id', which is an input parameter representing the model ID from OpenModel(). The function returns an EnnReturn result, where 0 indicates success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:51.108069+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Can you explain what buffers_info is and how it is used in the context of EnnGetBuffersInfo function?",
    "reference_contexts": [
      "```\nEnnReturn EnnGetBuffersInfo(\nNumberOfBuffersInfo * buffers_info,\nconst EnnModelId model_id\n)\n```  \nGet buffers information from loaded model.  \n**Parameters**:  \n* **buffers_info** [OUT] number of in / out buffer which caller should commit.\n* **model_id** [IN] model id from OpenModel()  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "Buffers_info is an output parameter in the EnnGetBuffersInfo function that indicates the number of input and output buffers that the caller should commit. This function retrieves information about the buffers from a loaded model, using the model_id parameter to specify which model's buffer information is being requested.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:51.108069+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Wht is the role of model_id in the EnnGetBufferInfoByIndex function?",
    "reference_contexts": [
      "```\nEnnReturn EnnGetBufferInfoByIndex(\nEnnBufferInfo * out_buf_info,\nconst EnnModelId model_id,\nconst enn_buf_dir_e direction,\nconst uint32_t index\n)\n```  \nGet one buffer information from loaded model.  \n**Parameters**:  \n* **out_buf_info** [OUT] output buffer information\n* **model_id** [IN] model ID from load_model\n* **direction** [IN] direction (IN, OUT)\n* **index** [IN] buffer's index number in model  \n**Return**: EnnReturn result, 0 is success  \n```cpp\ntypedef struct _ennBufferInfo {\nbool     is_able_to_update;  // this is not used\nuint32_t n;\nuint32_t width;\nuint32_t height;\nuint32_t channel;\nuint32_t size;\nconst char *label;\n} EnnBufferInfo;\n```  \na caller can identify a buffer as {DIR, Index} such as {IN, 0}"
    ],
    "reference": "The model_id is an input parameter in the EnnGetBufferInfoByIndex function, which represents the model ID from load_model.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:51.108069+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the significance of model_id in the context of EnnGetBufferInfoByIndex function?",
    "reference_contexts": [
      "```\nEnnReturn EnnGetBufferInfoByIndex(\nEnnBufferInfo * out_buf_info,\nconst EnnModelId model_id,\nconst enn_buf_dir_e direction,\nconst uint32_t index\n)\n```  \nGet one buffer information from loaded model.  \n**Parameters**:  \n* **out_buf_info** [OUT] output buffer information\n* **model_id** [IN] model ID from load_model\n* **direction** [IN] direction (IN, OUT)\n* **index** [IN] buffer's index number in model  \n**Return**: EnnReturn result, 0 is success  \n```cpp\ntypedef struct _ennBufferInfo {\nbool     is_able_to_update;  // this is not used\nuint32_t n;\nuint32_t width;\nuint32_t height;\nuint32_t channel;\nuint32_t size;\nconst char *label;\n} EnnBufferInfo;\n```  \na caller can identify a buffer as {DIR, Index} such as {IN, 0}"
    ],
    "reference": "In the context of the EnnGetBufferInfoByIndex function, model_id is a parameter that represents the model ID from load_model. It is used to identify which model's buffer information is being retrieved.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:51.108069+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What does the EnnReturn function do in the context of buffer information retrieval from a loaded model?",
    "reference_contexts": [
      "```\nEnnReturn EnnGetBufferInfoByLabel(\nEnnBufferInfo * out_buf_info,\nconst EnnModelId model_id,\nconst char * label\n)\n```  \nGet one buffer information from loaded model.  \n**Parameters**:  \n* **out_buf_info** [OUT] output buffer information\n* **model_id** [IN] model ID from load_model\n* **label** [IN] label. if .nnc includes redundent label, the framework returns information of the first founded tensor. C-style string type.  \n**Return**: EnnReturn result, 0 is success  \n```cpp\ntypedef struct _ennBufferInfo {\nbool     is_able_to_update;  // this is not used\nuint32_t n;\nuint32_t width;\nuint32_t height;\nuint32_t channel;\nuint32_t size;\nconst char *label;\n} EnnBufferInfo;\n```  \na caller can identify a buffer as {label} or {tensor name}"
    ],
    "reference": "The EnnReturn function, specifically EnnGetBufferInfoByLabel, retrieves one buffer information from a loaded model. It takes parameters such as out_buf_info for output buffer information, model_id for the model ID from load_model, and label for identifying the tensor. If the .nnc includes redundant labels, the framework returns information of the first found tensor. The function returns an EnnReturn result, where 0 indicates success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:51.108069+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the relationship between EnnReturn and the functions EnnGetBuffersInfo and EnnSetBufferByIndex in the context of buffer management?",
    "reference_contexts": [
      "<1-hop>\n\n```\nEnnReturn EnnSetBufferByIndex(\nconst EnnModelId model_id,\nconst enn_buf_dir_e direction,\nconst uint32_t index,\nEnnBufferPtr buf,\nconst int session_id =0\n)\n```  \nSet memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core.  \n**Parameters**:  \n* **model_id** [IN] model ID from load_model\n* **direction** [IN] Direction (IN/OUT)\n* **index** [IN] index number of buffer\n* **buf** [IN] memory object from EnnCreateBufferXXX()\n* **session_id** [IN] If a caller generates 2 or more buffer space, session_id can be an identifier  \n**Return**: EnnReturn result, 0 is success",
      "<2-hop>\n\n|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnGetBuffersInfo](Modules/group__api__model.md#function-enngetbuffersinfo)**([NumberOfBuffersInfo](Classes/struct___number_of_buffers_info.md) * buffers_info, const EnnModelId model_id)<br>Get buffers information from loaded model.  |\n| EnnReturn | **[EnnGetBufferInfoByIndex](Modules/group__api__model.md#function-enngetbufferinfobyindex)**([EnnBufferInfo](Classes/struct__enn_buffer_info.md) * out_buf_info, const EnnModelId model_id, const enn_buf_dir_e direction, const uint32_t index)<br>Get one buffer information from loaded model.  |\n| EnnReturn | **[EnnGetBufferInfoByLabel](Modules/group__api__model.md#function-enngetbufferinfobylabel)**([EnnBufferInfo](Classes/struct__enn_buffer_info.md) * out_buf_info, const EnnModelId model_id, const char * label)<br>Get one buffer information from loaded model.  |\n| EnnReturn | **[EnnSetBufferByIndex](Modules/group__api__model.md#function-ennsetbufferbyindex)**(const EnnModelId model_id, const enn_buf_dir_e direction, const uint32_t index, [EnnBufferPtr](Classes/struct__enn_buffer.md) buf, const int session_id =0)<br>Set memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core.  |\n| EnnReturn | **[EnnSetBufferByLabel](Modules/group__api__model.md#function-ennsetbufferbylabel)**(const EnnModelId model_id, const char * label, [EnnBufferPtr](Classes/struct__enn_buffer.md) buf, const int session_id =0)<br>Set memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core.  |"
    ],
    "reference": "EnnReturn is the return type for several functions related to buffer management in the framework. Specifically, EnnGetBuffersInfo retrieves information about the buffers from a loaded model, while EnnSetBufferByIndex allows a user to set a memory object to a commit-space, enabling the generation of buffer space for committing. Both functions utilize EnnReturn to indicate the success or failure of their operations, with a return value of 0 indicating success.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:51.108069+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Wht is the diffrence betwen EnnGetBufferInfoByLabel and EnnGetBufferInfoByIndex?",
    "reference_contexts": [
      "<1-hop>\n\n|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnGetBuffersInfo](Modules/group__api__model.md#function-enngetbuffersinfo)**([NumberOfBuffersInfo](Classes/struct___number_of_buffers_info.md) * buffers_info, const EnnModelId model_id)<br>Get buffers information from loaded model.  |\n| EnnReturn | **[EnnGetBufferInfoByIndex](Modules/group__api__model.md#function-enngetbufferinfobyindex)**([EnnBufferInfo](Classes/struct__enn_buffer_info.md) * out_buf_info, const EnnModelId model_id, const enn_buf_dir_e direction, const uint32_t index)<br>Get one buffer information from loaded model.  |\n| EnnReturn | **[EnnGetBufferInfoByLabel](Modules/group__api__model.md#function-enngetbufferinfobylabel)**([EnnBufferInfo](Classes/struct__enn_buffer_info.md) * out_buf_info, const EnnModelId model_id, const char * label)<br>Get one buffer information from loaded model.  |\n| EnnReturn | **[EnnSetBufferByIndex](Modules/group__api__model.md#function-ennsetbufferbyindex)**(const EnnModelId model_id, const enn_buf_dir_e direction, const uint32_t index, [EnnBufferPtr](Classes/struct__enn_buffer.md) buf, const int session_id =0)<br>Set memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core.  |\n| EnnReturn | **[EnnSetBufferByLabel](Modules/group__api__model.md#function-ennsetbufferbylabel)**(const EnnModelId model_id, const char * label, [EnnBufferPtr](Classes/struct__enn_buffer.md) buf, const int session_id =0)<br>Set memory object to commit-space. A user can generates buffer space to commit. (Basically the framework generates 16 spaces) \"Set Buffer\" means a caller can put its memory object to it's space. \"Commit\" means send memory-buffer set which can run opened model completely to service core.  |",
      "<2-hop>\n\n```\nEnnReturn EnnGetBufferInfoByIndex(\nEnnBufferInfo * out_buf_info,\nconst EnnModelId model_id,\nconst enn_buf_dir_e direction,\nconst uint32_t index\n)\n```  \nGet one buffer information from loaded model.  \n**Parameters**:  \n* **out_buf_info** [OUT] output buffer information\n* **model_id** [IN] model ID from load_model\n* **direction** [IN] direction (IN, OUT)\n* **index** [IN] buffer's index number in model  \n**Return**: EnnReturn result, 0 is success  \n```cpp\ntypedef struct _ennBufferInfo {\nbool     is_able_to_update;  // this is not used\nuint32_t n;\nuint32_t width;\nuint32_t height;\nuint32_t channel;\nuint32_t size;\nconst char *label;\n} EnnBufferInfo;\n```  \na caller can identify a buffer as {DIR, Index} such as {IN, 0}"
    ],
    "reference": "The EnnGetBufferInfoByLabel function retrieves buffer information from a loaded model using a specific label, while the EnnGetBufferInfoByIndex function retrieves buffer information using a specific index. Both functions return an EnnReturn result, indicating success or failure, but they differ in how they identify the buffer: one uses a label and the other uses an index.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__model.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:27:51.108069+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the significance of OpenModel in the context of software development?",
    "reference_contexts": [
      "---\ntitle: OpenModel / CloseModel related  \n---"
    ],
    "reference": "The context provided does not contain specific information about the significance of OpenModel in software development.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:28:07.874728+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Can you elaborate on the functions associated with EnnReturn in the context of model handling?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnOpenModel](Modules/group__api__openmodel.md#function-ennopenmodel)**(const char * model_file, EnnModelId * model_id)<br>OpenModel with model file.  |\n| EnnReturn | **[EnnOpenModelFromMemory](Modules/group__api__openmodel.md#function-ennopenmodelfrommemory)**(const char * va, const uint32_t size, EnnModelId * model_id)<br>OpenModel from memory buffer.  |"
    ],
    "reference": "EnnReturn is associated with two functions: EnnOpenModel, which opens a model using a specified model file, and EnnOpenModelFromMemory, which opens a model from a memory buffer. Both functions utilize the EnnModelId to identify the model.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:28:07.874728+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What functions are associated with EnnOpenModel in the context of software development?",
    "reference_contexts": [
      "|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnOpenModel](Modules/group__api__openmodel.md#function-ennopenmodel)**(const char * model_file, EnnModelId * model_id)<br>OpenModel with model file.  |\n| EnnReturn | **[EnnOpenModelFromMemory](Modules/group__api__openmodel.md#function-ennopenmodelfrommemory)**(const char * va, const uint32_t size, EnnModelId * model_id)<br>OpenModel from memory buffer.  |"
    ],
    "reference": "The functions associated with EnnOpenModel include EnnOpenModel, which opens a model with a specified model file, and EnnOpenModelFromMemory, which opens a model from a memory buffer.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:28:07.874728+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the significance of the model_id parameter in the EnnOpenModel function?",
    "reference_contexts": [
      "```\nEnnReturn EnnOpenModel(\nconst char * model_file,\nEnnModelId * model_id\n)\n```  \nOpenModel with model file.  \n**Parameters**:  \n* **model_file** [IN] model_file, output from graph-gen. A caller should access the file.\n* **model_id** [OUT] model_id, 64 bit unsigned int  \n**Return**: EnnReturn result, 0 is success"
    ],
    "reference": "The model_id parameter in the EnnOpenModel function is an output parameter that is a 64-bit unsigned integer. It is used to identify the model that is opened with the specified model_file.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:28:07.874728+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What are the parameters required for the EnnOpenModelFromMemory function?",
    "reference_contexts": [
      "```\nEnnReturn EnnOpenModelFromMemory(\nconst char * va,\nconst uint32_t size,\nEnnModelId * model_id\n)\n```  \nOpenModel from memory buffer.  \n**Parameters**:  \n* **va** [IN] address which a model loaded from\n* **size** [IN] size of the buffer\n* **model_id** [OUT] model_id, 64 bit unsigned int  \n**Return**: EnnReturn result, 0 is success  \n-------------------------------  \nUpdated on 2023-08-11 at 16:24:05 +0900"
    ],
    "reference": "The EnnOpenModelFromMemory function requires three parameters: 'va', which is the address from which a model is loaded; 'size', which is the size of the buffer; and 'model_id', which is a pointer to a 64-bit unsigned int that will hold the model ID.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:28:07.874728+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What does the function EnnReturn EnnOpenModelFromMemory do as of 2023-08-11?",
    "reference_contexts": [
      "```\nEnnReturn EnnOpenModelFromMemory(\nconst char * va,\nconst uint32_t size,\nEnnModelId * model_id\n)\n```  \nOpenModel from memory buffer.  \n**Parameters**:  \n* **va** [IN] address which a model loaded from\n* **size** [IN] size of the buffer\n* **model_id** [OUT] model_id, 64 bit unsigned int  \n**Return**: EnnReturn result, 0 is success  \n-------------------------------  \nUpdated on 2023-08-11 at 16:24:05 +0900"
    ],
    "reference": "The function EnnReturn EnnOpenModelFromMemory is used to open a model from a memory buffer. It takes three parameters: 'va', which is the address from which the model is loaded; 'size', which is the size of the buffer; and 'model_id', which is a 64-bit unsigned integer that will hold the model ID. The function returns an EnnReturn result, where 0 indicates success.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:28:07.874728+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the purpose of the EnnOpenModelFromMemory function in relation to the memory buffer?",
    "reference_contexts": [
      "<1-hop>\n\n```\nEnnReturn EnnOpenModelFromMemory(\nconst char * va,\nconst uint32_t size,\nEnnModelId * model_id\n)\n```  \nOpenModel from memory buffer.  \n**Parameters**:  \n* **va** [IN] address which a model loaded from\n* **size** [IN] size of the buffer\n* **model_id** [OUT] model_id, 64 bit unsigned int  \n**Return**: EnnReturn result, 0 is success  \n-------------------------------  \nUpdated on 2023-08-11 at 16:24:05 +0900",
      "<2-hop>\n\n|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnOpenModel](Modules/group__api__openmodel.md#function-ennopenmodel)**(const char * model_file, EnnModelId * model_id)<br>OpenModel with model file.  |\n| EnnReturn | **[EnnOpenModelFromMemory](Modules/group__api__openmodel.md#function-ennopenmodelfrommemory)**(const char * va, const uint32_t size, EnnModelId * model_id)<br>OpenModel from memory buffer.  |"
    ],
    "reference": "The EnnOpenModelFromMemory function is designed to open a model from a memory buffer. It takes parameters such as the address from which the model is loaded (va), the size of the buffer (size), and outputs the model_id, which is a 64-bit unsigned integer. The function returns an EnnReturn result, where 0 indicates success.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:28:07.874728+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the difference between EnnOpenModel and EnnOpenModelFromMemory in terms of their parameters and usage?",
    "reference_contexts": [
      "<1-hop>\n\n```\nEnnReturn EnnOpenModelFromMemory(\nconst char * va,\nconst uint32_t size,\nEnnModelId * model_id\n)\n```  \nOpenModel from memory buffer.  \n**Parameters**:  \n* **va** [IN] address which a model loaded from\n* **size** [IN] size of the buffer\n* **model_id** [OUT] model_id, 64 bit unsigned int  \n**Return**: EnnReturn result, 0 is success  \n-------------------------------  \nUpdated on 2023-08-11 at 16:24:05 +0900",
      "<2-hop>\n\n|                | Name           |\n| -------------- | -------------- |\n| EnnReturn | **[EnnOpenModel](Modules/group__api__openmodel.md#function-ennopenmodel)**(const char * model_file, EnnModelId * model_id)<br>OpenModel with model file.  |\n| EnnReturn | **[EnnOpenModelFromMemory](Modules/group__api__openmodel.md#function-ennopenmodelfrommemory)**(const char * va, const uint32_t size, EnnModelId * model_id)<br>OpenModel from memory buffer.  |"
    ],
    "reference": "EnnOpenModel is used to open a model from a model file, while EnnOpenModelFromMemory is used to open a model from a memory buffer. The parameters for EnnOpenModel include a model_file and a model_id, whereas EnnOpenModelFromMemory requires an address (va) from which the model is loaded, the size of the buffer, and also a model_id. Both functions return an EnnReturn result, where 0 indicates success.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\api-reference\\Modules\\group__api__openmodel.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:28:07.874728+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How does the AI Studio Farm service enhance the testing of applications on mobile devices for users?",
    "reference_contexts": [
      "---  \nAI Studio Farm is a service that provides a customized environment for easily and efficiently testing applications on real mobile devices.  \nTo use the AI Studio Farm remote service, you can instantly access it or schedule a connection for a desired date and time."
    ],
    "reference": "AI Studio Farm provides a customized environment that allows users to easily and efficiently test applications on real mobile devices. Users can instantly access the service or schedule a connection for a desired date and time, which enhances their productivity and management of reservations.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:28:43.641832+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How can I acsess the Rezervation List in AI Studio?",
    "reference_contexts": [
      "If you want to use the AI Studio Farm service on a specific date and time, you can make a reservation using the **Reservation** feature.  \n![](images/Reserve_Here_1.png)  \n1. Log in to use the AI Studio Farm service,\nthen click the Start Now button.  \n2. Set your desired date and time, then click\nthe Reserve button. Check the required tickets\nfor your reservation and your available ticket balance.  \n![](images/Reserve_Here_2.png)  \n3. After completing the reservation,\nyou can check your reservation details in Reservation Status.  \n**Tip\\!**  \n* Click the Cancel button to cancel your reservation.  \n[**Go to Reservation List**](https://prd.ai-studio-farm.com/global/remotelab/reservation-status?deviceTypeId=000d4a92-5b7c-4d17-83e3-b2015630a566)"
    ],
    "reference": "To access the Reservation List in AI Studio, you can check your reservation details in Reservation Status after completing your reservation.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:28:43.641832+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What happens during a service interruption regarding ticket credits?",
    "reference_contexts": [
      "To use **the Device Remote service** and make **reservations** on the **AI Studio Farm** page, you need **Tickets**.  \n**How to Earn Tickets:**  \n* **Log in**: Get **10 Tickets** (Max **10 Tickets per day**)  \n* **Write a forum post**: Get **1 Ticket** per post (Max **5 Tickets per day**)  \n* **Create a project post**: Get **10 Tickets** per post (Max **20 Tickets per day**)  \n* **Convert an NNC file**: Get **2 Tickets** per conversion (Max **10 Tickets per day**)  \n**Reservation Ticket Policies**  \n* **Reservation Credit**: Credits used for reservations will be refunded if you do not connect within 30 minutes. If you cancel the reservation before the scheduled time, all credits will be refunded.  \n* **Service Interruption**: 1 credit will be deducted every 30 minutes, and the remaining credits will be refunded.  \n* **Validity**: Free credits will expire after 90 days and will be forfeited without prior notice.  \n* **Credit Balance Check**: Your credit balance will be updated immediately each time you use a credit.  \n* **Management**: You can check the expiration date, acquisition, and usage history of your credits in the \"My Credits\" menu.  \n![](images/Ticket_Management.png)  \n**Tip\\!**  \n* Click the Ticket icon in the top right corner of the page to check your Ticket usage history."
    ],
    "reference": "During a service interruption, 1 credit will be deducted every 30 minutes, and the remaining credits will be refunded.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:28:43.641832+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Wht is the File Upload feature in the Remote Streaming Service?",
    "reference_contexts": [
      "The **File Upload** feature is a useful tool in the **Remote Streaming Service**. It allows you to transfer files from your local storage to the **Device** for various purposes. **Supports most file formats**, **APK files** are **automatically installed and executed** on the device  \n( If an **APK file is not installed**, it may not be supported by the device)  \n![](images/Uploading_File.App.png)  \n**Tip\\!**  \n* All files except APK are uploaded to the /sdcard/download/ directory.\nYou can upload files up to 200MB each, with a total upload limit of 2GB."
    ],
    "reference": "The File Upload feature in the Remote Streaming Service allows you to transfer files from your local storage to the Device for various purposes. It supports most file formats, and APK files are automatically installed and executed on the device. If an APK file is not installed, it may not be supported by the device. All files except APK are uploaded to the /sdcard/download/ directory, and you can upload files up to 200MB each, with a total upload limit of 2GB.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:28:43.641832+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Where are screenshots and recorded videos saved when using the Remote Streaming Service?",
    "reference_contexts": [
      "**[Screenshot]**  \nWhile using the **Remote Streaming Service**, you can capture the **Device screen** for documentation or review.\nScreenshots are saved in the **/sdcard/Download/** directory.  \n![](images/Screenshot.Recording_1.png)  \n**[Recording]**  \nYou can record the **Device screen** on the **Remote Streaming** page and save it as a video file.The recorded video file is saved in the **/sdcard/Download/** directory.  \n![](images/Screenshot_Recording_2.png)"
    ],
    "reference": "Screenshots and recorded video files are saved in the /sdcard/Download/ directory.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:28:43.641832+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How can I manage files on my Device using the Remote Streaming Service?",
    "reference_contexts": [
      "The **File Directory** feature in the **Remote Streaming Service** allows you to access and manage files on the **Device**.  \n![](images/File_Directory.Management.png)  \nDouble-click the **folder icon** to view the desired files. Double-click the **folder icon** to view the desired files.  \n* After uploading a file, access the **/sdcard/download/** directory to check the uploaded file.  \n* Double-click the file to **download it to your local PC**."
    ],
    "reference": "The File Directory feature in the Remote Streaming Service allows you to access and manage files on the Device. You can double-click the folder icon to view the desired files. After uploading a file, access the /sdcard/download/ directory to check the uploaded file, and double-click the file to download it to your local PC.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:28:43.641832+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What mv command do?",
    "reference_contexts": [
      "The **Command Shell** feature in the **Remote Streaming Service** allows you to control the device by entering commands.  \n![](images/Command_Shell_Access.png)  \n**Command Shell Rules:**  \n* Enter commands in the **Command Input** field to control the device.  \n* To view in a larger window, click the **Command Shell Window** button to continue entering commands in a modal window.  \nTry This Command to Control the Device.  \n1. View File  \nUse the **cat** command to display the contents of a file:\n```sh\ncat/sdcard/path/to/file.txt\n```  \n2. Move or Rename File  \nUse the **mv** command to move or rename a file.\n```sh\nmv /sdcard/path/to/file.txt /sdcard/path/to/new_name.txt\n```  \n3. Copy File  \nUse the **cp** command to copy a file:\n```sh\ncp /sdcard/path/to/file.txt /sdcard/path/to/backup_file.txt\n```"
    ],
    "reference": "The mv command is used to move or rename a file.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:28:43.641832+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What can INFO logs be used for in the Remote Streaming Service?",
    "reference_contexts": [
      "You can review **INFO logs**, which record usage history in real-time, in the **Remote Streaming Service**.  \nClick the **Log** tab to review **system logs and debugging information**.\nTo download system logs for the current session, click the **Download** button.  \n![](images/Logcat_Window.png)  \n**Tip\\!**  \n* Logs can only be downloaded while the remote service is connected.  \n* To view logs more comfortably, click the expand icon to enlarge the window."
    ],
    "reference": "INFO logs record usage history in real-time and can be reviewed in the Remote Streaming Service by clicking the Log tab.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:28:43.641832+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How can I make a reservation for the AI Studio Farm service and what is the role of the ADB Client Proxy in the Remote Lab?",
    "reference_contexts": [
      "<1-hop>\n\nIf you want to use the AI Studio Farm service on a specific date and time, you can make a reservation using the **Reservation** feature.  \n![](images/Reserve_Here_1.png)  \n1. Log in to use the AI Studio Farm service,\nthen click the Start Now button.  \n2. Set your desired date and time, then click\nthe Reserve button. Check the required tickets\nfor your reservation and your available ticket balance.  \n![](images/Reserve_Here_2.png)  \n3. After completing the reservation,\nyou can check your reservation details in Reservation Status.  \n**Tip\\!**  \n* Click the Cancel button to cancel your reservation.  \n[**Go to Reservation List**](https://prd.ai-studio-farm.com/global/remotelab/reservation-status?deviceTypeId=000d4a92-5b7c-4d17-83e3-b2015630a566)",
      "<2-hop>\n\nThe **ADB Client Proxy** is a tool that enables ADB connections in the **Remote Lab**, allowing you to develop Android applications remotely using ADB.  \n![](images/Adb_proxy_guide.png)  \n1\\. Check your platform and download the ADB Client Proxy.  \n2\\. Go to the Streaming Page , Click menu icon,and copy the token.  \n[**Go to Resource page**](https://prd.ai-studio-farm.com/global/development/enn-sdk)  \n![](images/Adb_proxy_guide_2.png)  \n3\\. Launch the ADB Client Proxy and enter the copied token.  \n4\\. Ensure that the ADB connection is successfully established.  \n**Tip\\!**  \n* Make sure the ADB server is turned off before proceeding.  \n* The ADB Client Proxy connects to ADB on a remote machine, so the ADB server port must be available.  \n* Do not open Android Studio before connecting to the remote ADB server, as it may keep the ADB server running."
    ],
    "reference": "To make a reservation for the AI Studio Farm service, you need to log in and click the Start Now button. Then, set your desired date and time and click the Reserve button, ensuring you check the required tickets for your reservation and your available ticket balance. After completing the reservation, you can view your reservation details in Reservation Status. On the other hand, the ADB Client Proxy is a tool that enables ADB connections in the Remote Lab, allowing you to develop Android applications remotely using ADB. You need to download the ADB Client Proxy, copy the token from the Streaming Page, and launch the proxy to establish a successful ADB connection.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:28:43.641832+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How can users manage files and access screenshots or recorded videos saved in the /sdcard/Download/ directory while utilizing the Remote Streaming Service?",
    "reference_contexts": [
      "<1-hop>\n\nThe **File Directory** feature in the **Remote Streaming Service** allows you to access and manage files on the **Device**.  \n![](images/File_Directory.Management.png)  \nDouble-click the **folder icon** to view the desired files. Double-click the **folder icon** to view the desired files.  \n* After uploading a file, access the **/sdcard/download/** directory to check the uploaded file.  \n* Double-click the file to **download it to your local PC**.",
      "<2-hop>\n\n**[Screenshot]**  \nWhile using the **Remote Streaming Service**, you can capture the **Device screen** for documentation or review.\nScreenshots are saved in the **/sdcard/Download/** directory.  \n![](images/Screenshot.Recording_1.png)  \n**[Recording]**  \nYou can record the **Device screen** on the **Remote Streaming** page and save it as a video file.The recorded video file is saved in the **/sdcard/Download/** directory.  \n![](images/Screenshot_Recording_2.png)"
    ],
    "reference": "Users can manage files by accessing the File Directory feature in the Remote Streaming Service, which allows them to view and manage files on the Device. After uploading a file, they can check the uploaded file in the /sdcard/download/ directory by double-clicking the folder icon. Additionally, screenshots captured while using the Remote Streaming Service are saved in the /sdcard/Download/ directory, and users can also record the Device screen, with the recorded video files being saved in the same directory.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\device-farm\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:28:43.641832+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What optimization scenarios are predetermined for large vision models (LVM)?",
    "reference_contexts": [
      "This section details the optimization flow according to model type. Currently, optimization scenarios are predetermined for specific model types, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). We plan to update the features to allow flexible scheduling of optimization functions."
    ],
    "reference": "The optimization scenarios for large vision models (LVM) are predetermined as part of the optimization flow according to model type, which also includes computer vision (CV) models and large language models (LLM).",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:29:07.301520+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What are large vision models in the context of optimization?",
    "reference_contexts": [
      "This section details the optimization flow according to model type. Currently, optimization scenarios are predetermined for specific model types, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). We plan to update the features to allow flexible scheduling of optimization functions."
    ],
    "reference": "Large vision models (LVM) are one of the specific model types for which optimization scenarios are predetermined, alongside computer vision (CV) models and large language models (LLM).",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:29:07.301520+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What are the steps involved in converting a CNNX model to SNC format?",
    "reference_contexts": [
      "![cv_flow.png](images/cv.png)  \n1. **ONNX-to-CNNX Conversion**  \nConverts an opset16 ONNX model to CNNX format.  \noutput_path : `{result_dir}/cnnx`  \n2. **Optimization**  \nPasses the CNNX model through Simplifier and applies an Optimization template.  \noutput_path : `{result_dir}/optimized`  \n3. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \n4. **Quantization**  \nApplies Fixed Precision Quantization to the optimized CNNX model.  \noutput_path : `{result_dir}/quantized`  \n5. **Performance Evaluation of Quantization**  \nCompares the inference results of the model before and after quantization.  \n6. **CNNX-to-SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`"
    ],
    "reference": "The steps involved in converting a CNNX model to SNC format include: 1. **ONNX-to-CNNX Conversion**: Converts an opset16 ONNX model to CNNX format. 2. **Optimization**: Passes the CNNX model through Simplifier and applies an Optimization template. 3. **Performance Evaluation of Optimization**: Compares the inference results of the model before and after optimization. 4. **Quantization**: Applies Fixed Precision Quantization to the optimized CNNX model. 5. **Performance Evaluation of Quantization**: Compares the inference results of the model before and after quantization. 6. **CNNX-to-SNC Conversion**: Converts the CNNX model to SNC format.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:29:07.301520+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What SNC do?",
    "reference_contexts": [
      "![cv_flow.png](images/cv.png)  \n1. **ONNX-to-CNNX Conversion**  \nConverts an opset16 ONNX model to CNNX format.  \noutput_path : `{result_dir}/cnnx`  \n2. **Optimization**  \nPasses the CNNX model through Simplifier and applies an Optimization template.  \noutput_path : `{result_dir}/optimized`  \n3. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \n4. **Quantization**  \nApplies Fixed Precision Quantization to the optimized CNNX model.  \noutput_path : `{result_dir}/quantized`  \n5. **Performance Evaluation of Quantization**  \nCompares the inference results of the model before and after quantization.  \n6. **CNNX-to-SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`"
    ],
    "reference": "SNC conversion takes the CNNX model and converts it to SNC format.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:29:07.301520+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is Softmax Bias Correction?",
    "reference_contexts": [
      "![lvm_flow.png](images/lvm.png)  \n1. **ONNX-to-CNNX Conversion**  \nConverts an opset16 ONNX model to CNNX format.  \noutput_path : `{result_dir}/cnnx`  \n2. **Optimization**  \nPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.  \noutput_path : `{result_dir}/optimized`  \n3. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \n4. **Quantization**  \nApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.  \noutput_path :  \n`{result_dir}/smoothquant`  \n`{result_dir}/mixed_precision_quant`  \n`{result_dir}/softmax_bias_correction`  \n5. **Performance Evaluation of Quantization**  \nCompares the inference results of the model before and after quantization.  \n6. **CNNX-to-SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`"
    ],
    "reference": "Softmax Bias Correction is a method applied during the quantization process, which is part of the optimization techniques used on the CNNX FP32 model.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:29:07.301520+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What are the steps involved in the ONNX-to-CNNX convrsion process and how does it relate to optimization and performance evaluation?",
    "reference_contexts": [
      "![lvm_flow.png](images/lvm.png)  \n1. **ONNX-to-CNNX Conversion**  \nConverts an opset16 ONNX model to CNNX format.  \noutput_path : `{result_dir}/cnnx`  \n2. **Optimization**  \nPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.  \noutput_path : `{result_dir}/optimized`  \n3. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \n4. **Quantization**  \nApplies the Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model.  \noutput_path :  \n`{result_dir}/smoothquant`  \n`{result_dir}/mixed_precision_quant`  \n`{result_dir}/softmax_bias_correction`  \n5. **Performance Evaluation of Quantization**  \nCompares the inference results of the model before and after quantization.  \n6. **CNNX-to-SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`"
    ],
    "reference": "The ONNX-to-CNNX conversion process involves several steps: First, it converts an opset16 ONNX model to CNNX format, with the output path specified as `{result_dir}/cnnx`. Next, the CNNX model undergoes optimization, which includes passing through a Simplifier and 4-Dimensional Conversion, followed by the application of an Optimization template, with the output path as `{result_dir}/optimized`. After optimization, the performance of the model is evaluated by comparing the inference results before and after the optimization. Additionally, quantization is applied, which involves Smooth Quantization, Mixed Precision Quantization, and Softmax Bias Correction methods sequentially to the CNNX FP32 model, with output paths for each method specified. Finally, the performance of the model is evaluated again after quantization, and the CNNX model is converted to SNC format, with the output path as `{result_dir}/snc`.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:29:07.301520+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What are the steps involved in optimizing a CNNX model?",
    "reference_contexts": [
      "![llm_flow.png](images/llm.png)  \n1. **Optimization**  \nPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.  \noutput_path : `{result_dir}/cnnx`  \n2. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \noutput_path : `{result_dir}/optimized`  \n3. **CNNX-to SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`"
    ],
    "reference": "The steps involved in optimizing a CNNX model include passing the CNNX model through Simplifier and 4-Dimensional Conversion, applying an Optimization template, evaluating the performance of the optimization by comparing the inference results before and after optimization, and converting the CNNX model to SNC format.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:29:07.301520+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What are the steps involved in optimizing the CNNX model?",
    "reference_contexts": [
      "![llm_flow.png](images/llm.png)  \n1. **Optimization**  \nPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.  \noutput_path : `{result_dir}/cnnx`  \n2. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \noutput_path : `{result_dir}/optimized`  \n3. **CNNX-to SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`"
    ],
    "reference": "The optimization of the CNNX model involves several steps: first, the model is passed through Simplifier and 4-Dimensional Conversion, followed by the application of an Optimization template. Next, the performance evaluation compares the inference results of the model before and after optimization. Finally, the CNNX model is converted to SNC format.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:29:07.301520+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How does the ONNX-to-CNNX conversion process contribute to the optimization of the model, and what steps are involved in evaluating its performance?",
    "reference_contexts": [
      "<1-hop>\n\n![llm_flow.png](images/llm.png)  \n1. **Optimization**  \nPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.  \noutput_path : `{result_dir}/cnnx`  \n2. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \noutput_path : `{result_dir}/optimized`  \n3. **CNNX-to SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`",
      "<2-hop>\n\n![cv_flow.png](images/cv.png)  \n1. **ONNX-to-CNNX Conversion**  \nConverts an opset16 ONNX model to CNNX format.  \noutput_path : `{result_dir}/cnnx`  \n2. **Optimization**  \nPasses the CNNX model through Simplifier and applies an Optimization template.  \noutput_path : `{result_dir}/optimized`  \n3. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \n4. **Quantization**  \nApplies Fixed Precision Quantization to the optimized CNNX model.  \noutput_path : `{result_dir}/quantized`  \n5. **Performance Evaluation of Quantization**  \nCompares the inference results of the model before and after quantization.  \n6. **CNNX-to-SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`"
    ],
    "reference": "The ONNX-to-CNNX conversion process contributes to the optimization of the model by transforming an opset16 ONNX model into CNNX format, which is then passed through a Simplifier and an Optimization template. This process is crucial as it sets the stage for further performance evaluation. The performance evaluation of optimization involves comparing the inference results of the model before and after optimization to assess improvements. Additionally, the CNNX model undergoes further steps such as quantization and conversion to SNC format, with performance evaluations conducted at each stage to ensure the model's efficiency and effectiveness.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:29:07.301520+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What are the steps involved in the conversion of a CNNX model to SNC format, and how does this process relate to the performance evaluation of optimization and quantization?",
    "reference_contexts": [
      "<1-hop>\n\n![llm_flow.png](images/llm.png)  \n1. **Optimization**  \nPasses the CNNX model through Simplifier and 4-Dimensional Conversion. Afterwards, an Optimization template is applied.  \noutput_path : `{result_dir}/cnnx`  \n2. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \noutput_path : `{result_dir}/optimized`  \n3. **CNNX-to SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`",
      "<2-hop>\n\n![cv_flow.png](images/cv.png)  \n1. **ONNX-to-CNNX Conversion**  \nConverts an opset16 ONNX model to CNNX format.  \noutput_path : `{result_dir}/cnnx`  \n2. **Optimization**  \nPasses the CNNX model through Simplifier and applies an Optimization template.  \noutput_path : `{result_dir}/optimized`  \n3. **Performance Evaluation of Optimization**  \nCompares the inference results of the model before and after optimization.  \n4. **Quantization**  \nApplies Fixed Precision Quantization to the optimized CNNX model.  \noutput_path : `{result_dir}/quantized`  \n5. **Performance Evaluation of Quantization**  \nCompares the inference results of the model before and after quantization.  \n6. **CNNX-to-SNC Conversion**  \nConverts the CNNX model to SNC format.  \noutput_path : `{result_dir}/snc`"
    ],
    "reference": "The conversion of a CNNX model to SNC format involves several steps. Initially, the CNNX model is passed through a Simplifier and an Optimization template is applied, which is followed by a performance evaluation comparing the inference results before and after optimization. After this, the CNNX model is converted to SNC format, with the output path specified as `{result_dir}/snc`. Additionally, the process includes an ONNX-to-CNNX conversion, where an opset16 ONNX model is converted to CNNX format, and further steps involve applying Fixed Precision Quantization to the optimized CNNX model, followed by another performance evaluation comparing the inference results before and after quantization. This comprehensive approach ensures that both optimization and quantization are effectively evaluated before the final conversion to SNC format.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\model_optimization_flow\\model_optimization_flow.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:29:07.301520+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How does the Reshape operation interact with the 4D conversion process in the context of model optimization for deep learning?",
    "reference_contexts": [
      "Models are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.  \n- shape_inference\nIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\n- 4dim conversion\nTo deploy a model with a .nnc extension on a device, all operators must have input and output shapes with four dimensions. To facilitate this, the 4D conversion process identifies and converts all channel axes of the operators to 4D while maintaining their original structure and channel axes. The channel axes can be determined by certain layers, such as Convolutional and Linear layers, which have fixed channel axes at positions 1 and -1, respectively. However, they can be modified by operators like MatMul and Reshape, which can expand or reduce them. To account for these changes, the conversion process tracks channel axes in both the forward and backward pass. As a result, the 4D conversion process may require additional time to complete.\n- support optimization method.\nOptimizer supports many optimization features so that the model works efficiently on the device.\n- Fold\n- GeGLU\n- GeLU\n- GroupNorm\n- LayerNorm\n- PReLU\n- RMSNorm\n- SiLU\n- Fuse\n- SiLU (to GroupNorm)\n- BatchNorm into Convolution\n- Cast\n- Deconvolution bias\n- Math\n- multiple reshape and transpose in a row (when possible)\n- multiple concat in a row\n- Insert\n- Depthwise Convolution for activation\n- Remove\n- unecesary slices\n- Replace\n- Average Pooling to Depthwise convolution\n- Eltwise concat convolution\n- expand with concat (by concatenating the same input multiple times)\n- Convolution kernel 1 to 3\n- Matrix multiplication to dynamic convolution\n- ReduceMean to Global Average Pool\n- ReduceSum to Convolution\n- Slice to Split\n- Global Average Pool to 2 Average Pool\n- Change attribute\n- axis of softmax  \nIn addition, Optimizer Template, one of the Optimizer's powerful features, supports custom optimization methods made by users."
    ],
    "reference": "The Reshape operation can expand or reduce channel axes, which are crucial for the 4D conversion process. This process identifies and converts all channel axes of the operators to 4D while maintaining their original structure and channel axes. The conversion process tracks these channel axes in both the forward and backward pass, and any modifications made by operations like Reshape can affect the overall conversion time, as the process may require additional time to complete.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\optimizer\\optimizer.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:29:32.242538+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What are CPUs used for in model training?",
    "reference_contexts": [
      "Models are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.  \n- shape_inference\nIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\n- 4dim conversion\nTo deploy a model with a .nnc extension on a device, all operators must have input and output shapes with four dimensions. To facilitate this, the 4D conversion process identifies and converts all channel axes of the operators to 4D while maintaining their original structure and channel axes. The channel axes can be determined by certain layers, such as Convolutional and Linear layers, which have fixed channel axes at positions 1 and -1, respectively. However, they can be modified by operators like MatMul and Reshape, which can expand or reduce them. To account for these changes, the conversion process tracks channel axes in both the forward and backward pass. As a result, the 4D conversion process may require additional time to complete.\n- support optimization method.\nOptimizer supports many optimization features so that the model works efficiently on the device.\n- Fold\n- GeGLU\n- GeLU\n- GroupNorm\n- LayerNorm\n- PReLU\n- RMSNorm\n- SiLU\n- Fuse\n- SiLU (to GroupNorm)\n- BatchNorm into Convolution\n- Cast\n- Deconvolution bias\n- Math\n- multiple reshape and transpose in a row (when possible)\n- multiple concat in a row\n- Insert\n- Depthwise Convolution for activation\n- Remove\n- unecesary slices\n- Replace\n- Average Pooling to Depthwise convolution\n- Eltwise concat convolution\n- expand with concat (by concatenating the same input multiple times)\n- Convolution kernel 1 to 3\n- Matrix multiplication to dynamic convolution\n- ReduceMean to Global Average Pool\n- ReduceSum to Convolution\n- Slice to Split\n- Global Average Pool to 2 Average Pool\n- Change attribute\n- axis of softmax  \nIn addition, Optimizer Template, one of the Optimizer's powerful features, supports custom optimization methods made by users."
    ],
    "reference": "Models are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\optimizer\\optimizer.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:29:32.242538+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What Optimizer do?",
    "reference_contexts": [
      "Optimizer is a tool that allows users to easily optimize their models. Users can add their own optimization methods using the \"Optimizer Template\" of Optimizer. The following process is required to apply the user-defined template to the user model.  \n- Create custom templates.\n- Prepare model to be optimized\n- Validate optimized model"
    ],
    "reference": "Optimizer is a tool that allows users to easily optimize their models. Users can add their own optimization methods using the \"Optimizer Template\" of Optimizer.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\optimizer\\optimizer.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:29:32.242538+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How can I use Optimizer Template to make my model better and what steps I need to do for that?",
    "reference_contexts": [
      "Optimizer is a tool that allows users to easily optimize their models. Users can add their own optimization methods using the \"Optimizer Template\" of Optimizer. The following process is required to apply the user-defined template to the user model.  \n- Create custom templates.\n- Prepare model to be optimized\n- Validate optimized model"
    ],
    "reference": "To use the Optimizer Template to optimize your model, you need to follow these steps: first, create custom templates; second, prepare the model to be optimized; and finally, validate the optimized model.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\optimizer\\optimizer.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:29:32.242538+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How does the CnnxModel interact with the optimization process in the provided context?",
    "reference_contexts": [
      "In this part, we introduce how to write custom templates step by step. As an example, we use a template that fuses basic arithmetic operations layers into a convolution layer, when possible. We provide `TemplateStepInternal`, `TemplateCaseInternal`, `OptimizerTemplateInternal` to prevent abnormal behavior of the code.  \n```python\nclass TemplateStepInternal(TemplateStep):\n@property\ndef name(self) -> AnyStr:\nreturn self._name\n\n@name.setter\ndef name(self, value: AnyStr):\nself._name = value\n\n@property\ndef origin(self) -> Dict:\nreturn self._origin\n\n@origin.setter\ndef origin(self, value: Dict):\nself._origin = value\n\ndef optimization(self, model: CnnxModel, node: CnnxNode) -> bool:\nreturn NotImplementedError\n\n\nclass TemplateCaseInternal(TemplateCase):\n@property\ndef name(self) -> AnyStr:\nreturn self._name\n\n@name.setter\ndef name(self, value: AnyStr):\nself._name = value\n\n@property\ndef origin(self) -> Dict:\nreturn self._origin\n\n@origin.setter\ndef origin(self, value: Dict):\nself._originn = value\n\n@property\ndef step_list(self) -> Dict:\nreturn self._step_list\n\n@step_list.setter\ndef step_list(self, value: Dict):\nself._step_list = value\n\ndef origin_condition(self, model: CnnxModel, node: CnnxNode) -> bool:\nraise NotImplementedError\n\ndef launch(self, model: CnnxModel, node: CnnxNode) -> bool:\nif self.origin_condition(model, node):\nfor st in self.step_list:\nif not st.optimization(model, node):\nreturn False\nreturn True\n\n\n\nclass OptimizerTemplateInternal(OptimizerTemplate):\ndef __repr__(self):\nreturn '{}(name={})'.format(\nself.__class__.__name__, self.name)\n\n@property\ndef name(self) -> AnyStr:\nreturn self._name\n\n@name.setter\ndef name(self, value: AnyStr):\nself._name = value\n\n@property\ndef case_list(self) -> List[TemplateCaseInternal]:\nreturn self._case_list\n\n@case_list.setter\ndef case_list(self, value: List[TemplateCaseInternal]):\nself._case_list = value\n\ndef trigger_op(self, node: CnnxNode) -> bool:\nraise NotImplementedError\n\n\ndef launch(self, model: CnnxModel, node: CnnxNode) -> bool:\nif self.trigger_op(node):\nfor case in self.case_list:\nif case.launch(model, node):\nreturn True\nreturn False\n```  \nInherit the above classes and declare each class corresponding to Step, Case, and Template. Import package classes of model_editor for updating the model. If you need more information about model_editor, please see the guide about model_editor.  \nWe **activate the trigger_op method** when the node is one of the four basic arithmetic operators and the input feature map has 4 dimension. In the **origin_condition method of the case class**, more detailled conditions are checked: the previous node of the arithmetic operator must be a convolution layer, the arithmetic operator must be the only node following the convolution node, and it must not be folded yet.\nIn the **optimization method of Step class**, update the value of the convolution node using the value of the arithmetic node. Once the value has been updated, you no longer need the original node. Create a port to clear the node from the graph. update the model using the port you created. First, remove the node from the model. Then, connect the next node of the removed node to the convolution node.  \n```python\n# That classes prevent the abnormal behavior of the code\nfrom optimizer.core.templates.base.template_internal import (\nTemplateStepInternal,\nTemplateCaseInternal,\nOptimizerTemplateInternal\n)\n# These packages have the ability to add, delete, and update nodes of the model.\n# Please see the guide about model_editor for more information.\nfrom model_editor import (\nCnnxNode,\nCnnxModel,\nCnnxInOut,\nCnnxAttribute,\n)\n\n# declare Step class\nclass StepFuseMath(TemplateStepInternal):\ndef __init__(self, device_info):\nself.name = 'Step Fusing Math'\nself.origin = 'Origin Operator, and Parameters'\nself.device_info = device_info\n\n# create bias\ndef make_bias_input(self, node: CnnxNode):\nbias_data = np.zeros(shape=node.outputs[0].inout.shape[1]).astype(np.float32)\nreturn CnnxInOut(\nname=f\"{node.name}_bias\",\ndtype=\"float\",\nshape=[node.outputs[0].inout.shape[1]],\ndata=bias_data\n)\n\ndef optimization(self, model: CnnxModel, node: CnnxNode) -> bool:\n# Set constant Value\n# Apply constant Value to weight, and bias\nparent_conv = node.prev_nodes[0]\nif not node.inputs[1].inout.shape or len(node.inputs[0].inout.shape) > 2 and len(node.inputs[1].inout.shape) > 2:\nshape_0 = node.inputs[0].inout.shape if node.inputs[0].inout.shape is not None else []\nshape_1 = node.inputs[1].inout.shape if node.inputs[1].inout.shape is not None else []\nif shape_0[-2:] == shape_1[-2:]:\nkernel_data = node.inputs[1].inout.data if node.inputs[1].inout.data is not None else node.inputs[0].inout.data\nbias_data = node.inputs[1].inout.data if node.inputs[1].inout.data is not None else node.inputs[0].inout.data\nelse:\nkernel_data = node.inputs[1].inout.data.reshape((-1, 1, 1, 1)) if node.inputs[1].inout.data is not None else node.inputs[0].inout.data.reshape((-1, 1, 1, 1))\nbias_data = node.inputs[1].inout.data.reshape((-1)) if node.inputs[1].inout.data is not None else node.inputs[0].inout.data.reshape((-1))\nif node.op_type == 'Add':\nif len(parent_conv.inputs) != 3:\nparent_conv.inputs.append(self.make_bias_input(parent_conv))\nnew_data = parent_conv.inputs.data[2].inout.data + bias_data\nparent_conv.inputs.data[2].inout.data = new_data\nparent_conv.inputs.data[2].inout.shape = new_data.shape\n\nelif node.op_type == 'Sub':\nif len(parent_conv.inputs) != 3:\nparent_conv.inputs.append(self.make_bias_input(parent_conv))\nnew_data = parent_conv.inputs.data[2].inout.data - bias_data\nparent_conv.inputs.data[2].inout.data = new_data\nparent_conv.inputs.data[2].inout.shape = new_data.shape\nelif node.op_type == 'Mul':\nparent_conv.inputs.data[1].inout.data = parent_conv.inputs.data[1].inout.data * kernel_data\nif len(parent_conv.inputs) == 3:\nparent_conv.inputs.data[2].inout.data = parent_conv.inputs.data[2].inout.data * bias_data\n\nelif node.op_type == 'Div':\nparent_conv.inputs.data[1].inout.data = parent_conv.inputs.data[1].inout.data / kernel_data\nif len(parent_conv.inputs) == 3:\nparent_conv.inputs.data[2].inout.data = parent_conv.inputs.data[2].inout.data / bias_data\n\nconnections = []\nfor next_node in node.next_nodes:\nfor output_tensor in node.outputs:\nport_from = node.prev_nodes[0].outputs[0]\nfor i in next_node.inputs:\nif i.name == output_tensor.name:\nconnections.append((port_from, i))\n\noptimized_nodes = [parent_conv]\nprevious_nodes = [node]\n\ntry:\nmodel.graph.nodes.remove(node)\nfor connection in connections:\nmodel.graph.nodes.connect(connection[0], connection[1])\nlogger.info(f'Fuse {node.op_type}({node.name}) into Deconv({parent_conv.name})')\nreturn True\nexcept Exception as e:\nlogger.error(e)\nreturn False\n\n\nclass CaseFuseMath(TemplateCaseInternal):\ndef __init__(self, device_info):\nself.name = 'Fuse Math to DWCONV'\nself.step_list = [StepFuseMath(device_info)]\n\n# Check the number of Child of [CONV, DWCONV]\ndef origin_condition(self, model: CnnxModel, node: CnnxNode) -> bool:\nif node.prev_nodes == []:\nreturn False\nif node.inputs[0].inout.data is None and node.inputs[1].inout.data is None:\nreturn False\nfm_shape = node.inputs[0].inout.shape if node.inputs[0].inout.data is None else node.inputs[1].inout.shape\nbias_data = node.inputs[0].inout.data if node.inputs[0].inout.data is not None else node.inputs[1].inout.data\n\n# if bias data just have one value\n# if length of bias data same with output_channel of prev conv node\nif isinstance(bias_data.tolist(), list) and fm_shape[1] != bias_data.reshape((-1)).shape[0]:\nreturn False\nif len(node.prev_nodes[0].next_nodes) == 1 and \\\nlen(node.prev_nodes) == 1 and \\\nnode.inputs[0].inout.shape != node.inputs[1].inout.shape:\nif node.prev_nodes[0].op_type in ['Conv', 'ConvTranspose'] and \\\nnode.module is None:\nreturn True\nreturn False\n\n\nclass TemplateFuseMath(OptimizerTemplateInternal):\ndef __init__(self):\nself.name = 'FuseMath'\n# declare Case\nself.case_list = [\nCaseFuseMath()\n]\n'''\naccording to Flow, The first thing you encounter when navigating the nodes in the graph is trigger_op.\nIn this case, it was written to operate only when the input shape of the four arithmetic operations operator\nand a input feature map of node has 4 dimension.\n'''\ndef trigger_op(self, node: CnnxNode):\noptypes = ['Add', 'Sub', 'Mul', 'Div']\nif node.op_type in optypes:\nif len(node.inputs[0].shape) == 4 or len(node.inputs[1].shape) == 4:\nreturn True\nreturn False\n```"
    ],
    "reference": "The CnnxModel is utilized in the optimization method of the Step class, where it is updated by modifying the values of convolution nodes based on the arithmetic operations performed by the node. The optimization process involves removing the original node from the model and connecting the next node to the convolution node, thereby enhancing the model's efficiency.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\optimizer\\optimizer.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:29:32.242538+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Can you explain how StepFuseMath is used in the optimization process of deep learning models?",
    "reference_contexts": [
      "In this part, we introduce how to write custom templates step by step. As an example, we use a template that fuses basic arithmetic operations layers into a convolution layer, when possible. We provide `TemplateStepInternal`, `TemplateCaseInternal`, `OptimizerTemplateInternal` to prevent abnormal behavior of the code.  \n```python\nclass TemplateStepInternal(TemplateStep):\n@property\ndef name(self) -> AnyStr:\nreturn self._name\n\n@name.setter\ndef name(self, value: AnyStr):\nself._name = value\n\n@property\ndef origin(self) -> Dict:\nreturn self._origin\n\n@origin.setter\ndef origin(self, value: Dict):\nself._origin = value\n\ndef optimization(self, model: CnnxModel, node: CnnxNode) -> bool:\nreturn NotImplementedError\n\n\nclass TemplateCaseInternal(TemplateCase):\n@property\ndef name(self) -> AnyStr:\nreturn self._name\n\n@name.setter\ndef name(self, value: AnyStr):\nself._name = value\n\n@property\ndef origin(self) -> Dict:\nreturn self._origin\n\n@origin.setter\ndef origin(self, value: Dict):\nself._originn = value\n\n@property\ndef step_list(self) -> Dict:\nreturn self._step_list\n\n@step_list.setter\ndef step_list(self, value: Dict):\nself._step_list = value\n\ndef origin_condition(self, model: CnnxModel, node: CnnxNode) -> bool:\nraise NotImplementedError\n\ndef launch(self, model: CnnxModel, node: CnnxNode) -> bool:\nif self.origin_condition(model, node):\nfor st in self.step_list:\nif not st.optimization(model, node):\nreturn False\nreturn True\n\n\n\nclass OptimizerTemplateInternal(OptimizerTemplate):\ndef __repr__(self):\nreturn '{}(name={})'.format(\nself.__class__.__name__, self.name)\n\n@property\ndef name(self) -> AnyStr:\nreturn self._name\n\n@name.setter\ndef name(self, value: AnyStr):\nself._name = value\n\n@property\ndef case_list(self) -> List[TemplateCaseInternal]:\nreturn self._case_list\n\n@case_list.setter\ndef case_list(self, value: List[TemplateCaseInternal]):\nself._case_list = value\n\ndef trigger_op(self, node: CnnxNode) -> bool:\nraise NotImplementedError\n\n\ndef launch(self, model: CnnxModel, node: CnnxNode) -> bool:\nif self.trigger_op(node):\nfor case in self.case_list:\nif case.launch(model, node):\nreturn True\nreturn False\n```  \nInherit the above classes and declare each class corresponding to Step, Case, and Template. Import package classes of model_editor for updating the model. If you need more information about model_editor, please see the guide about model_editor.  \nWe **activate the trigger_op method** when the node is one of the four basic arithmetic operators and the input feature map has 4 dimension. In the **origin_condition method of the case class**, more detailled conditions are checked: the previous node of the arithmetic operator must be a convolution layer, the arithmetic operator must be the only node following the convolution node, and it must not be folded yet.\nIn the **optimization method of Step class**, update the value of the convolution node using the value of the arithmetic node. Once the value has been updated, you no longer need the original node. Create a port to clear the node from the graph. update the model using the port you created. First, remove the node from the model. Then, connect the next node of the removed node to the convolution node.  \n```python\n# That classes prevent the abnormal behavior of the code\nfrom optimizer.core.templates.base.template_internal import (\nTemplateStepInternal,\nTemplateCaseInternal,\nOptimizerTemplateInternal\n)\n# These packages have the ability to add, delete, and update nodes of the model.\n# Please see the guide about model_editor for more information.\nfrom model_editor import (\nCnnxNode,\nCnnxModel,\nCnnxInOut,\nCnnxAttribute,\n)\n\n# declare Step class\nclass StepFuseMath(TemplateStepInternal):\ndef __init__(self, device_info):\nself.name = 'Step Fusing Math'\nself.origin = 'Origin Operator, and Parameters'\nself.device_info = device_info\n\n# create bias\ndef make_bias_input(self, node: CnnxNode):\nbias_data = np.zeros(shape=node.outputs[0].inout.shape[1]).astype(np.float32)\nreturn CnnxInOut(\nname=f\"{node.name}_bias\",\ndtype=\"float\",\nshape=[node.outputs[0].inout.shape[1]],\ndata=bias_data\n)\n\ndef optimization(self, model: CnnxModel, node: CnnxNode) -> bool:\n# Set constant Value\n# Apply constant Value to weight, and bias\nparent_conv = node.prev_nodes[0]\nif not node.inputs[1].inout.shape or len(node.inputs[0].inout.shape) > 2 and len(node.inputs[1].inout.shape) > 2:\nshape_0 = node.inputs[0].inout.shape if node.inputs[0].inout.shape is not None else []\nshape_1 = node.inputs[1].inout.shape if node.inputs[1].inout.shape is not None else []\nif shape_0[-2:] == shape_1[-2:]:\nkernel_data = node.inputs[1].inout.data if node.inputs[1].inout.data is not None else node.inputs[0].inout.data\nbias_data = node.inputs[1].inout.data if node.inputs[1].inout.data is not None else node.inputs[0].inout.data\nelse:\nkernel_data = node.inputs[1].inout.data.reshape((-1, 1, 1, 1)) if node.inputs[1].inout.data is not None else node.inputs[0].inout.data.reshape((-1, 1, 1, 1))\nbias_data = node.inputs[1].inout.data.reshape((-1)) if node.inputs[1].inout.data is not None else node.inputs[0].inout.data.reshape((-1))\nif node.op_type == 'Add':\nif len(parent_conv.inputs) != 3:\nparent_conv.inputs.append(self.make_bias_input(parent_conv))\nnew_data = parent_conv.inputs.data[2].inout.data + bias_data\nparent_conv.inputs.data[2].inout.data = new_data\nparent_conv.inputs.data[2].inout.shape = new_data.shape\n\nelif node.op_type == 'Sub':\nif len(parent_conv.inputs) != 3:\nparent_conv.inputs.append(self.make_bias_input(parent_conv))\nnew_data = parent_conv.inputs.data[2].inout.data - bias_data\nparent_conv.inputs.data[2].inout.data = new_data\nparent_conv.inputs.data[2].inout.shape = new_data.shape\nelif node.op_type == 'Mul':\nparent_conv.inputs.data[1].inout.data = parent_conv.inputs.data[1].inout.data * kernel_data\nif len(parent_conv.inputs) == 3:\nparent_conv.inputs.data[2].inout.data = parent_conv.inputs.data[2].inout.data * bias_data\n\nelif node.op_type == 'Div':\nparent_conv.inputs.data[1].inout.data = parent_conv.inputs.data[1].inout.data / kernel_data\nif len(parent_conv.inputs) == 3:\nparent_conv.inputs.data[2].inout.data = parent_conv.inputs.data[2].inout.data / bias_data\n\nconnections = []\nfor next_node in node.next_nodes:\nfor output_tensor in node.outputs:\nport_from = node.prev_nodes[0].outputs[0]\nfor i in next_node.inputs:\nif i.name == output_tensor.name:\nconnections.append((port_from, i))\n\noptimized_nodes = [parent_conv]\nprevious_nodes = [node]\n\ntry:\nmodel.graph.nodes.remove(node)\nfor connection in connections:\nmodel.graph.nodes.connect(connection[0], connection[1])\nlogger.info(f'Fuse {node.op_type}({node.name}) into Deconv({parent_conv.name})')\nreturn True\nexcept Exception as e:\nlogger.error(e)\nreturn False\n\n\nclass CaseFuseMath(TemplateCaseInternal):\ndef __init__(self, device_info):\nself.name = 'Fuse Math to DWCONV'\nself.step_list = [StepFuseMath(device_info)]\n\n# Check the number of Child of [CONV, DWCONV]\ndef origin_condition(self, model: CnnxModel, node: CnnxNode) -> bool:\nif node.prev_nodes == []:\nreturn False\nif node.inputs[0].inout.data is None and node.inputs[1].inout.data is None:\nreturn False\nfm_shape = node.inputs[0].inout.shape if node.inputs[0].inout.data is None else node.inputs[1].inout.shape\nbias_data = node.inputs[0].inout.data if node.inputs[0].inout.data is not None else node.inputs[1].inout.data\n\n# if bias data just have one value\n# if length of bias data same with output_channel of prev conv node\nif isinstance(bias_data.tolist(), list) and fm_shape[1] != bias_data.reshape((-1)).shape[0]:\nreturn False\nif len(node.prev_nodes[0].next_nodes) == 1 and \\\nlen(node.prev_nodes) == 1 and \\\nnode.inputs[0].inout.shape != node.inputs[1].inout.shape:\nif node.prev_nodes[0].op_type in ['Conv', 'ConvTranspose'] and \\\nnode.module is None:\nreturn True\nreturn False\n\n\nclass TemplateFuseMath(OptimizerTemplateInternal):\ndef __init__(self):\nself.name = 'FuseMath'\n# declare Case\nself.case_list = [\nCaseFuseMath()\n]\n'''\naccording to Flow, The first thing you encounter when navigating the nodes in the graph is trigger_op.\nIn this case, it was written to operate only when the input shape of the four arithmetic operations operator\nand a input feature map of node has 4 dimension.\n'''\ndef trigger_op(self, node: CnnxNode):\noptypes = ['Add', 'Sub', 'Mul', 'Div']\nif node.op_type in optypes:\nif len(node.inputs[0].shape) == 4 or len(node.inputs[1].shape) == 4:\nreturn True\nreturn False\n```"
    ],
    "reference": "StepFuseMath is a class that inherits from TemplateStepInternal and is designed to optimize deep learning models by fusing basic arithmetic operations into convolution layers when possible. It sets constant values for weights and biases based on the operation type, such as Add, Sub, Mul, or Div. The optimization method updates the convolution node's value using the arithmetic node's value, and it also manages the connections in the model graph to ensure that the original node is removed and the next node is connected properly.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\optimizer\\optimizer.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:29:32.242538+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What are the features of the Optimizer that enhance model performance on Exynos chips, and how can a custom template be specified for optimization?",
    "reference_contexts": [
      "<1-hop>\n\nModels are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.  \n- shape_inference\nIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\n- 4dim conversion\nTo deploy a model with a .nnc extension on a device, all operators must have input and output shapes with four dimensions. To facilitate this, the 4D conversion process identifies and converts all channel axes of the operators to 4D while maintaining their original structure and channel axes. The channel axes can be determined by certain layers, such as Convolutional and Linear layers, which have fixed channel axes at positions 1 and -1, respectively. However, they can be modified by operators like MatMul and Reshape, which can expand or reduce them. To account for these changes, the conversion process tracks channel axes in both the forward and backward pass. As a result, the 4D conversion process may require additional time to complete.\n- support optimization method.\nOptimizer supports many optimization features so that the model works efficiently on the device.\n- Fold\n- GeGLU\n- GeLU\n- GroupNorm\n- LayerNorm\n- PReLU\n- RMSNorm\n- SiLU\n- Fuse\n- SiLU (to GroupNorm)\n- BatchNorm into Convolution\n- Cast\n- Deconvolution bias\n- Math\n- multiple reshape and transpose in a row (when possible)\n- multiple concat in a row\n- Insert\n- Depthwise Convolution for activation\n- Remove\n- unecesary slices\n- Replace\n- Average Pooling to Depthwise convolution\n- Eltwise concat convolution\n- expand with concat (by concatenating the same input multiple times)\n- Convolution kernel 1 to 3\n- Matrix multiplication to dynamic convolution\n- ReduceMean to Global Average Pool\n- ReduceSum to Convolution\n- Slice to Split\n- Global Average Pool to 2 Average Pool\n- Change attribute\n- axis of softmax  \nIn addition, Optimizer Template, one of the Optimizer's powerful features, supports custom optimization methods made by users.",
      "<2-hop>\n\nWrite the path of the custom template and the class name of the template. Following the above example, it can be written as follows:  \n```yaml\ninput_model_path: {INPUT_MODEL_PATH}\noutput_folder_path: {OUTPUT_MODEL_PATH}\ninput_model_format: onnx\nmodel_type: CV\n\nquantizer:\n~~~~~\n\nsimulator:\n~~~~~~\n\noptimizer:\ncustom_template_path: {}\noverwrite_input_shapes:\ndata: [1,4,384,384]\nskip_4_dim_conversion: false\ndevice_name: default\ncustom_template_path:\n- TemplateFuseMath: /path/to/your/custome/template.py:\n```"
    ],
    "reference": "The Optimizer enhances model performance on Exynos chips through several powerful features. These include shape inference, which allows users to define or modify input shapes; 4D conversion, which ensures that all operators have input and output shapes with four dimensions; and various optimization methods that improve efficiency on the device. Additionally, users can specify a custom template for optimization by writing the path of the custom template and the class name in a YAML format, including parameters such as input model path, output folder path, and device name.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\optimizer\\optimizer.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:29:32.242538+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What steps are involved in creating a custom optimizer template and how does it relate to the optimization process of a model?",
    "reference_contexts": [
      "<1-hop>\n\nIn this part, we introduce how to write custom templates step by step. As an example, we use a template that fuses basic arithmetic operations layers into a convolution layer, when possible. We provide `TemplateStepInternal`, `TemplateCaseInternal`, `OptimizerTemplateInternal` to prevent abnormal behavior of the code.  \n```python\nclass TemplateStepInternal(TemplateStep):\n@property\ndef name(self) -> AnyStr:\nreturn self._name\n\n@name.setter\ndef name(self, value: AnyStr):\nself._name = value\n\n@property\ndef origin(self) -> Dict:\nreturn self._origin\n\n@origin.setter\ndef origin(self, value: Dict):\nself._origin = value\n\ndef optimization(self, model: CnnxModel, node: CnnxNode) -> bool:\nreturn NotImplementedError\n\n\nclass TemplateCaseInternal(TemplateCase):\n@property\ndef name(self) -> AnyStr:\nreturn self._name\n\n@name.setter\ndef name(self, value: AnyStr):\nself._name = value\n\n@property\ndef origin(self) -> Dict:\nreturn self._origin\n\n@origin.setter\ndef origin(self, value: Dict):\nself._originn = value\n\n@property\ndef step_list(self) -> Dict:\nreturn self._step_list\n\n@step_list.setter\ndef step_list(self, value: Dict):\nself._step_list = value\n\ndef origin_condition(self, model: CnnxModel, node: CnnxNode) -> bool:\nraise NotImplementedError\n\ndef launch(self, model: CnnxModel, node: CnnxNode) -> bool:\nif self.origin_condition(model, node):\nfor st in self.step_list:\nif not st.optimization(model, node):\nreturn False\nreturn True\n\n\n\nclass OptimizerTemplateInternal(OptimizerTemplate):\ndef __repr__(self):\nreturn '{}(name={})'.format(\nself.__class__.__name__, self.name)\n\n@property\ndef name(self) -> AnyStr:\nreturn self._name\n\n@name.setter\ndef name(self, value: AnyStr):\nself._name = value\n\n@property\ndef case_list(self) -> List[TemplateCaseInternal]:\nreturn self._case_list\n\n@case_list.setter\ndef case_list(self, value: List[TemplateCaseInternal]):\nself._case_list = value\n\ndef trigger_op(self, node: CnnxNode) -> bool:\nraise NotImplementedError\n\n\ndef launch(self, model: CnnxModel, node: CnnxNode) -> bool:\nif self.trigger_op(node):\nfor case in self.case_list:\nif case.launch(model, node):\nreturn True\nreturn False\n```  \nInherit the above classes and declare each class corresponding to Step, Case, and Template. Import package classes of model_editor for updating the model. If you need more information about model_editor, please see the guide about model_editor.  \nWe **activate the trigger_op method** when the node is one of the four basic arithmetic operators and the input feature map has 4 dimension. In the **origin_condition method of the case class**, more detailled conditions are checked: the previous node of the arithmetic operator must be a convolution layer, the arithmetic operator must be the only node following the convolution node, and it must not be folded yet.\nIn the **optimization method of Step class**, update the value of the convolution node using the value of the arithmetic node. Once the value has been updated, you no longer need the original node. Create a port to clear the node from the graph. update the model using the port you created. First, remove the node from the model. Then, connect the next node of the removed node to the convolution node.  \n```python\n# That classes prevent the abnormal behavior of the code\nfrom optimizer.core.templates.base.template_internal import (\nTemplateStepInternal,\nTemplateCaseInternal,\nOptimizerTemplateInternal\n)\n# These packages have the ability to add, delete, and update nodes of the model.\n# Please see the guide about model_editor for more information.\nfrom model_editor import (\nCnnxNode,\nCnnxModel,\nCnnxInOut,\nCnnxAttribute,\n)\n\n# declare Step class\nclass StepFuseMath(TemplateStepInternal):\ndef __init__(self, device_info):\nself.name = 'Step Fusing Math'\nself.origin = 'Origin Operator, and Parameters'\nself.device_info = device_info\n\n# create bias\ndef make_bias_input(self, node: CnnxNode):\nbias_data = np.zeros(shape=node.outputs[0].inout.shape[1]).astype(np.float32)\nreturn CnnxInOut(\nname=f\"{node.name}_bias\",\ndtype=\"float\",\nshape=[node.outputs[0].inout.shape[1]],\ndata=bias_data\n)\n\ndef optimization(self, model: CnnxModel, node: CnnxNode) -> bool:\n# Set constant Value\n# Apply constant Value to weight, and bias\nparent_conv = node.prev_nodes[0]\nif not node.inputs[1].inout.shape or len(node.inputs[0].inout.shape) > 2 and len(node.inputs[1].inout.shape) > 2:\nshape_0 = node.inputs[0].inout.shape if node.inputs[0].inout.shape is not None else []\nshape_1 = node.inputs[1].inout.shape if node.inputs[1].inout.shape is not None else []\nif shape_0[-2:] == shape_1[-2:]:\nkernel_data = node.inputs[1].inout.data if node.inputs[1].inout.data is not None else node.inputs[0].inout.data\nbias_data = node.inputs[1].inout.data if node.inputs[1].inout.data is not None else node.inputs[0].inout.data\nelse:\nkernel_data = node.inputs[1].inout.data.reshape((-1, 1, 1, 1)) if node.inputs[1].inout.data is not None else node.inputs[0].inout.data.reshape((-1, 1, 1, 1))\nbias_data = node.inputs[1].inout.data.reshape((-1)) if node.inputs[1].inout.data is not None else node.inputs[0].inout.data.reshape((-1))\nif node.op_type == 'Add':\nif len(parent_conv.inputs) != 3:\nparent_conv.inputs.append(self.make_bias_input(parent_conv))\nnew_data = parent_conv.inputs.data[2].inout.data + bias_data\nparent_conv.inputs.data[2].inout.data = new_data\nparent_conv.inputs.data[2].inout.shape = new_data.shape\n\nelif node.op_type == 'Sub':\nif len(parent_conv.inputs) != 3:\nparent_conv.inputs.append(self.make_bias_input(parent_conv))\nnew_data = parent_conv.inputs.data[2].inout.data - bias_data\nparent_conv.inputs.data[2].inout.data = new_data\nparent_conv.inputs.data[2].inout.shape = new_data.shape\nelif node.op_type == 'Mul':\nparent_conv.inputs.data[1].inout.data = parent_conv.inputs.data[1].inout.data * kernel_data\nif len(parent_conv.inputs) == 3:\nparent_conv.inputs.data[2].inout.data = parent_conv.inputs.data[2].inout.data * bias_data\n\nelif node.op_type == 'Div':\nparent_conv.inputs.data[1].inout.data = parent_conv.inputs.data[1].inout.data / kernel_data\nif len(parent_conv.inputs) == 3:\nparent_conv.inputs.data[2].inout.data = parent_conv.inputs.data[2].inout.data / bias_data\n\nconnections = []\nfor next_node in node.next_nodes:\nfor output_tensor in node.outputs:\nport_from = node.prev_nodes[0].outputs[0]\nfor i in next_node.inputs:\nif i.name == output_tensor.name:\nconnections.append((port_from, i))\n\noptimized_nodes = [parent_conv]\nprevious_nodes = [node]\n\ntry:\nmodel.graph.nodes.remove(node)\nfor connection in connections:\nmodel.graph.nodes.connect(connection[0], connection[1])\nlogger.info(f'Fuse {node.op_type}({node.name}) into Deconv({parent_conv.name})')\nreturn True\nexcept Exception as e:\nlogger.error(e)\nreturn False\n\n\nclass CaseFuseMath(TemplateCaseInternal):\ndef __init__(self, device_info):\nself.name = 'Fuse Math to DWCONV'\nself.step_list = [StepFuseMath(device_info)]\n\n# Check the number of Child of [CONV, DWCONV]\ndef origin_condition(self, model: CnnxModel, node: CnnxNode) -> bool:\nif node.prev_nodes == []:\nreturn False\nif node.inputs[0].inout.data is None and node.inputs[1].inout.data is None:\nreturn False\nfm_shape = node.inputs[0].inout.shape if node.inputs[0].inout.data is None else node.inputs[1].inout.shape\nbias_data = node.inputs[0].inout.data if node.inputs[0].inout.data is not None else node.inputs[1].inout.data\n\n# if bias data just have one value\n# if length of bias data same with output_channel of prev conv node\nif isinstance(bias_data.tolist(), list) and fm_shape[1] != bias_data.reshape((-1)).shape[0]:\nreturn False\nif len(node.prev_nodes[0].next_nodes) == 1 and \\\nlen(node.prev_nodes) == 1 and \\\nnode.inputs[0].inout.shape != node.inputs[1].inout.shape:\nif node.prev_nodes[0].op_type in ['Conv', 'ConvTranspose'] and \\\nnode.module is None:\nreturn True\nreturn False\n\n\nclass TemplateFuseMath(OptimizerTemplateInternal):\ndef __init__(self):\nself.name = 'FuseMath'\n# declare Case\nself.case_list = [\nCaseFuseMath()\n]\n'''\naccording to Flow, The first thing you encounter when navigating the nodes in the graph is trigger_op.\nIn this case, it was written to operate only when the input shape of the four arithmetic operations operator\nand a input feature map of node has 4 dimension.\n'''\ndef trigger_op(self, node: CnnxNode):\noptypes = ['Add', 'Sub', 'Mul', 'Div']\nif node.op_type in optypes:\nif len(node.inputs[0].shape) == 4 or len(node.inputs[1].shape) == 4:\nreturn True\nreturn False\n```",
      "<2-hop>\n\nOptimizer is a tool that allows users to easily optimize their models. Users can add their own optimization methods using the \"Optimizer Template\" of Optimizer. The following process is required to apply the user-defined template to the user model.  \n- Create custom templates.\n- Prepare model to be optimized\n- Validate optimized model"
    ],
    "reference": "To create a custom optimizer template, users must follow several steps: first, they need to write custom templates, which can include classes like `TemplateStepInternal`, `TemplateCaseInternal`, and `OptimizerTemplateInternal` that prevent abnormal behavior of the code. Next, they prepare the model to be optimized by ensuring it meets the necessary conditions for optimization. Finally, they validate the optimized model to ensure that the custom optimizations have been applied correctly. The optimizer template serves as a framework that allows users to implement their own optimization methods, facilitating the optimization process of their models.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\eht\\optimizer\\optimizer.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:29:32.242538+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What are the key steps involved in using the Exynos Neural Network Software Development Kit for Android app development?",
    "reference_contexts": [
      "This guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (Exynos AI Studio).\nIt outlines the steps for creating an application that utilizes the Exynos AI Studio to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the Exynos AI Studio in their Android applications."
    ],
    "reference": "The key steps involved in using the Exynos Neural Network Software Development Kit include Android project setup, function implementation, and NNC model conversion, all aimed at developing an image classification Android application that utilizes the Exynos AI Studio to run neural network models on Samsung Exynos hardware.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:29:55.840031+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How to make Android app for image classification using Exynos?",
    "reference_contexts": [
      "This guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (Exynos AI Studio).\nIt outlines the steps for creating an application that utilizes the Exynos AI Studio to run neural network models on Samsung Exynos hardware.\nThis guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.\nThe guide aims at equipping developers with the necessary knowledge to use the Exynos AI Studio in their Android applications."
    ],
    "reference": "This guide provides a comprehensive overview for developing an image classification Android application using the Exynos Neural Network Software Development Kit (Exynos AI Studio). It outlines the steps for creating an application that utilizes the Exynos AI Studio to run neural network models on Samsung Exynos hardware. This guide covers the important steps that also include Android project setup, function implementation, and NNC model conversion.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:29:55.840031+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How does the Exynos AI Studio enhance the performance of models on Samsung Exynos hardware?",
    "reference_contexts": [
      "The [Exynos AI Studio](https://soc-developer.semiconductor.samsung.com/enn-sdk) provides a tool for converting [TFLite](https://www.tensorflow.org/lite) neural network models into models in Neural Network Container (NNC) format.\nThis conversion allows the NN models to operate efficiently on the [Samsung Exynos](https://semiconductor.samsung.com/processor/) hardware, to ensure optimal performance.\nThe Exynos AI Studio provides a framework that facilitates the execution of NNC models on the Exynos platform."
    ],
    "reference": "The Exynos AI Studio enhances performance by providing a tool for converting TFLite neural network models into models in Neural Network Container (NNC) format, allowing the NN models to operate efficiently on Samsung Exynos hardware for optimal performance.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:29:55.840031+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How does the Exynos AI Stuido contribute to the functionality of the sample application that classifies objects from a camera feed or image?",
    "reference_contexts": [
      "The sample application takes input from a camera feed or an image, and classifies the object in the input.\nIt also leverages the Exynos AI Studio to efficiently execute the NN model on the Exynos platform."
    ],
    "reference": "The Exynos AI Studio contributes to the functionality of the sample application by efficiently executing the NN model on the Exynos platform, which allows the application to classify the object in the input from a camera feed or an image.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:29:55.840031+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the Inception v4 model used for in Android app development?",
    "reference_contexts": [
      "This example uses the quantized `Inception v4` TFLite model from [TensorFlow Hub](https://tfhub.dev/tensorflow/lite-model/inception_v4_quant/1/default/1) for image classification."
    ],
    "reference": "The Inception v4 model is used for image classification in Android app development, specifically utilizing the quantized TFLite model from TensorFlow Hub.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:29:55.840031+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the significance of using the quantized Inception v4 TFLite model for image classification in Android application development?",
    "reference_contexts": [
      "This example uses the quantized `Inception v4` TFLite model from [TensorFlow Hub](https://tfhub.dev/tensorflow/lite-model/inception_v4_quant/1/default/1) for image classification."
    ],
    "reference": "The quantized Inception v4 TFLite model is significant for image classification as it provides a lightweight and efficient solution for deploying machine learning models on Android devices, allowing developers to integrate advanced image recognition capabilities into their applications.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:29:55.840031+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What are the steps involved in setting up an Android project for a sample application that includes both camera and image modes, and how do these modes function within the application?",
    "reference_contexts": [
      "<1-hop>\n\nImplementing sample application involves the following five steps:  \n1. [Starting Android project](getting-started-with-android-samples/starting-android-project): Covers the basics of starting an Android project, from installing Android studio to connecting the ERD board to Android studio.\n1. [Setting necessary UI](getting-started-with-android-samples/setting-necessary-ui): Provides essential information on Android UI view components.\n1. [Function implementation](getting-started-with-android-samples/function-implementation): Explains the functions available in the sample application.\n1. [Exynos AI Studio service](getting-started-with-android-samples/enn-sdk-service): Provides the step-by-step process for converting the TFLite model to NNC model.\n1. [ENN framework](getting-started-with-android-samples/enn-framework): Explains the implementation of the ENN framework in the sample application.  \nThe general workflow of writing and executing an Android application using the Exynos AI Studio is discribed in the following flowchart.\n```mermaid\ngraph TB\nA[Start]\nsubgraph B[Starting Android Project]\nB1[Install Android Studio] --> B2[Create a New Android Project]\nB2 --> B3[Connect the ERD Board to Android Studio]\nend\nsubgraph C[Setting Necessary UI]\nC1[Set up Android UI View Components]\nend\nsubgraph D[Function Implementation]\nD1[Implement UI Functions]\nD1 --> D2[Implement Listener]\nD2 --> D3[Implement Data Processing Functions]\nend\nsubgraph E[ENNTools]\nE1[Use ENNTools to Convert the TFLite Model to the NNC Model]\nend\nsubgraph F[ENN Framework]\nF1[Implement the ENN Framework in the Sample Application]\nend\nG[End]\nA --> B\nB --> C\nC --> D\nD --> E\nE --> F\nF --> G\n```",
      "<2-hop>\n\nThe sample application provides two modes such as camera and image.\nYou can select the desired mode by tapping the corresponding icon.\nIn the image mode, the application processes the selected image from your library.\nIn the camera mode, the application automatically processes the camera feed.  \nThe processed output is displayed at the bottom of the screen, as illustrated in the following example. The application displays the classified items and their scores.  \n<img src=\"./guide/img/sample1.png\" alt=\"drawing\" width=\"200\"/>\n<img src=\"./guide/img/sample2.png\" alt=\"drawing\" width=\"200\"/>\n<img src=\"./guide/img/sample3.png\" alt=\"drawing\" width=\"200\"/>"
    ],
    "reference": "The steps involved in setting up an Android project for a sample application include: 1. Starting the Android project, which covers the basics from installing Android Studio to connecting the ERD board. 2. Setting the necessary UI, which provides essential information on Android UI view components. 3. Function implementation, which explains the functions available in the sample application. 4. Utilizing the Exynos AI Studio service to convert the TFLite model to an NNC model. 5. Implementing the ENN framework in the sample application. The sample application itself provides two modes: camera and image. In image mode, the application processes a selected image from the user's library, while in camera mode, it automatically processes the camera feed. The processed output, which includes classified items and their scores, is displayed at the bottom of the screen.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:29:55.840031+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What are the steps to connect the ERD bord to Android studio when starting an Android project?",
    "reference_contexts": [
      "<1-hop>\n\n|Terms|Expanded Form|\n|-|-|\n|Exynos AI Studio|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NNC|Neural Network Container|\n|NN|Neural Network|\n|TFLite|TensorFlow Lite|\n|JNI|Java Native Interface|\n|UByte|Unsigned Byte|",
      "<2-hop>\n\nImplementing sample application involves the following five steps:  \n1. [Starting Android project](getting-started-with-android-samples/starting-android-project): Covers the basics of starting an Android project, from installing Android studio to connecting the ERD board to Android studio.\n1. [Setting necessary UI](getting-started-with-android-samples/setting-necessary-ui): Provides essential information on Android UI view components.\n1. [Function implementation](getting-started-with-android-samples/function-implementation): Explains the functions available in the sample application.\n1. [Exynos AI Studio service](getting-started-with-android-samples/enn-sdk-service): Provides the step-by-step process for converting the TFLite model to NNC model.\n1. [ENN framework](getting-started-with-android-samples/enn-framework): Explains the implementation of the ENN framework in the sample application.  \nThe general workflow of writing and executing an Android application using the Exynos AI Studio is discribed in the following flowchart.\n```mermaid\ngraph TB\nA[Start]\nsubgraph B[Starting Android Project]\nB1[Install Android Studio] --> B2[Create a New Android Project]\nB2 --> B3[Connect the ERD Board to Android Studio]\nend\nsubgraph C[Setting Necessary UI]\nC1[Set up Android UI View Components]\nend\nsubgraph D[Function Implementation]\nD1[Implement UI Functions]\nD1 --> D2[Implement Listener]\nD2 --> D3[Implement Data Processing Functions]\nend\nsubgraph E[ENNTools]\nE1[Use ENNTools to Convert the TFLite Model to the NNC Model]\nend\nsubgraph F[ENN Framework]\nF1[Implement the ENN Framework in the Sample Application]\nend\nG[End]\nA --> B\nB --> C\nC --> D\nD --> E\nE --> F\nF --> G\n```"
    ],
    "reference": "The steps to connect the ERD board to Android Studio when starting an Android project include installing Android Studio, creating a new Android project, and then connecting the ERD board to Android Studio.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:29:55.840031+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What steps are involved in adding C++ to a project in Android?",
    "reference_contexts": [
      "1. Right click **Project** panel with **Android** option being selected.  \n<img src=\"./img/cpp1.png\" alt=\"drawing\" width=\"500\"/>  \n1. Select the **Add C++ to Module** option and click OK.  \n<img src=\"./img/cpp2.png\" alt=\"drawing\" width=\"500\"/>"
    ],
    "reference": "To add C++ to a project in Android, right-click the Project panel with the Android option selected, then select the Add C++ to Module option and click OK.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:30:30.594538+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What modifications should be made to the CMakeLists.txt file when integrating the ENN framework library into an Android application?",
    "reference_contexts": [
      "Download the ENN framework library from [resources](https://soc-developer.semiconductor.samsung.com/support/resource).\nTo load the necessary libraries, perform the folowing steps:  \n1. Modiy Android Manifest to:\n```xml\n<manifest>\n<application>\n...\n<!-- Declare the native library in the Android Manifest -->\n<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />\n...\n</application>\n</manifest>\n```  \n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/app/src/main/jniLibs/arm64-v8a`.\n1. copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/app/src/main/cpp/include`.\n1. Modify `CMakeLists.txt` that is created [here](#adding-c-to-module).  \n```cmake\n# Include the directory where the header files are located\ninclude_directories(include)\n\n# Declare the imported shared library\nadd_library(\nenn_service_so\nSHARED\nIMPORTED\n)\n\n# Set the location of the imported library\nset_target_properties(\nenn_service_so\nPROPERTIES IMPORTED_LOCATION\n${CMAKE_SOURCE_DIR}/../jniLibs/${ANDROID_ABI}/libenn_public_api_ndk_v1.so\n)\n\n# Link the imported library to the target library\ntarget_link_libraries(\n...\nenn_service_so\n...\n)\n```"
    ],
    "reference": "To modify the CMakeLists.txt file for integrating the ENN framework library, you need to include the directory where the header files are located by using the command 'include_directories(include)'. Then, declare the imported shared library with 'add_library(enn_service_so SHARED IMPORTED)'. After that, set the location of the imported library using 'set_target_properties(enn_service_so PROPERTIES IMPORTED_LOCATION ${CMAKE_SOURCE_DIR}/../jniLibs/${ANDROID_ABI}/libenn_public_api_ndk_v1.so)'. Finally, link the imported library to the target library with 'target_link_libraries(... enn_service_so ...)'.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:30:30.594538+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What I need do for set arm64-v8a ABI in my project using ENN framework?",
    "reference_contexts": [
      "The ENN framework currently supports only the `arm64-v8a` ABI. To set ABI, modify the `build.gradle` file and specify the ABI as `arm64-v8a`.\n```\ndefaultConfig {\n...\nndk {\nabiFilters \"arm64-v8a\"\n}\n...\n}\n```"
    ],
    "reference": "To set the arm64-v8a ABI in your project using the ENN framework, you need to modify the build.gradle file and specify the ABI as arm64-v8a in the defaultConfig section, like this: defaultConfig { ... ndk { abiFilters \"arm64-v8a\" } ... }.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:30:30.594538+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How does the Java Native Interface (JNI) facilitate interaction between Java and C code in software development?",
    "reference_contexts": [
      "The Java Native Interface (JNI) is a framework that allows Java code to interact with the code written in other languages such as C or C++. In the sample application, JNI accesses features or libraries such as the ENN framework that are written in C or C++.  \n1. Create `enn_jni.cc` in `cpp` directory.\n1. Modify `CMakeLists.txt` created [here](#adding-c-to-module).  \n```cmake\nadd_library(\nenn_jni\nSHARED\nenn_jni.cc\n)\n\ntarget_link_libraries(\nenn_jni\n)\n```"
    ],
    "reference": "The Java Native Interface (JNI) is a framework that allows Java code to interact with code written in other languages such as C or C++. In the sample application, JNI accesses features or libraries, like the ENN framework, that are written in C or C++. This interaction is achieved by creating a file named `enn_jni.cc` in the `cpp` directory and modifying the `CMakeLists.txt` to include the necessary configurations for building the shared library.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:30:30.594538+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is EnnModelId in the JNI wrapper example?",
    "reference_contexts": [
      "Following function is an example of an implemented JNI wrapper.\nFor more information, refer to the  [Android Developer Documentation](https://developer.android.com/ndk/samples/sample_hellojni).  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/cpp/enn_jni.cc#L57)):\n```cpp\nextern \"C\"\nJNIEXPORT jlong JNICALL\nJava_com_samsung_imageclassification_executor_ModelExecutor_ennOpenModel(\nJNIEnv *env,\njobject thiz,\njstring j_filename\n) {\nEnnModelId model_id;\nconst char *filename = env->GetStringUTFChars(j_filename, 0);\n\nif (enn::api::EnnOpenModel(filename, &model_id)) {\n__android_log_print(ANDROID_LOG_ERROR, LOG_TAG, \"EnnOpenModel of [%s] Failed\", filename);\n}\n\nreturn static_cast<jlong>(model_id);\n}\n```"
    ],
    "reference": "EnnModelId is a variable used in the JNI wrapper example to hold the model identifier when opening a model with the function EnnOpenModel.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:30:30.594538+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What are the steps to use JNI function in Kotlin?",
    "reference_contexts": [
      "Following is an example for using JNI function in Kotlin.\n1. Load the JNI library\n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/executor/ModelExecutor.kt#L42)):\n```kotlin\ninit {\nSystem.loadLibrary(\"enn_jni\")\n...\n}\n```\n2. Declare the external function\n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/executor/ModelExecutor.kt#L28)):\n```kotlin\nprivate external fun ennOpenModel(filename: String): Long\n...\n```\n3. Use the JNI function ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/executor/ModelExecutor.kt#L54)):\n```kotlin\n//  to open a model\nprivate fun setupENN() {\n...\nmodelId = ennOpenModel(fileAbsoluteDirectory)\n...\n}\n```"
    ],
    "reference": "To use JNI function in Kotlin, follow these steps: 1. Load the JNI library using `System.loadLibrary(\"enn_jni\")`. 2. Declare the external function with `private external fun ennOpenModel(filename: String): Long`. 3. Use the JNI function in a method, for example, to open a model with `modelId = ennOpenModel(fileAbsoluteDirectory)`.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:30:30.594538+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Can you explain what ennDeinitialize does in the context of the ENN framework?",
    "reference_contexts": [
      "Following table describes the implemented native functions for the sample application.  \n| Method | Description | Inputs | Outputs |\n| --- | --- | --- | --- |\n| [`ennInitialize`](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/cpp/enn_jni.cc#L35) | Initialize the ENN framework | - | - |\n| [`ennDeinitialize`](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/cpp/enn_jni.cc#L46) | Deinitialize the ENN framework | - | - |\n| [`ennOpenModel`](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/cpp/enn_jni.cc#L57) | Open a Model from the model file. | `filename`: String - Directory of the model `nnc` file | `modelID`: Long - Model ID. |\n| [`ennCloseModel`](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/cpp/enn_jni.cc#L74) | Close the model and free up all resources from `ennOpenModel`. | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| [`ennAllocateAllBuffers`](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/cpp/enn_jni.cc#L86)  | Allocate all buffers that a caller should allocate. | `modelID`: Long - Model ID from `ennOpenModel` | `bufferSetInfo`: BufferSetInfo - Custom class that includes pointer and size of the EnnBuffer array. |\n| [`ennReleaseBuffers`](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/cpp/enn_jni.cc#L103) | Release buffer array from `ennAllocateAllBuffers` | `bufferSet`: Long - pointer of buffer set array.<br>`bufferSize`: Int - total number of buffers. | - |\n| [`ennExecute`](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/cpp/enn_jni.cc#L118) | Request to service core to execute model with committed buffers | `modelId`: Long - Model ID from `ennOpenModel`. | - |\n| [`ennMemcpyHostToDevice`](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/cpp/enn_jni.cc#L130) | Copy ByteArray to buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer.<br>`data`: ByteArray - ByteArray to copy. | - |\n| [`ennMemcpyDeviceToHost`](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/cpp/enn_jni.cc#L150) | Copy ByteArray from buffer array | `bufferSet`: Long - Pointer of buffer set array.<br>`layerNumber`: Int - Index of buffer. | `data`: ByteArray - Copied ByteArray. |"
    ],
    "reference": "ennDeinitialize is a method that is used to deinitialize the ENN framework. It does not take any inputs or produce any outputs.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:30:30.594538+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Can you explain the process of using the ennOpenModel function in the execution of neural network models?",
    "reference_contexts": [
      "To execute the NN models with implemented native functions, perform the following steps.  \n1. Preparing Execution of Model\n1. Initialize the framework using the `ennInitialize` function.\n2. Load the ML model into the framework using the `ennOpenModel` function.\n3. Allocate and commit the necessary buffers using the `ennAllocateAllBuffers` function.\n2. Executing a Model\n1. Set input data as a parameter.\n2. Call the `ennExecute` function.\n3. Get the execution result as a return.\n3. Deinitializing the Framework\n1. Release the allocated memory of the buffers with the `ennReleaseBuffers` function.\n2. Close the model and release other resources with the `ennCloseModel` function.\n3. Deinitialize the framework with the `ennDeinitialize` function."
    ],
    "reference": "The ennOpenModel function is used to load the machine learning model into the framework as part of the preparation for executing neural network models. This step follows the initialization of the framework with the ennInitialize function and precedes the allocation and commitment of necessary buffers using the ennAllocateAllBuffers function.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:30:30.594538+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How does the Java Native Interface (JNI) facilitate the use of the ENN framework in software applications, particularly in the context of model inference?",
    "reference_contexts": [
      "<1-hop>\n\nFollowing flowchart describes the lifecycle of inferring NN models using ENN framework.  \n```mermaid\ngraph TB\nsubgraph A[Initialize ENN Framework]\nA1[Initialize]\nA1 --> A2[Open Model]\nA2 --> A3[Allocate/Commit Buffers]\nend\nsubgraph B[Inference]\nB1[Copy Input Layer]\nB1 --> B2[Execute Model]\nB2 --> B3[Copy Output Layer]\nend\nsubgraph C[Deinitialize]\nC1[Release Buffers]\nC1 --> C2[Close Model]\nC2 --> C3[Deinitialize]\nend\nA --> B\nB --> C\n```  \nTo infer multiple data, repeat `Inference` process.",
      "<2-hop>\n\nThe Java Native Interface (JNI) is a framework that allows Java code to interact with the code written in other languages such as C or C++. In the sample application, JNI accesses features or libraries such as the ENN framework that are written in C or C++.  \n1. Create `enn_jni.cc` in `cpp` directory.\n1. Modify `CMakeLists.txt` created [here](#adding-c-to-module).  \n```cmake\nadd_library(\nenn_jni\nSHARED\nenn_jni.cc\n)\n\ntarget_link_libraries(\nenn_jni\n)\n```"
    ],
    "reference": "The Java Native Interface (JNI) facilitates the use of the ENN framework in software applications by allowing Java code to interact with code written in other languages, such as C or C++. In the context of model inference, the ENN framework is utilized to manage the lifecycle of inferring neural network models. The process begins with initializing the ENN framework, where the model is opened and buffers are allocated. During inference, input layers are copied, the model is executed, and output layers are copied. After inference, the buffers are released, and the model is closed. By using JNI, developers can access the ENN framework's features and libraries, enabling efficient model handling and buffer management.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:30:30.594538+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the purpose of the `ennOpenModel` function in the context of executing NN models?",
    "reference_contexts": [
      "<1-hop>\n\nTo execute the NN models with implemented native functions, perform the following steps.  \n1. Preparing Execution of Model\n1. Initialize the framework using the `ennInitialize` function.\n2. Load the ML model into the framework using the `ennOpenModel` function.\n3. Allocate and commit the necessary buffers using the `ennAllocateAllBuffers` function.\n2. Executing a Model\n1. Set input data as a parameter.\n2. Call the `ennExecute` function.\n3. Get the execution result as a return.\n3. Deinitializing the Framework\n1. Release the allocated memory of the buffers with the `ennReleaseBuffers` function.\n2. Close the model and release other resources with the `ennCloseModel` function.\n3. Deinitialize the framework with the `ennDeinitialize` function.",
      "<2-hop>\n\nFollowing function is an example of an implemented JNI wrapper.\nFor more information, refer to the  [Android Developer Documentation](https://developer.android.com/ndk/samples/sample_hellojni).  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/cpp/enn_jni.cc#L57)):\n```cpp\nextern \"C\"\nJNIEXPORT jlong JNICALL\nJava_com_samsung_imageclassification_executor_ModelExecutor_ennOpenModel(\nJNIEnv *env,\njobject thiz,\njstring j_filename\n) {\nEnnModelId model_id;\nconst char *filename = env->GetStringUTFChars(j_filename, 0);\n\nif (enn::api::EnnOpenModel(filename, &model_id)) {\n__android_log_print(ANDROID_LOG_ERROR, LOG_TAG, \"EnnOpenModel of [%s] Failed\", filename);\n}\n\nreturn static_cast<jlong>(model_id);\n}\n```"
    ],
    "reference": "The `ennOpenModel` function is used to load the ML model into the framework, which is a crucial step in preparing for the execution of neural network models with implemented native functions.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\enn-framework.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:30:30.594538+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Can you elaborate on the role of ModelExecutor.kt in the sample application for image processing?",
    "reference_contexts": [
      "Following classes are required to create the sample application:  \n- `app/java/package/executor`\n- `ModelExecutor.kt`: Includes methods for processing images and return classification results.\n- `app/java/package/fragments`\n- `CameraFragment.kt`: Handles user interactions and updates the UI in Camera mode.\n- `ImageFragment.kt`: Handles user interactions and updates the UI in Image mode.\n- `app/java/package/enn_type`\n- `BufferSetInfo`: Data class that holds information about the buffer set and number of input/output layers"
    ],
    "reference": "ModelExecutor.kt includes methods for processing images and returning classification results, which are essential for the functionality of the sample application.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\function-implementation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:00.126484+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What does onResults do in the ExecutorListener interface?",
    "reference_contexts": [
      "The `ExecutorListener` interface in `ModelExecutor.kt` is a custom listener that provides two methods such as `onError` and `onResults`.  \n- `onError`: Triggered when an error occurs during image classification. It logs the error or displays an error message.\n- `onResults`: Triggered when image classification is successfully completed. It updates the UI with the classification results and the time taken for classification.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/executor/ModelExecutor.kt#L258)):\n```kotlin\ninterface ExecutorListener {\nfun onError(error: String)\nfun onResults(\nresult: Map<String, Float>, inferenceTime: Long\n)\n}\n```  \nImplement the interface in the `fragment` class.\nThe `onError` method logs the error and the `onResults` method updates the UI.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L185)):  \n```kotlin\nclass SampleFragment : Fragment(), ImageClassifierHelper.ClassifierListener {\n...\noverride fun onError(error: String) {\nLog.e(TAG, \"ModelExecutor error: $error\")\n}\n\noverride fun onResults(\nresult: Map<String, Float>, inferenceTime: Long\n) {\nactivity?.runOnUiThread {\nbinding.processData.inferenceTime.text = \"$inferenceTime ms\"\nupdateUI(result)\n}\n}\n}\n```"
    ],
    "reference": "onResults is triggered when image classification is successfully completed. It updates the UI with the classification results and the time taken for classification.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\function-implementation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:00.126484+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How do you start the image classification process using the ModelExecutor object in the fragment class?",
    "reference_contexts": [
      "Create the `ModelExecutor` object iin the `fragment` class with the current context and the fragment as `executorListener`. The `process` method of `modelExecutor` is called to start the image classification.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L53)):\n```kotlin\nmodelExecutor = ModelExecutor(\ncontext = requireContext(), executorListener = this\n)\n...\nmodelExecutor.process(bitmapBuffer)\n```  \nIn the `ModelExecutor.kt` class, the `process` method processes the image and calls the `onResults` method of `executorListener` to pass the results back to the fragment.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/executor/ModelExecutor.kt#L76)):\n```kotlin\nfun process(image: Bitmap) {\n...\n\nexecutorListener?.onResults(\nresult, inferenceTime\n)\n}\n```"
    ],
    "reference": "To start the image classification process using the ModelExecutor object in the fragment class, you create the ModelExecutor object with the current context and set the fragment as the executorListener. You then call the process method of modelExecutor to initiate the image classification. The process method processes the image and calls the onResults method of executorListener to pass the results back to the fragment.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\function-implementation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:00.126484+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What does the setImageAnalyzer function do?",
    "reference_contexts": [
      "The `setImageAnalyzer` function sets an `ImageAnalysis` object to process the camera feed.\nIt creates a bitmap buffer and processes each image frame.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L100)):\n```kotlin\nprivate fun setImageAnalyzer() {\nimageAnalyzer =\nImageAnalysis.Builder()\n.setTargetRotation(binding.viewFinder.display.rotation)\n.setBackpressureStrategy(ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST)\n.setOutputImageFormat(ImageAnalysis.OUTPUT_IMAGE_FORMAT_RGBA_8888)\n.build().also {\nit.setAnalyzer(cameraExecutor) { image ->\nif (!::bitmapBuffer.isInitialized) {\nbitmapBuffer = Bitmap.createBitmap(\nimage.width, image.height, Bitmap.Config.ARGB_8888\n)\n}\nprocess(image)\n}\n}\n}\n```  \nThe `setCamera` function introduced [here](getting-started-with-android-samples/setting-necessary-ui#camera-preview) is updated to include the `setImageAnalyzer` method.\nThis inclusion allows the camera feed to be analyzed and processed.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L65)):\n```kotlin\ncameraProviderFuture.addListener(\n{\n...\nsetPreview()\nsetImageAnalyzer()\n\ntry {\ncameraProvider.unbindAll()\ncamera = cameraProvider.bindToLifecycle(\nthis, cameraSelector, preview, imageAnalyzer\n)\npreview?.setSurfaceProvider(binding.viewFinder.surfaceProvider)\n}\n...\n},\n...\n)\n```"
    ],
    "reference": "The `setImageAnalyzer` function sets an `ImageAnalysis` object to process the camera feed. It creates a bitmap buffer and processes each image frame.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\function-implementation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:00.126484+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How does the `uri` function in the `ActivityResult` object facilitate image selection in mobile applications?",
    "reference_contexts": [
      "The `ActivityResult` object `getContent` retrieves an image from the device media.\nThe selected image is displayed in an ImageView and converted to a bitmap.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/ImageFragment.kt#L28)):\n```kotlin\nprivate val getContent =\nregisterForActivityResult(ActivityResultContracts.GetContent()) { uri: Uri? ->\nuri?.let {\nbinding.imageView.setImageURI(it)\nbinding.buttonProcess.isEnabled = true\n\nbitmapBuffer = ImageDecoder.decodeBitmap(\nImageDecoder.createSource(\nrequireContext().contentResolver,\nit\n)\n) { decoder, _, _ ->\ndecoder.setTargetColorSpace(ColorSpace.get(ColorSpace.Named.SRGB))\ndecoder.allocator = ImageDecoder.ALLOCATOR_SOFTWARE\ndecoder.setTargetSampleSize(1)\n}\n}\n}\n```  \nClick **Load** to launch the `ActivityResult`.\nIt allows the user to select an image from their device.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/ImageFragment.kt#L69)):\n```kotlin\nbinding.buttonLoad.setOnClickListener {\ngetContent.launch(\"image/*\")\n}\n```"
    ],
    "reference": "The `uri` function in the `ActivityResult` object allows the user to select an image from their device. When an image is selected, the `getContent` retrieves the image as a `Uri`, which is then used to set the image in an ImageView and convert it to a bitmap for further processing.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\function-implementation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:00.126484+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Why need copy NNC model file for ENN Framework?",
    "reference_contexts": [
      "Use `copyNNCFromAssetsToInternalStorage` function to copy the NNC model file from the asset directory of app to its internal storage.\nIt is necessary to copy the NNC model file because the model file needs to be accessed from the internal storage when used by the ENN Framework.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/executor/ModelExecutor.kt#L231)):\n```kotlin\nprivate fun copyNNCFromAssetsToInternalStorage(filename: String) {\ntry {\nval inputStream = context.assets.open(filename)\nval outputFile = File(context.filesDir, filename)\nval outputStream = FileOutputStream(outputFile)\nval buffer = ByteArray(2048)\nvar bytesRead: Int\n\nwhile (inputStream.read(buffer).also { bytesRead = it } != -1) {\noutputStream.write(buffer, 0, bytesRead)\n}\ninputStream.close()\noutputStream.close()\n} catch (e: IOException) {\ne.printStackTrace()\n}\n}\n```"
    ],
    "reference": "It is necessary to copy the NNC model file because the model file needs to be accessed from the internal storage when used by the ENN Framework.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\function-implementation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:00.126484+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What information does the BufferSetInfo data class provide regarding the buffer set in a neural network model?",
    "reference_contexts": [
      "The `BufferSetInfo` data class holds the information about the buffer set used in the neural network model.\nIt includes the memory location of the buffer set (`buffer_set`), the number of input buffers (`n_in_buf`), and the number of output buffers (`n_out_buf`).\nUse the data class to return this information from the JNI library.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/enn_type/BufferSetInfo.kt)):\n```kotlin\nclass BufferSetInfo {\nvar buffer_set: Long = 0\nvar n_in_buf: Int = 0\nvar n_out_buf: Int = 0\n}\n```"
    ],
    "reference": "The BufferSetInfo data class holds information about the buffer set used in the neural network model, including the memory location of the buffer set (buffer_set), the number of input buffers (n_in_buf), and the number of output buffers (n_out_buf).",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\function-implementation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:00.126484+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How does the `ActivityResult` object facilitate image retrieval for processing in a mobile application, and what role does the `BufferSetInfo` data class play in the neural network model used for image processing?",
    "reference_contexts": [
      "<1-hop>\n\nThe `ActivityResult` object `getContent` retrieves an image from the device media.\nThe selected image is displayed in an ImageView and converted to a bitmap.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/ImageFragment.kt#L28)):\n```kotlin\nprivate val getContent =\nregisterForActivityResult(ActivityResultContracts.GetContent()) { uri: Uri? ->\nuri?.let {\nbinding.imageView.setImageURI(it)\nbinding.buttonProcess.isEnabled = true\n\nbitmapBuffer = ImageDecoder.decodeBitmap(\nImageDecoder.createSource(\nrequireContext().contentResolver,\nit\n)\n) { decoder, _, _ ->\ndecoder.setTargetColorSpace(ColorSpace.get(ColorSpace.Named.SRGB))\ndecoder.allocator = ImageDecoder.ALLOCATOR_SOFTWARE\ndecoder.setTargetSampleSize(1)\n}\n}\n}\n```  \nClick **Load** to launch the `ActivityResult`.\nIt allows the user to select an image from their device.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/ImageFragment.kt#L69)):\n```kotlin\nbinding.buttonLoad.setOnClickListener {\ngetContent.launch(\"image/*\")\n}\n```",
      "<2-hop>\n\nThe `BufferSetInfo` data class holds the information about the buffer set used in the neural network model.\nIt includes the memory location of the buffer set (`buffer_set`), the number of input buffers (`n_in_buf`), and the number of output buffers (`n_out_buf`).\nUse the data class to return this information from the JNI library.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/enn_type/BufferSetInfo.kt)):\n```kotlin\nclass BufferSetInfo {\nvar buffer_set: Long = 0\nvar n_in_buf: Int = 0\nvar n_out_buf: Int = 0\n}\n```"
    ],
    "reference": "The `ActivityResult` object facilitates image retrieval by allowing users to select an image from their device media through the `getContent` method. Once an image is selected, it is displayed in an ImageView and converted to a bitmap for further processing. The `BufferSetInfo` data class plays a crucial role in the neural network model by holding information about the buffer set used for processing the image. It includes details such as the memory location of the buffer set, the number of input buffers, and the number of output buffers, which are essential for managing the data flow within the neural network.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\function-implementation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:00.126484+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How do you create a ModelExecutor object in the fragment class and process the bitmap image retrieved from the device media using the ActivityResult object?",
    "reference_contexts": [
      "<1-hop>\n\nThe `ActivityResult` object `getContent` retrieves an image from the device media.\nThe selected image is displayed in an ImageView and converted to a bitmap.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/ImageFragment.kt#L28)):\n```kotlin\nprivate val getContent =\nregisterForActivityResult(ActivityResultContracts.GetContent()) { uri: Uri? ->\nuri?.let {\nbinding.imageView.setImageURI(it)\nbinding.buttonProcess.isEnabled = true\n\nbitmapBuffer = ImageDecoder.decodeBitmap(\nImageDecoder.createSource(\nrequireContext().contentResolver,\nit\n)\n) { decoder, _, _ ->\ndecoder.setTargetColorSpace(ColorSpace.get(ColorSpace.Named.SRGB))\ndecoder.allocator = ImageDecoder.ALLOCATOR_SOFTWARE\ndecoder.setTargetSampleSize(1)\n}\n}\n}\n```  \nClick **Load** to launch the `ActivityResult`.\nIt allows the user to select an image from their device.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/ImageFragment.kt#L69)):\n```kotlin\nbinding.buttonLoad.setOnClickListener {\ngetContent.launch(\"image/*\")\n}\n```",
      "<2-hop>\n\nCreate the `ModelExecutor` object iin the `fragment` class with the current context and the fragment as `executorListener`. The `process` method of `modelExecutor` is called to start the image classification.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L53)):\n```kotlin\nmodelExecutor = ModelExecutor(\ncontext = requireContext(), executorListener = this\n)\n...\nmodelExecutor.process(bitmapBuffer)\n```  \nIn the `ModelExecutor.kt` class, the `process` method processes the image and calls the `onResults` method of `executorListener` to pass the results back to the fragment.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/executor/ModelExecutor.kt#L76)):\n```kotlin\nfun process(image: Bitmap) {\n...\n\nexecutorListener?.onResults(\nresult, inferenceTime\n)\n}\n```"
    ],
    "reference": "To create a ModelExecutor object in the fragment class, you need to initialize it with the current context and set the fragment as the executorListener. This is done using the following code: `modelExecutor = ModelExecutor(context = requireContext(), executorListener = this)`. After creating the ModelExecutor, you can process the bitmap image that was retrieved from the device media. The bitmap is obtained by using the `getContent` method, which retrieves an image and converts it to a bitmap. Once you have the bitmapBuffer, you can call the `process` method of the modelExecutor with the bitmap as an argument: `modelExecutor.process(bitmapBuffer)`. This method processes the image and calls the `onResults` method of the executorListener to pass the results back to the fragment.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\function-implementation.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:00.126484+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the function of PreveiwView in mobile applications?",
    "reference_contexts": [
      "- **PreviewView**: Displays a live camera feed for real-time classification.\n- **TextView**: Displays the classification result and score.  \nThe implemented UI resembles the figure as illustrated below.  \n<img src=\"./img/ui1.png\" alt=\"drawing\" width=\"200\"/>\n<img src=\"./img/ui2.png\" alt=\"drawing\" width=\"200\"/>"
    ],
    "reference": "PreviewView displays a live camera feed for real-time classification.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\setting-necessary-ui.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:22.640662+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How I set text for sample text in TextView?",
    "reference_contexts": [
      "The TextView displays the classification result and score to the user.\nIn the layout XML file, define a TextView with a unique ID.\nIn the Kotlin file, use this ID to reference the TextView and set its text to the classification result and score.  \n- Layout `app/res/layout/*.xml` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/res/layout/enn_info.xml#L18)):\n```xml\n<TextView\nandroid:id=\"@+id/sample_text\" />\n```  \n- Kotlin `app/java/package/fragments/Fragment.kt` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L194)):\n```kotlin\nbinding.sampleText.text = \"Sample Text\"\n```"
    ],
    "reference": "In the Kotlin file, you can set the text for the sample text in the TextView by using the unique ID defined in the layout XML file. For example, you can reference the TextView with the ID `sample_text` and set its text to \"Sample Text\" using the code: `binding.sampleText.text = \"Sample Text\"`.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\setting-necessary-ui.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:22.640662+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How is the layout defined in app/res/layout/*.xml for displaying an image?",
    "reference_contexts": [
      "The ImageView displays the image that is classified.\nIn the layout XML file, define an ImageView with a unique ID.\nIn the Kotlin file, use this ID to reference the ImageView and set its image to the selected image.  \n- Layout `app/res/layout/*.xml` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/res/layout/fragment_image.xml#L19)):\n```xml\n<ImageView\nandroid:id=\"@+id/sample_image\" />\n```  \n- Kotlin `app/java/package/fragments/Fragment.kt` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/ImageFragment.kt#L41)):\n```kotlin\nbinding.imageView.setImageBitmap(resizedImage)\n```"
    ],
    "reference": "In the layout XML file located at app/res/layout/*.xml, an ImageView is defined with a unique ID, such as `@+id/sample_image`, to display the classified image.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\setting-necessary-ui.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:22.640662+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How is the PreviewView utilized in mobile app development?",
    "reference_contexts": [
      "The PreviewView displays a live camera feed for real-time classification.\nIn the layout XML file, define a PreviewView with a unique ID.\nIn the Kotlin file, use this ID to reference the PreviewView and set up the camera preview.\nThis process involves creating a Preview object, binding it to the lifecycle of the Fragment, and setting its surface provider to the surface provider of PreviewView.  \n- Layout `app/res/layout/*.xml` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/res/layout/fragment_camera.xml#L9)):\n```xml\n<androidx.camera.view.PreviewView\nandroid:id=\"@+id/view_finder\" />\n```  \n- Kotlin `app/java/package/fragments/Fragment.kt` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L61)):\n```kotlin\nprivate fun setCamera() {\ncameraExecutor = Executors.newSingleThreadExecutor()\n\nval cameraProviderFuture = ProcessCameraProvider.getInstance(requireContext())\ncameraProviderFuture.addListener(\n{\nval cameraProvider = cameraProviderFuture.get()\nval cameraSelector = CameraSelector.DEFAULT_BACK_CAMERA\n\nsetPreview()\nsetImageAnalyzer()\n\ntry {\ncameraProvider.unbindAll()\ncamera = cameraProvider.bindToLifecycle(\nthis, cameraSelector, preview, imageAnalyzer\n)\n\npreview?.setSurfaceProvider(binding.viewFinder.surfaceProvider)\n} catch (exc: java.lang.Exception) {\nLog.e(TAG, \"Camera binding failed\", exc)\n}\n}, ContextCompat.getMainExecutor(requireContext())\n)\n}\n\nprivate fun setPreview() {\npreview = Preview.Builder().setTargetRotation(binding.viewFinder.display.rotation).build()\n}\n```"
    ],
    "reference": "The PreviewView is used to display a live camera feed for real-time classification. In the layout XML file, it is defined with a unique ID, and in the Kotlin file, this ID is referenced to set up the camera preview. This involves creating a Preview object, binding it to the lifecycle of the Fragment, and setting its surface provider to the surface provider of PreviewView.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\setting-necessary-ui.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:22.640662+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How do you use the TextView to display the classification result in relation to the ImageView that shows the classified image?",
    "reference_contexts": [
      "<1-hop>\n\nThe TextView displays the classification result and score to the user.\nIn the layout XML file, define a TextView with a unique ID.\nIn the Kotlin file, use this ID to reference the TextView and set its text to the classification result and score.  \n- Layout `app/res/layout/*.xml` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/res/layout/enn_info.xml#L18)):\n```xml\n<TextView\nandroid:id=\"@+id/sample_text\" />\n```  \n- Kotlin `app/java/package/fragments/Fragment.kt` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L194)):\n```kotlin\nbinding.sampleText.text = \"Sample Text\"\n```",
      "<2-hop>\n\nThe ImageView displays the image that is classified.\nIn the layout XML file, define an ImageView with a unique ID.\nIn the Kotlin file, use this ID to reference the ImageView and set its image to the selected image.  \n- Layout `app/res/layout/*.xml` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/res/layout/fragment_image.xml#L19)):\n```xml\n<ImageView\nandroid:id=\"@+id/sample_image\" />\n```  \n- Kotlin `app/java/package/fragments/Fragment.kt` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/ImageFragment.kt#L41)):\n```kotlin\nbinding.imageView.setImageBitmap(resizedImage)\n```"
    ],
    "reference": "To display the classification result using the TextView, you first define a TextView in the layout XML file with a unique ID, such as `@+id/sample_text`. In the Kotlin file, you reference this TextView using its ID and set its text to the classification result. Meanwhile, the ImageView, defined in the layout XML with a unique ID like `@+id/sample_image`, is used to display the image that has been classified. You set the image in the ImageView by referencing it in the Kotlin file and using the method `binding.imageView.setImageBitmap(resizedImage)` to show the selected image.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\setting-necessary-ui.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:22.640662+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What steps are involved in setting up the PreviewView for displaying a live camera feed in a mobile application, and how does this relate to the classification results shown in the TextView?",
    "reference_contexts": [
      "<1-hop>\n\n- **PreviewView**: Displays a live camera feed for real-time classification.\n- **TextView**: Displays the classification result and score.  \nThe implemented UI resembles the figure as illustrated below.  \n<img src=\"./img/ui1.png\" alt=\"drawing\" width=\"200\"/>\n<img src=\"./img/ui2.png\" alt=\"drawing\" width=\"200\"/>",
      "<2-hop>\n\nThe PreviewView displays a live camera feed for real-time classification.\nIn the layout XML file, define a PreviewView with a unique ID.\nIn the Kotlin file, use this ID to reference the PreviewView and set up the camera preview.\nThis process involves creating a Preview object, binding it to the lifecycle of the Fragment, and setting its surface provider to the surface provider of PreviewView.  \n- Layout `app/res/layout/*.xml` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/res/layout/fragment_camera.xml#L9)):\n```xml\n<androidx.camera.view.PreviewView\nandroid:id=\"@+id/view_finder\" />\n```  \n- Kotlin `app/java/package/fragments/Fragment.kt` ([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/image-classification/app/src/main/java/com/samsung/imageclassification/fragments/CameraFragment.kt#L61)):\n```kotlin\nprivate fun setCamera() {\ncameraExecutor = Executors.newSingleThreadExecutor()\n\nval cameraProviderFuture = ProcessCameraProvider.getInstance(requireContext())\ncameraProviderFuture.addListener(\n{\nval cameraProvider = cameraProviderFuture.get()\nval cameraSelector = CameraSelector.DEFAULT_BACK_CAMERA\n\nsetPreview()\nsetImageAnalyzer()\n\ntry {\ncameraProvider.unbindAll()\ncamera = cameraProvider.bindToLifecycle(\nthis, cameraSelector, preview, imageAnalyzer\n)\n\npreview?.setSurfaceProvider(binding.viewFinder.surfaceProvider)\n} catch (exc: java.lang.Exception) {\nLog.e(TAG, \"Camera binding failed\", exc)\n}\n}, ContextCompat.getMainExecutor(requireContext())\n)\n}\n\nprivate fun setPreview() {\npreview = Preview.Builder().setTargetRotation(binding.viewFinder.display.rotation).build()\n}\n```"
    ],
    "reference": "To set up the PreviewView for displaying a live camera feed in a mobile application, you first need to define a PreviewView with a unique ID in the layout XML file. For example, you would include the following line in your XML: `<androidx.camera.view.PreviewView android:id=\"@+id/view_finder\" />`. In the Kotlin file, you reference this ID to set up the camera preview. This involves creating a Preview object, binding it to the lifecycle of the Fragment, and setting its surface provider to the surface provider of the PreviewView. The process includes initializing a camera executor, obtaining a camera provider, and binding the camera to the lifecycle while setting the surface provider for the preview. The classification results are then displayed in a TextView, which shows the classification result and score based on the live feed processed in real-time by the application.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\setting-necessary-ui.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:22.640662+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What Navigation Component do in Android development?",
    "reference_contexts": [
      "Before you proceed, ensure you have a basic understanding of the following Android development concepts:  \n- **Kotlin**: The primary programming language for Android development.\n- [**View Binding**](https://developer.android.com/topic/libraries/view-binding): A feature that allows you to easily write code that interacts with views.\n- [**Fragments**](https://developer.android.com/guide/fragments): A reusable piece of an user interface or behavior of Android application.\n- [**Navigation Component**](https://developer.android.com/guide/navigation): A component that helps to implement navigation.  \nIf you are new to Android, it is recommended to review Chapters 1 to 3 of the [Android Basics in Kotlin](https://developer.android.com/courses/android-basics-kotlin/course) course.\nThis course describes the basics of developing Android application using Kotlin."
    ],
    "reference": "Navigation Component helps to implement navigation in Android applications.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:40.946487+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is Navigation Component and how it helps in Android development?",
    "reference_contexts": [
      "Before you proceed, ensure you have a basic understanding of the following Android development concepts:  \n- **Kotlin**: The primary programming language for Android development.\n- [**View Binding**](https://developer.android.com/topic/libraries/view-binding): A feature that allows you to easily write code that interacts with views.\n- [**Fragments**](https://developer.android.com/guide/fragments): A reusable piece of an user interface or behavior of Android application.\n- [**Navigation Component**](https://developer.android.com/guide/navigation): A component that helps to implement navigation.  \nIf you are new to Android, it is recommended to review Chapters 1 to 3 of the [Android Basics in Kotlin](https://developer.android.com/courses/android-basics-kotlin/course) course.\nThis course describes the basics of developing Android application using Kotlin."
    ],
    "reference": "Navigation Component is a component that helps to implement navigation in Android applications.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:40.946487+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How do you add the path to the Platform Tools directory in environment variables?",
    "reference_contexts": [
      "1. **To Download Android studio**, visit the [official website](https://developer.android.com/studio) and click **Download Android Studio**.\n1. **To install and set up Android Studio**, execute the downloaded file and follow the instructions of installation wizard.  \n> For more information, such as requirement for each OS, refer to [Install Android Studio](https://developer.android.com/studio/install) from Android Developers.\n1. Add the path to the Platform Tools directory in environment variables\n- Windows\n1. Navigate to Start > Control Panel > System > Advanced System Settings > Environment Variables.\n1. Select `Path` User variable, then select **Edit**.\n1. Select New and add `%USERPROFILE%\\AppData\\Local\\Android\\sdk\\platform-tools` directory.\n1. Select **Ok** to close all the settings windows.S  \n<img src=\"./img/windows-env.jpg\" alt=\"drawing\" width=\"400\"/>"
    ],
    "reference": "To add the path to the Platform Tools directory in environment variables on Windows, navigate to Start > Control Panel > System > Advanced System Settings > Environment Variables. Then, select the 'Path' User variable, click 'Edit', select New, and add '%USERPROFILE%\\AppData\\Local\\Android\\sdk\\platform-tools' directory. Finally, select 'Ok' to close all the settings windows.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:40.946487+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How do I add the Android SDK path in Windows environment variables?",
    "reference_contexts": [
      "1. **To Download Android studio**, visit the [official website](https://developer.android.com/studio) and click **Download Android Studio**.\n1. **To install and set up Android Studio**, execute the downloaded file and follow the instructions of installation wizard.  \n> For more information, such as requirement for each OS, refer to [Install Android Studio](https://developer.android.com/studio/install) from Android Developers.\n1. Add the path to the Platform Tools directory in environment variables\n- Windows\n1. Navigate to Start > Control Panel > System > Advanced System Settings > Environment Variables.\n1. Select `Path` User variable, then select **Edit**.\n1. Select New and add `%USERPROFILE%\\AppData\\Local\\Android\\sdk\\platform-tools` directory.\n1. Select **Ok** to close all the settings windows.S  \n<img src=\"./img/windows-env.jpg\" alt=\"drawing\" width=\"400\"/>"
    ],
    "reference": "To add the Android SDK path in Windows environment variables, navigate to Start > Control Panel > System > Advanced System Settings > Environment Variables. Then, select the `Path` User variable, click on **Edit**, select New, and add `%USERPROFILE%\\AppData\\Local\\Android\\sdk\\platform-tools` directory. Finally, select **Ok** to close all the settings windows.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:40.946487+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is build number in android?",
    "reference_contexts": [
      "Enabling Developer Mode in ERD board\n1. Open the **Settings** app.\n2. Scroll down and tap **About phone**.\n3. Find the **Build number** and tap it a few times until the **You are now a developer** message appears.  \nEnabling USB Debugging in ERD board\n1. Navigate to the main **Settings** screen.\n2. Scroll down and tap **System**.\nThe **Developer options** is now displayed.\n3. Tap **Developer options**, then scroll down and turn on **USB debugging**."
    ],
    "reference": "The build number is found in the About phone section of the Settings app, and you need to tap it a few times to enable Developer Mode.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:40.946487+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How do I enable USB Debugging on my Android device?",
    "reference_contexts": [
      "Enabling Developer Mode in ERD board\n1. Open the **Settings** app.\n2. Scroll down and tap **About phone**.\n3. Find the **Build number** and tap it a few times until the **You are now a developer** message appears.  \nEnabling USB Debugging in ERD board\n1. Navigate to the main **Settings** screen.\n2. Scroll down and tap **System**.\nThe **Developer options** is now displayed.\n3. Tap **Developer options**, then scroll down and turn on **USB debugging**."
    ],
    "reference": "To enable USB Debugging on your Android device, first navigate to the main Settings screen. Then, scroll down and tap System. The Developer options will now be displayed. Tap Developer options, then scroll down and turn on USB debugging.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:40.946487+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How do you connect the ERD board to your computer using USB, and what steps should be followed to ensure proper detection in Android Studio?",
    "reference_contexts": [
      "1. Connect the ERD board to your computer using a USB cable.\n1. In the pop-up that appears, select **Allow** to enable the USB debugging.\n1. Android Studio automatically detects the device. If the device is not detected, enable **File transfer** on the device."
    ],
    "reference": "To connect the ERD board to your computer using a USB cable, first, plug in the USB cable to both the board and the computer. When a pop-up appears, select **Allow** to enable USB debugging. After this, Android Studio will automatically detect the device. If the device is not detected, make sure to enable **File transfer** on the device.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:40.946487+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How do I connect my ERD board to my computer using USB for Android development?",
    "reference_contexts": [
      "1. Connect the ERD board to your computer using a USB cable.\n1. In the pop-up that appears, select **Allow** to enable the USB debugging.\n1. Android Studio automatically detects the device. If the device is not detected, enable **File transfer** on the device."
    ],
    "reference": "To connect your ERD board to your computer using USB, first, use a USB cable to make the connection. Then, in the pop-up that appears, select **Allow** to enable USB debugging. Android Studio will automatically detect the device; if it is not detected, ensure that **File transfer** is enabled on the device.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:40.946487+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How do you enable USB debugging mode and access Developer options on the ERD board for mobile app development?",
    "reference_contexts": [
      "<1-hop>\n\n***<span style=\"color:red\">WARNING:</span>* Updating ERD board binary results in *erasing all data* on ERD board.**  \nTo update the binary of an ERD board (Windows):\n1. Download ERD board binary from [resources](https:/./soc-developer.semiconductor.samsung.com/support/resource).\n1. Extract the contents of the downloaded `zip` file.\n1. Enable USB debugging mode and connect the device as demonstrated [here](#configuring-the-erd-board).\n1. Boot ERD board to `fastboot` bootloader mode.\n```bash\nadb reboot bootloader\n```\n1. Check if ERD board is ready to flash by executing:\n```bash\nfastboot devices\n```\n1. From the extracted files, find and execute `ff_erd9925_all.exe`.\n1. Press any key to continue.\n1. After `ff_erd9925_all.exe` is executed, the ERD Board reboots automatically and the following message appears.\n```shell\n=======================================\nFINISHED\n2023/11/10 21:43:36\nSUCCESS 1/1 fastboot devices\n0) 00000a8fcf39b308, SUCCESS, elapsed=168s\n=======================================\n```",
      "<2-hop>\n\nEnabling Developer Mode in ERD board\n1. Open the **Settings** app.\n2. Scroll down and tap **About phone**.\n3. Find the **Build number** and tap it a few times until the **You are now a developer** message appears.  \nEnabling USB Debugging in ERD board\n1. Navigate to the main **Settings** screen.\n2. Scroll down and tap **System**.\nThe **Developer options** is now displayed.\n3. Tap **Developer options**, then scroll down and turn on **USB debugging**."
    ],
    "reference": "To enable USB debugging mode on the ERD board, first navigate to the main Settings screen, then scroll down and tap System. The Developer options will now be displayed. Tap Developer options, then scroll down and turn on USB debugging. To access Developer options, open the Settings app, scroll down to About phone, find the Build number, and tap it a few times until the You are now a developer message appears.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:40.946487+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What are the steps to update the ERD board binary on a Windows system, and how does this process relate to setting up Android Studio on the same operating system?",
    "reference_contexts": [
      "<1-hop>\n\n1. **To Download Android studio**, visit the [official website](https://developer.android.com/studio) and click **Download Android Studio**.\n1. **To install and set up Android Studio**, execute the downloaded file and follow the instructions of installation wizard.  \n> For more information, such as requirement for each OS, refer to [Install Android Studio](https://developer.android.com/studio/install) from Android Developers.\n1. Add the path to the Platform Tools directory in environment variables\n- Windows\n1. Navigate to Start > Control Panel > System > Advanced System Settings > Environment Variables.\n1. Select `Path` User variable, then select **Edit**.\n1. Select New and add `%USERPROFILE%\\AppData\\Local\\Android\\sdk\\platform-tools` directory.\n1. Select **Ok** to close all the settings windows.S  \n<img src=\"./img/windows-env.jpg\" alt=\"drawing\" width=\"400\"/>",
      "<2-hop>\n\n***<span style=\"color:red\">WARNING:</span>* Updating ERD board binary results in *erasing all data* on ERD board.**  \nTo update the binary of an ERD board (Windows):\n1. Download ERD board binary from [resources](https:/./soc-developer.semiconductor.samsung.com/support/resource).\n1. Extract the contents of the downloaded `zip` file.\n1. Enable USB debugging mode and connect the device as demonstrated [here](#configuring-the-erd-board).\n1. Boot ERD board to `fastboot` bootloader mode.\n```bash\nadb reboot bootloader\n```\n1. Check if ERD board is ready to flash by executing:\n```bash\nfastboot devices\n```\n1. From the extracted files, find and execute `ff_erd9925_all.exe`.\n1. Press any key to continue.\n1. After `ff_erd9925_all.exe` is executed, the ERD Board reboots automatically and the following message appears.\n```shell\n=======================================\nFINISHED\n2023/11/10 21:43:36\nSUCCESS 1/1 fastboot devices\n0) 00000a8fcf39b308, SUCCESS, elapsed=168s\n=======================================\n```"
    ],
    "reference": "To update the ERD board binary on a Windows system, follow these steps: First, download the ERD board binary from the resources page and extract the contents of the downloaded zip file. Then, enable USB debugging mode on the device and connect it. Boot the ERD board into fastboot bootloader mode by executing the command `adb reboot bootloader`. Check if the ERD board is ready to flash by running `fastboot devices`. Next, from the extracted files, find and execute `ff_erd9925_all.exe`, and press any key to continue. After execution, the ERD board will reboot automatically, indicating success. This process is distinct yet related to setting up Android Studio on Windows, where you also need to add the path to the Platform Tools directory in the environment variables, ensuring that the necessary tools for development and device management are properly configured.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-android-samples\\guide\\starting-android-project.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:40.946487+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Can you explain how to use Exynos Neural Network Software Development Kit for making native programs and what is the main goal of this guide?",
    "reference_contexts": [
      "This guide provides a detailed walkthrough for developing a native program using the Exynos Neural Network Software Development Kit (Exynos AI Studio).\nIt describes the steps for developing a native program that leverages the Exynos AI Studio to execute neural network models on Samsung Exynos hardware within the adb shell environment.\nThe guide aims at equipping developers with the necessary knowledge so that they can create native binaries for testing their models without having to create an Android application."
    ],
    "reference": "This guide provides a detailed walkthrough for developing a native program using the Exynos Neural Network Software Development Kit (Exynos AI Studio). It describes the steps for developing a native program that leverages the Exynos AI Studio to execute neural network models on Samsung Exynos hardware within the adb shell environment. The guide aims at equipping developers with the necessary knowledge so that they can create native binaries for testing their models without having to create an Android application.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:58.996430+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How does Exynos AI Studio assist developers in creating native programs?",
    "reference_contexts": [
      "This guide provides a detailed walkthrough for developing a native program using the Exynos Neural Network Software Development Kit (Exynos AI Studio).\nIt describes the steps for developing a native program that leverages the Exynos AI Studio to execute neural network models on Samsung Exynos hardware within the adb shell environment.\nThe guide aims at equipping developers with the necessary knowledge so that they can create native binaries for testing their models without having to create an Android application."
    ],
    "reference": "Exynos AI Studio provides a detailed walkthrough for developing native programs that leverage its capabilities to execute neural network models on Samsung Exynos hardware within the adb shell environment. The guide equips developers with the necessary knowledge to create native binaries for testing their models without the need to create an Android application.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:58.996430+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How Exynos AI Studio help with NNC models?",
    "reference_contexts": [
      "The [Exynos AI Studio](https://soc-developer.semiconductor.samsung.com/enn-sdk) tool facilitates the conversion of [TFLite](https://www.tensorflow.org/lite) neural network models into NNC format models.\nThis conversion enables the NN models to execute efficiently on the [Samsung Exynos](https://semiconductor.samsung.com/processor/) hardware to ensure optimal performance.\nThe Exynos AI Studio provides a framework that facilitates the execution of NNC models on the Exynos platform."
    ],
    "reference": "The Exynos AI Studio tool helps by converting TFLite neural network models into NNC format models, which allows the NN models to execute efficiently on Samsung Exynos hardware for optimal performance.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:58.996430+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How does Samsung Exynos help in making neural network models work better on mobile apps using the ENN framework?",
    "reference_contexts": [
      "The [Exynos AI Studio](https://soc-developer.semiconductor.samsung.com/enn-sdk) tool facilitates the conversion of [TFLite](https://www.tensorflow.org/lite) neural network models into NNC format models.\nThis conversion enables the NN models to execute efficiently on the [Samsung Exynos](https://semiconductor.samsung.com/processor/) hardware to ensure optimal performance.\nThe Exynos AI Studio provides a framework that facilitates the execution of NNC models on the Exynos platform."
    ],
    "reference": "Samsung Exynos helps by allowing the Exynos AI Studio tool to convert TFLite neural network models into NNC format models, which then execute efficiently on the Exynos hardware to ensure optimal performance.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:58.996430+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What files in enn-sdk-samples-9925?",
    "reference_contexts": [
      "In this sample, a converted NNC file and raw input/output file available in the [Github Repository](https://github.com/exynos-eco/enn-sdk-samples-9925) are used."
    ],
    "reference": "In enn-sdk-samples-9925, a converted NNC file and raw input/output file are used.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:58.996430+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Can you explain the significance of the Github Repository mentioned in the context, particularly in relation to the sample files provided?",
    "reference_contexts": [
      "In this sample, a converted NNC file and raw input/output file available in the [Github Repository](https://github.com/exynos-eco/enn-sdk-samples-9925) are used."
    ],
    "reference": "The Github Repository mentioned in the context is significant as it contains a converted NNC file and a raw input/output file, which are essential for understanding and utilizing the ENN framework and NDK in mobile application development.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:58.996430+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How to run the exynos-eco program?",
    "reference_contexts": [
      "To execute the sample native program, refer to [README](https://github.com/exynos-eco/enn-sdk-samples-9925/tree/main/nnc-model-tester#readme) file."
    ],
    "reference": "To run the exynos-eco program, you need to refer to the README file.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:58.996430+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How do I execute the sample native program using exynos-eco?",
    "reference_contexts": [
      "To execute the sample native program, refer to [README](https://github.com/exynos-eco/enn-sdk-samples-9925/tree/main/nnc-model-tester#readme) file."
    ],
    "reference": "To execute the sample native program, refer to the README file.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:58.996430+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How do you write a C++ program using the Exynos AI Studio and compile it with NDK?",
    "reference_contexts": [
      "<1-hop>\n\nThe [Exynos AI Studio](https://soc-developer.semiconductor.samsung.com/enn-sdk) tool facilitates the conversion of [TFLite](https://www.tensorflow.org/lite) neural network models into NNC format models.\nThis conversion enables the NN models to execute efficiently on the [Samsung Exynos](https://semiconductor.samsung.com/processor/) hardware to ensure optimal performance.\nThe Exynos AI Studio provides a framework that facilitates the execution of NNC models on the Exynos platform.",
      "<2-hop>\n\nThis guide comprises the following sections:\n1. [**Writing Native Program**](getting-started-with-native-samples/writing-native-program): This section provides the process of writing a C++ program to implement the ENN framework.\n1. [**Compiling Using NDK**](getting-started-with-native-samples/compiling-using-ndk): This section provides the step-by-step process to compile the native program using NDK.\n1. [**Using ADB to Execute Native Program**](getting-started-with-native-samples/using-adb): This section explains the method to execute the native program using ADB.  \nThe general workflow of writing and executing a native program using the Exynos AI Studio is described in the following flowchart.  \n```mermaid\ngraph TB\n\nA[Start]\nB[Write Native Program]\nsubgraph C[Android Native Development Kit]\nC1[Create Makefile]-->C2[Compile Native Program]\nend\nsubgraph D[Android Debug Bridge]\nD1[Push Native Program and Data]--> D2[Execute Native Program]\nend\nE[End]\nA-->B-->C-->D-->E\n\n```"
    ],
    "reference": "To write a C++ program using the Exynos AI Studio, you can refer to the section titled 'Writing Native Program,' which outlines the process of implementing the ENN framework. After writing the program, you can compile it using the NDK by following the 'Compiling Using NDK' section, which provides a step-by-step guide for this process. The general workflow involves writing the native program, creating a Makefile, compiling the program, and then using ADB to execute it.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:58.996430+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the role of TFLite in the conversion process facilitated by the Exynos AI Studio for executing neural network models on Samsung Exynos hardware?",
    "reference_contexts": [
      "<1-hop>\n\nThe [Exynos AI Studio](https://soc-developer.semiconductor.samsung.com/enn-sdk) tool facilitates the conversion of [TFLite](https://www.tensorflow.org/lite) neural network models into NNC format models.\nThis conversion enables the NN models to execute efficiently on the [Samsung Exynos](https://semiconductor.samsung.com/processor/) hardware to ensure optimal performance.\nThe Exynos AI Studio provides a framework that facilitates the execution of NNC models on the Exynos platform.",
      "<2-hop>\n\n|Terms|Expanded Form|\n|-|-|\n|Exynos AI Studio|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NN|Neural Network|\n|NNC|Neural Network Container|\n|TFLite|TensorFlow Lite|"
    ],
    "reference": "TFLite, which stands for TensorFlow Lite, plays a crucial role in the conversion process facilitated by the Exynos AI Studio. The Exynos AI Studio tool converts TFLite neural network models into NNC format models, enabling these models to execute efficiently on Samsung Exynos hardware. This conversion ensures optimal performance of the neural network models on the Exynos platform.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:31:58.996430+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How is ADB used in executing programs on ERD boards?",
    "reference_contexts": [
      "This section provides the detailed information on using the ADB to execute the native program on the ERD board.\nThis process comprises of two main steps such as copying data to the board and executing the native program on the ERD board."
    ],
    "reference": "ADB is used to execute native programs on the ERD board by following two main steps: copying data to the board and then executing the native program.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:32:14.226302+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How do you use ADB for run native program on ERD board?",
    "reference_contexts": [
      "This section provides the detailed information on using the ADB to execute the native program on the ERD board.\nThis process comprises of two main steps such as copying data to the board and executing the native program on the ERD board."
    ],
    "reference": "You use ADB to execute the native program on the ERD board by following two main steps: first, you copy data to the board, and then you execute the native program on the ERD board.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:32:14.226302+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What are the main steps involved in using the ADB to execute a native program on the ERD bord?",
    "reference_contexts": [
      "This section provides the detailed information on using the ADB to execute the native program on the ERD board.\nThis process comprises of two main steps such as copying data to the board and executing the native program on the ERD board."
    ],
    "reference": "The main steps involved in using the ADB to execute a native program on the ERD board are copying data to the board and executing the native program on the ERD board.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:32:14.226302+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What does adb do in the context of ERD boards?",
    "reference_contexts": [
      "The following commands copy the necessary files to the ERD board:  \n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/* /data/local/tmp/\n```  \nThese commands perform the following actions:  \n- `adb push` is used to copy the model file (`model.nnc`), input data file (`input.bin`), golden data file (`golden.bin`), library file (`libenn_public_api_ndk_v1.so`), and the native program (`enn_nnc_model_tester`) to the `/data/vendor/enn/` directory on the ERD board."
    ],
    "reference": "adb push is used to copy the model file (model.nnc), input data file (input.bin), golden data file (golden.bin), library file (libenn_public_api_ndk_v1.so), and the native program (enn_nnc_model_tester) to the /data/vendor/enn/ directory on the ERD board.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:32:14.226302+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How to use enn_nnc_model_tester on ERD board?",
    "reference_contexts": [
      "The following commands copy the necessary files to the ERD board:  \n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/* /data/local/tmp/\n```  \nThese commands perform the following actions:  \n- `adb push` is used to copy the model file (`model.nnc`), input data file (`input.bin`), golden data file (`golden.bin`), library file (`libenn_public_api_ndk_v1.so`), and the native program (`enn_nnc_model_tester`) to the `/data/vendor/enn/` directory on the ERD board."
    ],
    "reference": "To use enn_nnc_model_tester on the ERD board, you need to copy it along with the necessary files using the following commands: `adb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/`, `adb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/`, and `adb push example/* /data/local/tmp/`.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:32:14.226302+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the role of libenn_public_api_ndk_v1.so in the process of copying files to the ERD board?",
    "reference_contexts": [
      "The following commands copy the necessary files to the ERD board:  \n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/* /data/local/tmp/\n```  \nThese commands perform the following actions:  \n- `adb push` is used to copy the model file (`model.nnc`), input data file (`input.bin`), golden data file (`golden.bin`), library file (`libenn_public_api_ndk_v1.so`), and the native program (`enn_nnc_model_tester`) to the `/data/vendor/enn/` directory on the ERD board."
    ],
    "reference": "The library file libenn_public_api_ndk_v1.so is copied to the ERD board using the command 'adb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/', which is part of the process to ensure that the necessary files are available for execution on the board.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:32:14.226302+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How use adb for run program?",
    "reference_contexts": [
      "After copying the necessary files to the ERD board, execute the native program using the following commands:  \n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```  \nThese commands perform the following actions:  \n- `adb shell` starts a shell session on the board.\n- `cd /data/local/tmp/` changes the current directory to `/data/local/tmp/`, where the necessary files are available.\n- The `LD_LIBRARY_PATH` environment variable sets the directory that contains `libenn_public_api_ndk_v1.so`.\n- The native program is executed with the `--model`, `--input`, `--golden`, and `--threshold` parameters, which specify the model file, input data file, golden data file, and threshold value, respectively."
    ],
    "reference": "To use adb for running the program, first start a shell session on the board with `adb shell`. Then, change the directory to where the necessary files are located using `cd /data/local/tmp/`. After that, set the `LD_LIBRARY_PATH` with `export LD_LIBRARY_PATH=/data/local/tmp` and finally execute the native program with the command `./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001`.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:32:14.226302+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What role does LD_LIBRARY_PATH play when executing a native program on an ERD board?",
    "reference_contexts": [
      "After copying the necessary files to the ERD board, execute the native program using the following commands:  \n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```  \nThese commands perform the following actions:  \n- `adb shell` starts a shell session on the board.\n- `cd /data/local/tmp/` changes the current directory to `/data/local/tmp/`, where the necessary files are available.\n- The `LD_LIBRARY_PATH` environment variable sets the directory that contains `libenn_public_api_ndk_v1.so`.\n- The native program is executed with the `--model`, `--input`, `--golden`, and `--threshold` parameters, which specify the model file, input data file, golden data file, and threshold value, respectively."
    ],
    "reference": "The LD_LIBRARY_PATH environment variable sets the directory that contains libenn_public_api_ndk_v1.so, which is necessary for executing the native program.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:32:14.226302+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What role does the LD_LIBRARY_PATH environment variable play in executing native programs on the ERD board?",
    "reference_contexts": [
      "After copying the necessary files to the ERD board, execute the native program using the following commands:  \n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```  \nThese commands perform the following actions:  \n- `adb shell` starts a shell session on the board.\n- `cd /data/local/tmp/` changes the current directory to `/data/local/tmp/`, where the necessary files are available.\n- The `LD_LIBRARY_PATH` environment variable sets the directory that contains `libenn_public_api_ndk_v1.so`.\n- The native program is executed with the `--model`, `--input`, `--golden`, and `--threshold` parameters, which specify the model file, input data file, golden data file, and threshold value, respectively."
    ],
    "reference": "The LD_LIBRARY_PATH environment variable sets the directory that contains libenn_public_api_ndk_v1.so, which is necessary for executing the native program.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:32:14.226302+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What ADB commands are required to copy files to the ERD board and subsequently execute the native program?",
    "reference_contexts": [
      "<1-hop>\n\nThe following commands copy the necessary files to the ERD board:  \n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/* /data/local/tmp/\n```  \nThese commands perform the following actions:  \n- `adb push` is used to copy the model file (`model.nnc`), input data file (`input.bin`), golden data file (`golden.bin`), library file (`libenn_public_api_ndk_v1.so`), and the native program (`enn_nnc_model_tester`) to the `/data/vendor/enn/` directory on the ERD board.",
      "<2-hop>\n\nAfter copying the necessary files to the ERD board, execute the native program using the following commands:  \n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```  \nThese commands perform the following actions:  \n- `adb shell` starts a shell session on the board.\n- `cd /data/local/tmp/` changes the current directory to `/data/local/tmp/`, where the necessary files are available.\n- The `LD_LIBRARY_PATH` environment variable sets the directory that contains `libenn_public_api_ndk_v1.so`.\n- The native program is executed with the `--model`, `--input`, `--golden`, and `--threshold` parameters, which specify the model file, input data file, golden data file, and threshold value, respectively."
    ],
    "reference": "To copy files to the ERD board, the following ADB commands are used: `adb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/`, `adb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/`, and `adb push example/* /data/local/tmp/`. After copying the necessary files, the native program is executed using the commands: `adb shell`, `cd /data/local/tmp/`, and `export LD_LIBRARY_PATH=/data/local/tmp`, followed by `./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001`.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:32:14.226302+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What are the steps to execute the native program on the ERD board after copying the necessary files, and how does the `LD_LIBRARY_PATH` relate to `libenn_public_api_ndk_v1.so`?",
    "reference_contexts": [
      "<1-hop>\n\nThe following commands copy the necessary files to the ERD board:  \n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/* /data/local/tmp/\n```  \nThese commands perform the following actions:  \n- `adb push` is used to copy the model file (`model.nnc`), input data file (`input.bin`), golden data file (`golden.bin`), library file (`libenn_public_api_ndk_v1.so`), and the native program (`enn_nnc_model_tester`) to the `/data/vendor/enn/` directory on the ERD board.",
      "<2-hop>\n\nAfter copying the necessary files to the ERD board, execute the native program using the following commands:  \n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```  \nThese commands perform the following actions:  \n- `adb shell` starts a shell session on the board.\n- `cd /data/local/tmp/` changes the current directory to `/data/local/tmp/`, where the necessary files are available.\n- The `LD_LIBRARY_PATH` environment variable sets the directory that contains `libenn_public_api_ndk_v1.so`.\n- The native program is executed with the `--model`, `--input`, `--golden`, and `--threshold` parameters, which specify the model file, input data file, golden data file, and threshold value, respectively."
    ],
    "reference": "After copying the necessary files to the ERD board, the steps to execute the native program are as follows: First, start a shell session on the board using the command `adb shell`. Then, change the current directory to `/data/local/tmp/` with `cd /data/local/tmp/`, where the necessary files are located. The `LD_LIBRARY_PATH` environment variable is set to `/data/local/tmp`, which is crucial because it specifies the directory that contains the library file `libenn_public_api_ndk_v1.so`. Finally, the native program is executed with the command `./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001`, where the parameters define the model file, input data file, golden data file, and threshold value.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\using-adb.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:32:14.226302+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How do I properly set up the ENN framwork for my AI model development?",
    "reference_contexts": [
      "Download the ENN framework library (ENN Public API NDK) from [resources](https://soc-developer.semiconductor.samsung.com/support/resource).\nNext, copy the necessary libraries by perform the following steps:  \n1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/jni/lib64``\n2. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/jni/include`\n3. Add following code in `en_nnc_model_tester.cpp` to load ENN framework.\n([Example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L13)):\n```cpp\n#include \"include/enn_api-public_ndk_v1.hpp\"\n```"
    ],
    "reference": "To set up the ENN framework, first download the ENN framework library (ENN Public API NDK) from the provided resources. Then, copy the necessary libraries by performing the following steps: 1. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/jni/lib64`. 2. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/jni/include`. 3. Add the following code in `en_nnc_model_tester.cpp` to load the ENN framework: `#include \"include/enn_api-public_ndk_v1.hpp\"`.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\writing-native-program.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:32:41.190831+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Can you tell me what steps is for executing NN models on the ENN framework and where I can find more info about it?",
    "reference_contexts": [
      "This section describes the steps for executing NN models on the ENN framework.\nFor more information on the ENN framework, refer to the [documentation](developer-guide#4-enn-framework-api)."
    ],
    "reference": "This section describes the steps for executing NN models on the ENN framework. For more information on the ENN framework, refer to the documentation.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\writing-native-program.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:32:41.190831+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What parameters are necessary to execute ML models using the ENN framework?",
    "reference_contexts": [
      "To execute NNC models using the ENN framework, the following parameters are required:  \n|Parameter|Data Type|Explanation|\n|--|--|--|\n|`model_name`|string|Path to the ML model file|\n|`inputs`|vector&lt;string&gt;|List of input file paths|\n|`goldens` (optional)|vector&lt;string&gt;|List of golden file paths for validation|\n|`threshold` (optional)|float|Threshold for golden matching, used to determine the acceptable deviation|"
    ],
    "reference": "To execute ML models using the ENN framework, the following parameters are required: `model_name` (string) which is the path to the ML model file, `inputs` (vector<string>) which is a list of input file paths, and optionally, `goldens` (vector<string>) which is a list of golden file paths for validation, and `threshold` (float) which is the threshold for golden matching, used to determine the acceptable deviation.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\writing-native-program.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:32:41.190831+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What are the steps involved in executing NN models on the ENN framework?",
    "reference_contexts": [
      "Executing NN models on the ENN framework comprises of three steps such as initializing framework, inferring the models, and deinitializing the framework.  \nThe following chart describes the lifecycle and process of inferring NN models using the ENN framework.  \n```mermaid\ngraph TB\nsubgraph A[Initialize ENN Framework]\nA1[Initialize]\nA1 --> A2[Open Model]\nA2 --> A3[Allocate/Commit Buffers]\nend\nsubgraph B[Inference]\nB1[Copy Input Layer]\nB1 --> B2[Execute Model]\nB2 --> B3[Copy Output Layer]\nend\nsubgraph C[Deinitialize]\nC1[Release Buffers]\nC1 --> C2[Close Model]\nC2 --> C3[Deinitialize]\nend\nA --> B\nB --> C\n```  \nTo infer multiple data, repeat `Inference`."
    ],
    "reference": "Executing NN models on the ENN framework involves three steps: initializing the framework, inferring the models, and deinitializing the framework. The initialization includes opening the model and allocating/committing buffers. The inference step consists of copying the input layer, executing the model, and copying the output layer. Finally, the deinitialization step involves releasing buffers, closing the model, and deinitializing the framework.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\writing-native-program.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:32:41.190831+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What EnnInitialize do?",
    "reference_contexts": [
      "Before executing ML models on the ENN framework, initialize the framework, load the model, and allocate the necessary buffers.  \n1. [EnnInitialize](api-reference/enn-framework-api-functions#function-enninitialize):\nThis function initializes the ENN Framework.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L48)):\n```cpp\nenn::api::EnnInitialize();\n```  \n1. [EnnOpenModel](api-reference/enn-framework-api-functions#function-ennopenmodel):\nThis function opens the specified model and return a model ID.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L56)):\n```cpp\nEnnModelId model_id;\nenn::api::EnnOpenModel(model_name.c_str(), &model_id);\n```  \n1. [EnnAllocateAllBuffers](api-reference/enn-framework-api-functions#function-ennallocateallbuffers):\nThis function allocates all the necessary buffers for the model.\nIt also provides the number of input/output buffers (`buffer_info`), their locations, and sizes (`buffer_set`).  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L68)):\n```cpp\nEnnBufferPtr *buffer_set;\nNumberOfBuffersInfo buffer_info;\nenn::api::EnnAllocateAllBuffers(model_id, &buffer_set, &buffer_info);\n```  \nFollowing is the data structure of `EnnBufferPtr` and `NumberOfBuffersInfo`.  \n```cpp\ntypedef struct _ennBuffer {\nvoid *va;\nuint32_t size;\nuint32_t offset;\n} EnnBuffer;\n\ntypedef EnnBuffer* EnnBufferPtr;\n\ntypedef struct _NumberOfBuffersInfo {\nuint32_t n_in_buf;\nuint32_t n_out_buf;\n} NumberOfBuffersInfo;\n```"
    ],
    "reference": "EnnInitialize function initializes the ENN Framework.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\writing-native-program.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:32:41.190831+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What EnnReleaseBuffers do in enn framework?",
    "reference_contexts": [
      "After executing the model, deinitialize the framework to release resources.  \n1. [EnnReleaseBuffers](api-reference/enn-framework-api-functions#function-ennreleasebuffers):\nThis function releases the allocated buffers.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L112)):\n```cpp\nenn::api::EnnReleaseBuffers(buffer_set, buffer_info.n_in_buf + buffer_info.n_out_buf)\n```  \n1. [EnnCloseModel](api-reference/enn-framework-api-functions#function-ennclosemodel):\nThis function closes the specified model.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L118)):\n```cpp\nenn::api::EnnCloseModel(model_id)\n```  \n1. [EnnDeinitialize](api-reference/enn-framework-api-functions#function-enndeinitialize):\nThis function deinitializes the ENN framework.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L124)):\n```cpp\nenn::api::EnnDeinitialize()\n```"
    ],
    "reference": "EnnReleaseBuffers function releases the allocated buffers after executing the model and deinitializing the framework to release resources.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\writing-native-program.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:32:41.190831+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What steps are involved in data provision and model execution in the ENN framework?",
    "reference_contexts": [
      "<1-hop>\n\nTo provide the data to the model, copy input data to the specified memory location.\nUse the [EnnAllocateAllBuffer](#ennallocateallbuffers) function to obtain the input memory location.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L161)):\n```cpp\nFILE *f = fopen(filename, \"rb\");\nchar *dst = reinterpret_cast<char *>(buffer_set[idx]->va);\n\nsize_t size;\n\nfseek(f, 0, SEEK_END);\nsize = ftell(f);\nfseek(f, 0, SEEK_SET);\n\nfread(dst, 1, size, f);\n\nfclose(f);\n```",
      "<2-hop>\n\nAfter executing the model, deinitialize the framework to release resources.  \n1. [EnnReleaseBuffers](api-reference/enn-framework-api-functions#function-ennreleasebuffers):\nThis function releases the allocated buffers.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L112)):\n```cpp\nenn::api::EnnReleaseBuffers(buffer_set, buffer_info.n_in_buf + buffer_info.n_out_buf)\n```  \n1. [EnnCloseModel](api-reference/enn-framework-api-functions#function-ennclosemodel):\nThis function closes the specified model.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L118)):\n```cpp\nenn::api::EnnCloseModel(model_id)\n```  \n1. [EnnDeinitialize](api-reference/enn-framework-api-functions#function-enndeinitialize):\nThis function deinitializes the ENN framework.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L124)):\n```cpp\nenn::api::EnnDeinitialize()\n```",
      "<3-hop>\n\nExecuting NN models on the ENN framework comprises of three steps such as initializing framework, inferring the models, and deinitializing the framework.  \nThe following chart describes the lifecycle and process of inferring NN models using the ENN framework.  \n```mermaid\ngraph TB\nsubgraph A[Initialize ENN Framework]\nA1[Initialize]\nA1 --> A2[Open Model]\nA2 --> A3[Allocate/Commit Buffers]\nend\nsubgraph B[Inference]\nB1[Copy Input Layer]\nB1 --> B2[Execute Model]\nB2 --> B3[Copy Output Layer]\nend\nsubgraph C[Deinitialize]\nC1[Release Buffers]\nC1 --> C2[Close Model]\nC2 --> C3[Deinitialize]\nend\nA --> B\nB --> C\n```  \nTo infer multiple data, repeat `Inference`."
    ],
    "reference": "The steps involved in data provision include copying input data to the specified memory location using the EnnAllocateAllBuffer function. After the model is executed, the steps for model execution include deinitializing the framework to release resources, which involves using functions like EnnReleaseBuffers to release allocated buffers, EnnCloseModel to close the specified model, and EnnDeinitialize to deinitialize the ENN framework.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\writing-native-program.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:32:41.190831+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What are the necessary parameters for executing NNC models using the ENN framework, and what steps should be taken to properly deinitialize the framework after execution?",
    "reference_contexts": [
      "<1-hop>\n\nAfter executing the model, deinitialize the framework to release resources.  \n1. [EnnReleaseBuffers](api-reference/enn-framework-api-functions#function-ennreleasebuffers):\nThis function releases the allocated buffers.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L112)):\n```cpp\nenn::api::EnnReleaseBuffers(buffer_set, buffer_info.n_in_buf + buffer_info.n_out_buf)\n```  \n1. [EnnCloseModel](api-reference/enn-framework-api-functions#function-ennclosemodel):\nThis function closes the specified model.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L118)):\n```cpp\nenn::api::EnnCloseModel(model_id)\n```  \n1. [EnnDeinitialize](api-reference/enn-framework-api-functions#function-enndeinitialize):\nThis function deinitializes the ENN framework.  \n([example](https://github.com/exynos-eco/enn-sdk-samples-9925/blob/main/nnc-model-tester/jni/enn_nnc_model_tester.cpp#L124)):\n```cpp\nenn::api::EnnDeinitialize()\n```",
      "<2-hop>\n\nTo execute NNC models using the ENN framework, the following parameters are required:  \n|Parameter|Data Type|Explanation|\n|--|--|--|\n|`model_name`|string|Path to the ML model file|\n|`inputs`|vector&lt;string&gt;|List of input file paths|\n|`goldens` (optional)|vector&lt;string&gt;|List of golden file paths for validation|\n|`threshold` (optional)|float|Threshold for golden matching, used to determine the acceptable deviation|"
    ],
    "reference": "To execute NNC models using the ENN framework, the following parameters are required: `model_name` (a string representing the path to the ML model file), `inputs` (a vector of strings listing the input file paths), and optionally, `goldens` (a vector of strings for validation) and `threshold` (a float for determining acceptable deviation). After executing the model, it is important to deinitialize the framework to release resources. This involves using the function `EnnReleaseBuffers` to release allocated buffers, followed by `EnnCloseModel` to close the specified model, and finally, `EnnDeinitialize` to deinitialize the ENN framework.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\getting-started-with-native-samples\\guide\\writing-native-program.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:32:41.190831+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What CameraX do?",
    "reference_contexts": [
      "The CameraX library, an integral component of Android Jetpack, has been designed to simplify camera operations across various Android devices. It offers a consistent and intuitive API, ensuring developers face minimal challenges when incorporating camera functionalities. This documentation delves deeper into the integration process: from initializing the hardware to intricately binding the camera's lifecycle."
    ],
    "reference": "CameraX library made to simplify camera operations on Android devices, giving a consistent API for developers.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\other\\WorkingWithTheCamera.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:33:06.166795+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How I can make sure my Android Studio setup is good for making apps with camera features?",
    "reference_contexts": [
      "1. Proficiency with Android application development.\n2. A configured Android Studio environment.\n3. CameraX library integrated into your project's Gradle dependencies."
    ],
    "reference": "To ensure your Android Studio setup is good for making apps with camera features, you need to have proficiency with Android application development, a configured Android Studio environment, and the CameraX library integrated into your project's Gradle dependencies.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\other\\WorkingWithTheCamera.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:33:06.166795+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Can you explain the role of ProccessCameraProvider in managing camera devices' lifecycle within an Android application?",
    "reference_contexts": [
      "**Objective**: Understand the necessary steps to access and initialize the camera hardware in your application.  \n**Class Utilized**: `ProcessCameraProvider` - A class that manages camera devices' lifecycle. It acts as a bridge between your application and the camera hardware."
    ],
    "reference": "The ProcessCameraProvider class is utilized to manage the lifecycle of camera devices. It serves as a bridge between your application and the camera hardware, facilitating access and initialization of the camera.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\other\\WorkingWithTheCamera.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:33:06.166795+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How does the cameraExecutor function in the context of Android application development?",
    "reference_contexts": [
      "```kotlin\nprivate var preview: Preview? = null\nprivate var camera: Camera? = null\n\nprivate fun setCamera() {\ncameraExecutor = Executors.newSingleThreadExecutor()\nval cameraProviderFuture = ProcessCameraProvider.getInstance(requireContext())\n\ncameraProviderFuture.addListener(\n{\nval cameraProvider = cameraProviderFuture.get()\nval cameraSelector = CameraSelector.DEFAULT_BACK_CAMERA\nsetPreview()\n\ntry {\ncameraProvider.unbindAll()\ncamera = cameraProvider.bindToLifecycle(this, cameraSelector, preview)\npreview?.setSurfaceProvider(binding.viewFinder.surfaceProvider)\n} catch (exc: java.lang.Exception) {\nLog.e(TAG, \"Camera binding failed\", exc)\n}\n},\nContextCompat.getMainExecutor(requireContext())\n)\n}\n```\n**Annotations**:\n- `cameraExecutor`: A dedicated executor ensuring that camera operations run on a separate background thread, avoiding any UI slowdowns.\n- `cameraProviderFuture`: An asynchronous task to fetch an instance of `ProcessCameraProvider`. This instance helps control and manage camera devices.\n- `cameraSelector`: Utilized to specify preferences for camera initialization. Here, we select the device's default back camera."
    ],
    "reference": "The cameraExecutor is a dedicated executor that ensures camera operations run on a separate background thread, which helps avoid any UI slowdowns during camera operations.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\other\\WorkingWithTheCamera.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:33:06.166795+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What the Preview class do and how it help in showing camera feed to users in real time?",
    "reference_contexts": [
      "**Objective**: Offer a real-time view of the camera feed to users, enabling them to see what the camera lens captures.  \n**Class Utilized**: `Preview` - A class designed to handle and showcase the real-time camera feed."
    ],
    "reference": "The Preview class is designed to handle and showcase the real-time camera feed, offering users a view of what the camera lens captures.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\other\\WorkingWithTheCamera.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:33:06.166795+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How does AspectRatio.RATIO_4_3 affect the camera preview in Android applications?",
    "reference_contexts": [
      "```xml\n<androidx.constraintlayout.widget.ConstraintLayout>\n...\n<androidx.camera.view.PreviewView\nandroid:id=\"@+id/viewFinder\"\nandroid:layout_width=\"match_parent\"\nandroid:layout_height=\"match_parent\" />\n...\n</androidx.constraintlayout.widget.ConstraintLayout>\n```  \n```kotlin\nprivate fun setPreview() {\npreview = Preview.Builder()\n.setTargetAspectRatio(AspectRatio.RATIO_4_3)\n.setTargetRotation(binding.viewFinder.display.rotation)\n.build()\n}\n```  \n**Annotations**:\n- Declare the component that will output the preview image, such as \"PreviewView\".\n- The `Preview.Builder` facilitates the configuration of the live feed properties.\n- `setTargetAspectRatio(AspectRatio.RATIO_4_3)`: Configures the aspect ratio of the camera preview. Here, a 4:3 ratio is selected.\n- `setTargetRotation()`: Aligns the camera feed's rotation with the device's current display rotation, ensuring the feed orientation is consistent with user expectations."
    ],
    "reference": "AspectRatio.RATIO_4_3 is used in the `Preview.Builder` to configure the aspect ratio of the camera preview, specifically selecting a 4:3 ratio for the output image.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\other\\WorkingWithTheCamera.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:33:06.166795+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How does the synchronization of the camera's operations with the Fragment's lifecycle affect CPU usage?",
    "reference_contexts": [
      "**Objective**: To harmonize the camera's operations with the Fragment's lifecycle. Such synchronization guarantees the camera's optimal performance, ensuring that it doesn't run unnecessarily, conserving both CPU cycles and battery life.  \n**Methods Utilized**:\n- `unbindAll()`: Clears any previous bindings, ensuring a fresh slate for binding. This step is crucial to prevent potential conflicts.\n- `bindToLifecycle()`: Merges the camera's lifecycle with the fragment's lifecycle."
    ],
    "reference": "The synchronization of the camera's operations with the Fragment's lifecycle ensures the camera's optimal performance, preventing it from running unnecessarily and thereby conserving both CPU cycles and battery life.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\other\\WorkingWithTheCamera.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:33:06.166795+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What ContextCompat do in takePhoto function?",
    "reference_contexts": [
      "```kotlin\nprivate fun takePhoto() {\nval imageCapture = imageCapture ?: return\n\n...\n\nval contentValues = ContentValues().apply {\nput(MediaStore.MediaColumns.DISPLAY_NAME, name)\nput(MediaStore.MediaColumns.MIME_TYPE, \"image/jpeg\")\nif(Build.VERSION.SDK_INT > Build.VERSION_CODES.P) {\nput(MediaStore.Images.Media.RELATIVE_PATH, \"Pictures/CameraX-Image\")\n}\n}\n\nval outputOptions = ImageCapture.OutputFileOptions\n.Builder(contentResolver, MediaStore.Images.Media.EXTERNAL_CONTENT_URI, contentValues)\n.build()\n\nimageCapture.takePicture(\noutputOptions,\nContextCompat.getMainExecutor(this),\nobject : ImageCapture.OnImageSavedCallback {\noverride fun onImageSaved(output: ImageCapture.OutputFileResults){\n<Success Capture to run for your code>\n}\noverride fun onError(exc: ImageCaptureException) {\n<Fail Capture to run for your code>\n}\n}\n)\n}\n```\n**Annotations**:\n- This allows you to capture an image and save it to your device.\n- `outputOptions`: You can specify how you want the output to appear. Additionally, you can set the save name, MIMETYPE, and more in `ContentValues`.\n- When the image capture function finishes executing, \"OnImageSavedCallback\" is executed based on the result.\n- `onImageSaved()`: Function executed on successful capture.\n- `onError()`: Function executed on failed capture."
    ],
    "reference": "ContextCompat.getMainExecutor(this) is used to get the main executor for the image capture process in the takePhoto function.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\other\\WorkingWithTheCamera.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:33:06.166795+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How does the camera binding process integrate with ConstraintLayout in Android development?",
    "reference_contexts": [
      "<1-hop>\n\n```kotlin\ncameraProviderFuture.addListener(\n{\n...\ntry {\ncameraProvider.unbindAll()\ncamera = cameraProvider.bindToLifecycle(this, cameraSelector, preview)\npreview?.setSurfaceProvider(binding.viewFinder.surfaceProvider)\n} catch (exc: java.lang.Exception) {\nLog.e(TAG, \"Camera binding failed\", exc)\n}\n},\nContextCompat.getMainExecutor(requireContext())\n)\n```\n**Annotations**:\n- This binding approach ensures that the camera activates or deactivates in conjunction with the Fragment's lifecycle events. For instance, the camera initiates when the Fragment starts and ceases when the Fragment stops.",
      "<2-hop>\n\n```xml\n<androidx.constraintlayout.widget.ConstraintLayout>\n...\n<androidx.camera.view.PreviewView\nandroid:id=\"@+id/viewFinder\"\nandroid:layout_width=\"match_parent\"\nandroid:layout_height=\"match_parent\" />\n...\n</androidx.constraintlayout.widget.ConstraintLayout>\n```  \n```kotlin\nprivate fun setPreview() {\npreview = Preview.Builder()\n.setTargetAspectRatio(AspectRatio.RATIO_4_3)\n.setTargetRotation(binding.viewFinder.display.rotation)\n.build()\n}\n```  \n**Annotations**:\n- Declare the component that will output the preview image, such as \"PreviewView\".\n- The `Preview.Builder` facilitates the configuration of the live feed properties.\n- `setTargetAspectRatio(AspectRatio.RATIO_4_3)`: Configures the aspect ratio of the camera preview. Here, a 4:3 ratio is selected.\n- `setTargetRotation()`: Aligns the camera feed's rotation with the device's current display rotation, ensuring the feed orientation is consistent with user expectations."
    ],
    "reference": "The camera binding process integrates with ConstraintLayout by using the `PreviewView` component within the layout to display the camera feed. The camera is bound to the lifecycle of the Fragment, ensuring it activates when the Fragment starts and deactivates when it stops. The `Preview.Builder` is used to configure the camera preview properties, such as aspect ratio and rotation, which are essential for optimizing the camera functionalities within the ConstraintLayout.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\other\\WorkingWithTheCamera.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:33:06.166795+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the purpose of the PreviewView in the Android application, and how does the Preview class contribute to achieving this objective?",
    "reference_contexts": [
      "<1-hop>\n\n```xml\n<androidx.constraintlayout.widget.ConstraintLayout>\n...\n<androidx.camera.view.PreviewView\nandroid:id=\"@+id/viewFinder\"\nandroid:layout_width=\"match_parent\"\nandroid:layout_height=\"match_parent\" />\n...\n</androidx.constraintlayout.widget.ConstraintLayout>\n```  \n```kotlin\nprivate fun setPreview() {\npreview = Preview.Builder()\n.setTargetAspectRatio(AspectRatio.RATIO_4_3)\n.setTargetRotation(binding.viewFinder.display.rotation)\n.build()\n}\n```  \n**Annotations**:\n- Declare the component that will output the preview image, such as \"PreviewView\".\n- The `Preview.Builder` facilitates the configuration of the live feed properties.\n- `setTargetAspectRatio(AspectRatio.RATIO_4_3)`: Configures the aspect ratio of the camera preview. Here, a 4:3 ratio is selected.\n- `setTargetRotation()`: Aligns the camera feed's rotation with the device's current display rotation, ensuring the feed orientation is consistent with user expectations.",
      "<2-hop>\n\n**Objective**: Offer a real-time view of the camera feed to users, enabling them to see what the camera lens captures.  \n**Class Utilized**: `Preview` - A class designed to handle and showcase the real-time camera feed."
    ],
    "reference": "The purpose of the PreviewView in the Android application is to offer a real-time view of the camera feed to users, enabling them to see what the camera lens captures. The Preview class contributes to this objective by handling and showcasing the real-time camera feed, allowing developers to configure properties such as aspect ratio and rotation to enhance the user experience.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\other\\WorkingWithTheCamera.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:33:06.166795+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How use Exynos Neural Network Software Development Kit?",
    "reference_contexts": [
      "This guide provides basic instructions for using Exynos Neural Network Software Development Kit (Exynos AI Studio).\nThis guide explains the method to convert Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also describes the execution of NNC models on Exynos devices."
    ],
    "reference": "This guide provides basic instructions for using Exynos Neural Network Software Development Kit (Exynos AI Studio), explaining how to convert Neural Network (NN) models to Neural Network Container (NNC) models and execute NNC models on Exynos devices.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:33:30.882472+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What are ENNTools used for in Exynos AI Studio?",
    "reference_contexts": [
      "[Exynos AI Studio](https://soc-developer.semiconductor.samsung.com/enn-sdk) allows users to convert the trained [TensorFlow Lite](https://www.tensorflow.org/lite) neural network models to a format that can run efficiently in [Samsung Exynos](https://semiconductor.samsung.com/processor/) hardware.\nExynos AI Studio contains ENNTools to convert trained NN models and ENN framework for executing converted models on Exynos platforms.  \nThis guide covers the basics of using [Exynos AI Studio service](https://soc-developer.semiconductor.samsung.com/enn-sdk/project/) and executing NN models with ENN framework."
    ],
    "reference": "ENNTools are used to convert trained neural network models to a format that can run efficiently on Samsung Exynos hardware.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:33:30.882472+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How ENN Framework help in executing NN model?",
    "reference_contexts": [
      "Following figure illustrates the three steps for converting and executing an NN model:  \n```mermaid\nflowchart LR\nsubgraph \"Exynos AI Studio Service\"\ndirection LR\nconvert(\"Convert The Model\")\nend\nsubgraph \"ENN Framework\"\ndirection LR\nexecute(\"Execute The Model\")\nend\nmodel(\"Prepare Trained Model<br>(TFLite)\")-->convert-->execute\n```"
    ],
    "reference": "ENN Framework is involved in the step of executing the model after it has been converted from the trained model (TFLite).",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:33:30.882472+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How can I convert TFLite models to NNC models using Exynos AI Studio?",
    "reference_contexts": [
      "To convert TensorFlow Lite models, Exynos AI Studio provides an online conversion tool through the Samsung Exynos Eco-system [Portal](https://soc-developer.semiconductor.samsung.com/enn-sdk).\nThis online conversion tool allows users to upload their TFLite models, convert them to NNC models, and download the NNC models to their devices.  \nFor more information on the process of converting NN models, refer to [Converting NN Models with Exynos AI Studio Service](#converting-nn-models-with-enn-sdk-service)."
    ],
    "reference": "To convert TensorFlow Lite models, Exynos AI Studio provides an online conversion tool that allows users to upload their TFLite models, convert them to NNC models, and download the NNC models to their devices.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:33:30.882472+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How can I utilize the ENN frmaework to execute NNC models on Exynos platforms, and what are the necessary steps involved?",
    "reference_contexts": [
      "To execute NNC models on Exynos platforms, users must implement a program with ENN framework.\nENN framework provides C++ APIs for utilizing the framework that accelerate graph-based NN applications using NPU/DSP.\nThe Exynos AI Studio provides only C++ APIs.\nTherefore, the user must implement the Java Native Interface (JNI) layer to use the ENN framework on Android applications.  \nFor more information on the process of executing NN models, refer to [Executing Models Using Native Program](#executing-models-using-native-program) and [Executing Models Using Android Application](#executing-models-using-android-application)."
    ],
    "reference": "To execute NNC models on Exynos platforms, users must implement a program with the ENN framework. The ENN framework provides C++ APIs for utilizing the framework that accelerates graph-based NN applications using NPU/DSP. Since the Exynos AI Studio provides only C++ APIs, the user must implement the Java Native Interface (JNI) layer to use the ENN framework on Android applications.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:33:30.882472+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How to convert MobileNet V1 model from TensorFlow Hub?",
    "reference_contexts": [
      "In this example, let us consider converting a trained TFLite MobileNet V1 model from TensorFlow Hub.  \nTo get started, you must be a member of the ENN Eco-system [Portal](https://soc-developer.semiconductor.samsung.com/).\n- If you are not a member of the ENN Eco-system Portal, sign up from [here](https://soc-developer.semiconductor.samsung.com/register).\n- If you already have an account, log in to the ENN Eco-system Portal.  \nTo convert MobileNet V1 model:\n1. Download `lite-model_mobilenet_v1_100_224_uint8_1.tflite` from [here](https://tfhub.dev/iree/lite-model/mobilenet_v1_100_224/uint8/1).\n1. Navigate to the Exynos AI Studio Service [page](https://soc-developer.semiconductor.samsung.com/enn-sdk/project) and provide a title for your project.\n1. Then, upload the downloaded TFLite model\n1. Next, select hardware preferences.\n- The **Default** option creates a model that utilizes only the CPU and GPU for conversion.\n- The **Accelerate** option creates a model that utilizes NPU as an accelerator with CPU and GPU.\n1. After confirming your selections, click **Convert** to convert the model.\n1. After the compilation process is successfully completed, the **NNC Download** button is enabled.\n1. Click **NNC Download** to download the converted NNC model.\nYou can now integrate the NNC model into the desired application."
    ],
    "reference": "To convert MobileNet V1 model from TensorFlow Hub, first, you need to be a member of the ENN Eco-system Portal. If you are not a member, sign up from the provided link. If you already have an account, log in. Then, download `lite-model_mobilenet_v1_100_224_uint8_1.tflite` from the specified link. Navigate to the Exynos AI Studio Service page and provide a title for your project. Upload the downloaded TFLite model and select hardware preferences. The Default option uses only CPU and GPU, while the Accelerate option uses NPU as an accelerator with CPU and GPU. After confirming your selections, click Convert. Once the compilation process is completed, the NNC Download button will be enabled. Click NNC Download to download the converted NNC model, which you can then integrate into your application.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:33:30.882472+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Whaat is the role of Github in downloading the sample NNC model for embedded systems development?",
    "reference_contexts": [
      "For this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the [Github](https://github.com/exynos-eco/enn-sdk-samples-9925).  \n1. Download the samples and navigate to the directory that contain the sample program.\n```shell\ngit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\ncd enn-sdk-samples-9925/nnc-model-tester\n```  \n1. Push the necessary files (native program and test files) using adb push.\n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/* /data/local/tmp/\n```  \n1. When nnc_model_tester is built from Windows, execute permission must be provided.\n```shell\nadb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n```  \n1. Execute native binary on ERD board using adb shell.\n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```  \n1. When the command is successful, the following message is displayed:\n```shell\nLoaded Model:\nmodel.nnc(00000B7E01000000)\nModel Execution Time (1): 5413 microseconds\nAvg. Model Execution Time: 5413 microseconds\nOutput Layer(0): Golden Match\n-       snr value:104.802\nENN Framework Execute Model Sucess\n```  \nFor more information on this sample program, refer to [Exynos AI Studio Samples](enn-sdk-samples).\nFor more information on writing native programs using ENN framework, refer to [Getting Started With Native Samples](getting-started-with-native-samples)."
    ],
    "reference": "Github is used to host the sample program for the NNC model, which can be downloaded by cloning the repository. The specific command to clone the repository is `git clone https://github.com/exynos-eco/enn-sdk-samples-9925.git`, allowing developers to access the necessary files for implementing and testing neural network models on embedded devices.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:33:30.882472+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How can I execute an Android application using Android Studio on the ERD board?",
    "reference_contexts": [
      "For this example, we will execute an Android application using Android Studio on the ERD board.\nThis example allows users to execute an image classification Android application.  \nThe source code for the example is available at `enn-sdk-samples-9925/image-classification` within the [Github](https://github.com/exynos-eco/enn-sdk-samples-9925).\n1. Download the samples by cloning the Github repository\n```shell\ngit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\n```\n2. Open the downloaded **image-classification** project in Android Studio.\n3. Connect the ERD board and click \"run 'app'\".\nThe application is launched on the ERD board after the build.  \nFor more information, refer to [Getting Started With Android Samples](getting-started-with-android-samples)."
    ],
    "reference": "To execute an Android application using Android Studio on the ERD board, first download the samples by cloning the Github repository with the command `git clone https://github.com/exynos-eco/enn-sdk-samples-9925.git`. Then, open the downloaded **image-classification** project in Android Studio. After connecting the ERD board, click 'run 'app'' to launch the application on the ERD board after the build.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:33:30.882472+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How do you convert TensorFlow Lite models and execute them on Exynos platforms using the ENN framework?",
    "reference_contexts": [
      "<1-hop>\n\nTo convert TensorFlow Lite models, Exynos AI Studio provides an online conversion tool through the Samsung Exynos Eco-system [Portal](https://soc-developer.semiconductor.samsung.com/enn-sdk).\nThis online conversion tool allows users to upload their TFLite models, convert them to NNC models, and download the NNC models to their devices.  \nFor more information on the process of converting NN models, refer to [Converting NN Models with Exynos AI Studio Service](#converting-nn-models-with-enn-sdk-service).",
      "<2-hop>\n\nTo execute NNC models on Exynos platforms, users must implement a program with ENN framework.\nENN framework provides C++ APIs for utilizing the framework that accelerate graph-based NN applications using NPU/DSP.\nThe Exynos AI Studio provides only C++ APIs.\nTherefore, the user must implement the Java Native Interface (JNI) layer to use the ENN framework on Android applications.  \nFor more information on the process of executing NN models, refer to [Executing Models Using Native Program](#executing-models-using-native-program) and [Executing Models Using Android Application](#executing-models-using-android-application)."
    ],
    "reference": "To convert TensorFlow Lite models, Exynos AI Studio provides an online conversion tool that allows users to upload their TFLite models, convert them to NNC models, and download the NNC models to their devices. To execute these NNC models on Exynos platforms, users must implement a program using the ENN framework, which provides C++ APIs for accelerating graph-based neural network applications using NPU/DSP. Additionally, to use the ENN framework on Android applications, users need to implement the Java Native Interface (JNI) layer.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:33:30.882472+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How do you execute a sample NNC model on the ERD board using the ENN framework, and what is the role of JNI in this process?",
    "reference_contexts": [
      "<1-hop>\n\nFor this example, we will execute a sample NNC model using the native program on the ERD board.\nThe sample program is available at `enn-sdk-samples-9925/nnc-model-tester` within the [Github](https://github.com/exynos-eco/enn-sdk-samples-9925).  \n1. Download the samples and navigate to the directory that contain the sample program.\n```shell\ngit clone https://github.com/exynos-eco/enn-sdk-samples-9925.git\ncd enn-sdk-samples-9925/nnc-model-tester\n```  \n1. Push the necessary files (native program and test files) using adb push.\n```shell\nadb push libs/arm64-v8a/enn_nnc_model_tester /data/local/tmp/\nadb push libs/arm64-v8a/libenn_public_api_ndk_v1.so /data/local/tmp/\nadb push example/* /data/local/tmp/\n```  \n1. When nnc_model_tester is built from Windows, execute permission must be provided.\n```shell\nadb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n```  \n1. Execute native binary on ERD board using adb shell.\n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```  \n1. When the command is successful, the following message is displayed:\n```shell\nLoaded Model:\nmodel.nnc(00000B7E01000000)\nModel Execution Time (1): 5413 microseconds\nAvg. Model Execution Time: 5413 microseconds\nOutput Layer(0): Golden Match\n-       snr value:104.802\nENN Framework Execute Model Sucess\n```  \nFor more information on this sample program, refer to [Exynos AI Studio Samples](enn-sdk-samples).\nFor more information on writing native programs using ENN framework, refer to [Getting Started With Native Samples](getting-started-with-native-samples).",
      "<2-hop>\n\nTo execute NNC models on Exynos platforms, users must implement a program with ENN framework.\nENN framework provides C++ APIs for utilizing the framework that accelerate graph-based NN applications using NPU/DSP.\nThe Exynos AI Studio provides only C++ APIs.\nTherefore, the user must implement the Java Native Interface (JNI) layer to use the ENN framework on Android applications.  \nFor more information on the process of executing NN models, refer to [Executing Models Using Native Program](#executing-models-using-native-program) and [Executing Models Using Android Application](#executing-models-using-android-application)."
    ],
    "reference": "To execute a sample NNC model on the ERD board using the ENN framework, you first need to download the sample program from the GitHub repository and navigate to the directory containing the sample program. You can do this by cloning the repository and changing to the appropriate directory. After that, you push the necessary files to the device using adb push commands. Once the nnc_model_tester is built from Windows, you must provide execute permission before running the native binary on the ERD board. When executed successfully, the model will display execution time and output layer results. Additionally, to utilize the ENN framework on Android applications, users must implement a Java Native Interface (JNI) layer, as the ENN framework only provides C++ APIs for accelerating graph-based NN applications on Exynos platforms.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\quick-start-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:33:30.882472+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is the NNC model tester in the context of Android applications?",
    "reference_contexts": [
      "|Sample Name|Description|\n|-------------|-------|\n|[Image Classification In Android](#image-classification-in-android)| Sample Android application to demonstrate the execution of `Inception v4` model with Exynos AI Studio|\n|[Object Detection In Android](#object-detection-in-android)| Sample Android application to demonstrate the execution of `YOLOv5` model with Exynos AI Studio|\n|[Segmentation In Android](#segmentation-in-android)| Sample Android application to demonstrate the execution of `DeeplabV3` model with Exynos AI Studio|\n|[Pose Estimation In Android](#pose-estimation-in-android)| Sample Android application to demonstrate the execution of `PoseNet` model with Exynos AI Studio|\n|[Image Enhance In Android](#image-enhance-in-android)| Sample Android application to demonstrate the execution of `Zero-DCE` model with Exynos AI Studio|\n|[Depth Estimation In Andriod](#depth-estimation-in-andriod)| Sample Android application to demonstrate the execution of `MiDaS v2` model with Exynos AI Studio|\n|[Performance Comparison](#performance-comparison)| Sample Android application to demonstrate the difference between Exynos AI Studio and TFLite |\n|[NNC Model Tester](#nnc-model-tester)|Sample C++ program to demonstrate the execution of NNC model with Exynos AI Studio|"
    ],
    "reference": "The NNC model tester is a sample C++ program designed to demonstrate the execution of the NNC model with Exynos AI Studio.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:34:07.550210+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What is Android sample applications about?",
    "reference_contexts": [
      "This section provides an overview of Android (Kotlin) sample applications.\nEach sample application entry provides the details of the functionality of the sample application, its location, and instructions for running it.\nFor more information on implementing the sample applications, refer to [Getting Started With Android Samples](getting-started-with-android-samples) guide.  \n***"
    ],
    "reference": "This section gives overview of Android (Kotlin) sample applications, detailing functionality, location, and instructions for running it.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:34:07.550210+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How does Exynos AI Studio contribute to the execution of AI models in Android applications?",
    "reference_contexts": [
      "This sample application demonstrates the execution of a converted [Inception v4](https://tfhub.dev/tensorflow/lite-model/inception_v4_quant/1/default/1) model using the ENN framework.\nThe model is converted using Exynos AI Studio service with the **Accelerate** hardware type option."
    ],
    "reference": "Exynos AI Studio contributes to the execution of AI models in Android applications by providing a service that allows for the conversion of models, such as the Inception v4 model, using the ENN framework. The conversion process can utilize the **Accelerate** hardware type option.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:34:07.550210+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Where can I find the sample for image-classification in the enn-sdk-samples-9925 repository?",
    "reference_contexts": [
      "The sample is available in the `enn-sdk-samples-9925/image-classification` directory within the [Github](https://github.com/exynos-eco/enn-sdk-samples-9925) repository."
    ],
    "reference": "The sample for image-classification is available in the `enn-sdk-samples-9925/image-classification` directory within the Github repository.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:34:07.550210+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How do you modify the model parameters in ModelConstants.kt for the sample application?",
    "reference_contexts": [
      "To utilize the sample application:\n1.Download or clone the sample application from the Github repository.\n2.Open the sample application project in Android Studio.\n3.Connect the ERD board to the computer.\n4.Run the application (using Shift + F10).\n5.Select Camera or Image mode and provide the data for inference.  \nTo modify the model used in the sample application:\n1.Copy the desired model file to the `assets` directory of the project.\n2.Copy the corresponding label text file to the `assets` directory.\n3.Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4.If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.  \n***"
    ],
    "reference": "To modify the model used in the sample application, you need to change the parameters in the ModelConstants.kt file to reflect the specifications of the new model after copying the desired model file and the corresponding label text file to the `assets` directory.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:34:07.550210+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How is YOLOv5 utilized in the sample application?",
    "reference_contexts": [
      "This sample application demonstrates the execution of a converted [YOLOv5](https://github.com/ultralytics/yolov5) model using the ENN framework.\nThe model is converted using Exynos AI Studio service with the **Default** hardware type option."
    ],
    "reference": "The sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework, with the model converted using Exynos AI Studio service and the Default hardware type option.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:34:07.550210+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Where can I find the sample for object detection on Github?",
    "reference_contexts": [
      "The sample is available in the `enn-sdk-samples-9925/object-detection` directory within the [Github](https://github.com/exynos-eco/enn-sdk-samples-9925) repository."
    ],
    "reference": "The sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:34:07.550210+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "How do I connect the ERD bord to my computer?",
    "reference_contexts": [
      "To utilize the sample application:\n1.Download or clone the sample application from the Github repository.\n2.Open the sample application project in Android Studio.\n3.Connect the ERD board to the computer.\n4.Run the application (using Shift + F10).\n5.Select Camera or Image mode and provide the data for inference.  \nTo modify the model used in the sample application:\n1.Copy the desired model file to the `assets` directory of the project.\n2.Copy the corresponding label text file to the `assets` directory.\n3.Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4.If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.  \n***"
    ],
    "reference": "To connect the ERD board to the computer, simply follow step 3 in the instructions: Connect the ERD board to the computer.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:34:07.550210+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What are the steps to utilize the sample application from the Github repository, and how can one modify the model used in this application?",
    "reference_contexts": [
      "<1-hop>\n\nTo utilize the sample application:\n1.Download or clone the sample application from the Github repository.\n2.Open the sample application project in Android Studio.\n3.Connect the ERD board to the computer.\n4.Run the application (using Shift + F10).\n5.Select Camera or Image mode and provide the data for inference.  \nTo modify the model used in the sample application:\n1.Copy the desired model file to the `assets` directory of the project.\n2.Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3.If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.  \n***",
      "<2-hop>\n\nTo utilize the sample application:\n1.Download or clone the sample application from the Github repository.\n2.Open the sample application project in Android Studio.\n3.Connect the ERD board to the computer.\n4.Run the application (using Shift + F10).\n5.Select Camera or Image mode and provide the data for inference.  \nTo modify the model used in the sample application:\n1.Copy the desired model file to the `assets` directory within the project.\n2.Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3.If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.  \n***"
    ],
    "reference": "To utilize the sample application from the Github repository, follow these steps: 1. Download or clone the sample application from the Github repository. 2. Open the sample application project in Android Studio. 3. Connect the ERD board to the computer. 4. Run the application (using Shift + F10). 5. Select Camera or Image mode and provide the data for inference. To modify the model used in the sample application, you should: 1. Copy the desired model file to the `assets` directory of the project. 2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model. 3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
    "synthesizer_name": "multi_hop_abstract_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:34:07.550210+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "What steps must be taken to modify the model used in the sample application, specifically regarding the changes needed in the ModelConstants.kt file?",
    "reference_contexts": [
      "<1-hop>\n\nTo utilize the sample application:\n1.Download or clone the sample application from the Github repository.\n2.Open the sample application project in Android Studio.\n3.Connect the ERD board to the computer.\n4.Run the application (using Shift + F10).\n5.Select Camera or Image mode and provide the data for inference.  \nTo modify the model used in the sample application:\n1.Copy the desired model file to the `assets` directory of the project.\n2.Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3.If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.  \n***",
      "<2-hop>\n\nTo utilize the sample application:\n1.Download or clone the sample application from the Github repository.\n2.Open the sample application project in Android Studio.\n3.Connect the ERD board to the computer.\n4.Run the application (using Shift + F10).\n5.Select Camera or Image mode and provide the data for inference.  \nTo modify the model used in the sample application:\n1.Copy the desired model file to the `assets` directory of the project.\n2.Copy the corresponding label text file to the `assets` directory.\n3.Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4.If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.  \n***"
    ],
    "reference": "To modify the model used in the sample application, you need to follow these steps: First, copy the desired model file to the `assets` directory of the project. Then, modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model. Additionally, if the inputs and outputs of the model differ from the pre-designed sample application, you must also modify the `preProcess()` and `postProcess()` functions accordingly.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\documentation_MX\\sample-support-guide\\README.md",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:34:07.550210+09:00",
    "chatbot_server": "https://prd.ai-studio-farm.com/global/chatbot"
  },
  {
    "user_input": "Wht is the purpose of the AI Chllenger program by Samsung S.LSI?",
    "reference_contexts": [
      "The AI Challenger is a developer program by Samsung S.LSI that focuses on using and developing AI technology through the Exynos AI Studio. The AI Challenger aims to drive technological innovation in AI and provides a versatile toolkit for creating and optimizing AI models. The AI Challenger program is structured into three stages: familiarization with tools, active research and development, and finalizing and presenting projects. The AI Challenger targets Korean students and postgraduates, aiming to enhance their AI expertise and competitiveness in the industry. The AI Challenger promotes Edge AI technology advancement, industry-academia collaboration, and the development of innovative prototypes. The AI Challenger is also designed to address real-world technological challenges and stimulate the growth of the AI sector. The AI Challenger 1 includes a chipset called the Exynos 2200, which is integral to the AI development tools offered. The AI Challenger 2 includes a chipset called the Exynos 2400, which is integral to the AI development tools offered."
    ],
    "reference": "The AI Challenger is a developer program by Samsung S.LSI that focuses on using and developing AI technology through the Exynos AI Studio. It aims to drive technological innovation in AI and provides a versatile toolkit for creating and optimizing AI models. The program is structured into three stages: familiarization with tools, active research and development, and finalizing and presenting projects. It targets Korean students and postgraduates to enhance their AI expertise and competitiveness in the industry, promoting Edge AI technology advancement, industry-academia collaboration, and the development of innovative prototypes.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\testset_2503",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:47:00.640064+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "Wht is the AI Chllenger prgram by Samsung?",
    "reference_contexts": [
      "The AI Challenger is a developer program by Samsung S.LSI that focuses on using and developing AI technology through the Exynos AI Studio. The AI Challenger aims to drive technological innovation in AI and provides a versatile toolkit for creating and optimizing AI models. The AI Challenger program is structured into three stages: familiarization with tools, active research and development, and finalizing and presenting projects. The AI Challenger targets Korean students and postgraduates, aiming to enhance their AI expertise and competitiveness in the industry. The AI Challenger promotes Edge AI technology advancement, industry-academia collaboration, and the development of innovative prototypes. The AI Challenger is also designed to address real-world technological challenges and stimulate the growth of the AI sector. The AI Challenger 1 includes a chipset called the Exynos 2200, which is integral to the AI development tools offered. The AI Challenger 2 includes a chipset called the Exynos 2400, which is integral to the AI development tools offered."
    ],
    "reference": "The AI Challenger is a developer program by Samsung S.LSI that focuses on using and developing AI technology through the Exynos AI Studio. It aims to drive technological innovation in AI and provides a versatile toolkit for creating and optimizing AI models.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\testset_2503",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:47:00.640064+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the main objectives of the AI Challenger program by Samsung S.LSI?",
    "reference_contexts": [
      "The AI Challenger is a developer program by Samsung S.LSI that focuses on using and developing AI technology through the Exynos AI Studio. The AI Challenger aims to drive technological innovation in AI and provides a versatile toolkit for creating and optimizing AI models. The AI Challenger program is structured into three stages: familiarization with tools, active research and development, and finalizing and presenting projects. The AI Challenger targets Korean students and postgraduates, aiming to enhance their AI expertise and competitiveness in the industry. The AI Challenger promotes Edge AI technology advancement, industry-academia collaboration, and the development of innovative prototypes. The AI Challenger is also designed to address real-world technological challenges and stimulate the growth of the AI sector. The AI Challenger 1 includes a chipset called the Exynos 2200, which is integral to the AI development tools offered. The AI Challenger 2 includes a chipset called the Exynos 2400, which is integral to the AI development tools offered."
    ],
    "reference": "The AI Challenger program by Samsung S.LSI aims to drive technological innovation in AI, enhance the AI expertise and competitiveness of Korean students and postgraduates, promote Edge AI technology advancement, foster industry-academia collaboration, and develop innovative prototypes. It is structured into three stages: familiarization with tools, active research and development, and finalizing and presenting projects, while also addressing real-world technological challenges and stimulating the growth of the AI sector.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\testset_2503",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:47:00.640064+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the key features of the Exynos 2400 chipset?",
    "reference_contexts": [
      "The Exynos 2200 chipset features a CPU configuration with Cortex®-X2, Cortex®-A710, and Cortex®-A510, and includes the Samsung Xclipse 920 GPU. The Exynos 2200 chipset is equipped with an AI Engine that has a Dual-core NPU and DSP. Camera capabilities of the Exynos 2200 support up to 200MP in single camera mode and dual-camera setups of 64MP + 32MP at 30fps. Video performance of the Exynos 2200 chipset includes up to 8K decoding at 60fps and 8K encoding at 30fps, supporting 10-bit HEVC(H.265) and VP9. The display capabilities of the Exynos 2200 chipset include supporting 4K/WQUXGA resolutions at 120Hz and QHD+ at 144Hz. The Exynos 2200 chipset aims to enhance mobile gaming with features like hardware-accelerated ray tracing and variable rate shading. Smartphones utilizing the Exynos 2200 chipset include the Samsung Galaxy S22 Ultra, S22+, and S22. The Exynos 2200 chipset is built using a 4-nanometer EUV process, incorporates Arm's latest Armv9 CPU cores, and includes a fast 5G modem compliant with 3GPP Release 16. The Exynos 2200 chipset offers advanced security features with an integrated Secure Element and robust encryption. The Exynos 2200 chipset's ISP supports resolutions up to 200MP and features AI-integrated camera functionalities for professional-quality images. There is no native library support for the Fourier Transform in the Exynos 2200 chipset; third-party libraries or manual implementation are recommended. The Exynos 2400 chipset includes a CPU configuration of Cortex®-X4, Cortex®-A720, and Cortex®-A520, paired with the Samsung Xclipse 940 GPU. The Exynos 2400 chipset features an AI Engine equipped with a 17K MAC NPU (2-GNPU+2-SNPU) and DSP. The Exynos 2400 chipset's camera capabilities support up to 320MP in single camera mode and a dual-camera setup of 64MP + 32MP at 30fps. The Exynos 2400 chipset's video performance includes up to 8K decoding at 60fps and encoding at 30fps for 10-bit HEVC(H.265) and VP9. The Exynos 2400 chipset's display support extends to 4K/WQUXGA at 120Hz and QHD+ at 144Hz. The Exynos 2400 aims to transform mobile gaming with console-quality graphics and advanced GPU technology. Smartphones utilizing the Exynos 2400 chipset include the Samsung Galaxy S24+ and Samsung Galaxy S24. The Exynos 2400 chipset offers a 14.7x improvement in AI performance over its predecessor, enhancing efficiency and reducing latency while boosting device security. The Exynos 2400 chipset includes a 5G NR modem supporting up to 9.6Gbps downlink speeds in FR1 and 12.1Gbps in FR2, compliant with 3GPP’s Rel.17 standard. The Exynos 2400 chipset also supports non-terrestrial network (NTN) satellite connections and has an integrated GNSS block for accurate positioning, enhancing connectivity even in cellular dead zones. The AI Challenger is a developer program by Samsung S.LSI that focuses on using and developing AI technology through the Exynos AI Studio. The AI Challenger aims to drive technological innovation in AI and provides a versatile toolkit for creating and optimizing AI models. The AI Challenger program is structured into three stages: familiarization with tools, active research and development, and finalizing and presenting projects. The AI Challenger targets Korean students and postgraduates, aiming to enhance their AI expertise and competitiveness in the industry. The AI Challenger promotes Edge AI technology advancement, industry-academia collaboration, and the development of innovative prototypes. The AI Challenger is also designed to address real-world technological challenges and stimulate the growth of the AI sector. The AI Challenger 1 includes a chipset called the Exynos 2200, which is integral to the AI development tools offered. The AI Challenger 2 includes a chipset called the Exynos 2400, which is integral to the AI development tools offered. The"
    ],
    "reference": "The Exynos 2400 chipset includes a CPU configuration of Cortex®-X4, Cortex®-A720, and Cortex®-A520, paired with the Samsung Xclipse 940 GPU. It features an AI Engine equipped with a 17K MAC NPU (2-GNPU+2-SNPU) and DSP, supporting camera capabilities of up to 320MP in single camera mode and a dual-camera setup of 64MP + 32MP at 30fps. The video performance includes up to 8K decoding at 60fps and encoding at 30fps for 10-bit HEVC(H.265) and VP9. Display support extends to 4K/WQUXGA at 120Hz and QHD+ at 144Hz. The Exynos 2400 aims to transform mobile gaming with console-quality graphics and advanced GPU technology, offering a 14.7x improvement in AI performance over its predecessor, enhancing efficiency and reducing latency while boosting device security.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\testset_2503\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:53:10.804823+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the key features of the Exynos 2200 chipset?",
    "reference_contexts": [
      "The Exynos 2200 chipset features a CPU configuration with Cortex®-X2, Cortex®-A710, and Cortex®-A510, and includes the Samsung Xclipse 920 GPU. The Exynos 2200 chipset is equipped with an AI Engine that has a Dual-core NPU and DSP. Camera capabilities of the Exynos 2200 support up to 200MP in single camera mode and dual-camera setups of 64MP + 32MP at 30fps. Video performance of the Exynos 2200 chipset includes up to 8K decoding at 60fps and 8K encoding at 30fps, supporting 10-bit HEVC(H.265) and VP9. The display capabilities of the Exynos 2200 chipset include supporting 4K/WQUXGA resolutions at 120Hz and QHD+ at 144Hz. The Exynos 2200 chipset aims to enhance mobile gaming with features like hardware-accelerated ray tracing and variable rate shading. Smartphones utilizing the Exynos 2200 chipset include the Samsung Galaxy S22 Ultra, S22+, and S22. The Exynos 2200 chipset is built using a 4-nanometer EUV process, incorporates Arm's latest Armv9 CPU cores, and includes a fast 5G modem compliant with 3GPP Release 16. The Exynos 2200 chipset offers advanced security features with an integrated Secure Element and robust encryption. The Exynos 2200 chipset's ISP supports resolutions up to 200MP and features AI-integrated camera functionalities for professional-quality images. There is no native library support for the Fourier Transform in the Exynos 2200 chipset; third-party libraries or manual implementation are recommended. The Exynos 2400 chipset includes a CPU configuration of Cortex®-X4, Cortex®-A720, and Cortex®-A520, paired with the Samsung Xclipse 940 GPU. The Exynos 2400 chipset features an AI Engine equipped with a 17K MAC NPU (2-GNPU+2-SNPU) and DSP. The Exynos 2400 chipset's camera capabilities support up to 320MP in single camera mode and a dual-camera setup of 64MP + 32MP at 30fps. The Exynos 2400 chipset's video performance includes up to 8K decoding at 60fps and encoding at 30fps for 10-bit HEVC(H.265) and VP9. The Exynos 2400 chipset's display support extends to 4K/WQUXGA at 120Hz and QHD+ at 144Hz. The Exynos 2400 aims to transform mobile gaming with console-quality graphics and advanced GPU technology. Smartphones utilizing the Exynos 2400 chipset include the Samsung Galaxy S24+ and Samsung Galaxy S24. The Exynos 2400 chipset offers a 14.7x improvement in AI performance over its predecessor, enhancing efficiency and reducing latency while boosting device security. The Exynos 2400 chipset includes a 5G NR modem supporting up to 9.6Gbps downlink speeds in FR1 and 12.1Gbps in FR2, compliant with 3GPP’s Rel.17 standard. The Exynos 2400 chipset also supports non-terrestrial network (NTN) satellite connections and has an integrated GNSS block for accurate positioning, enhancing connectivity even in cellular dead zones. The AI Challenger is a developer program by Samsung S.LSI that focuses on using and developing AI technology through the Exynos AI Studio. The AI Challenger aims to drive technological innovation in AI and provides a versatile toolkit for creating and optimizing AI models. The AI Challenger program is structured into three stages: familiarization with tools, active research and development, and finalizing and presenting projects. The AI Challenger targets Korean students and postgraduates, aiming to enhance their AI expertise and competitiveness in the industry. The AI Challenger promotes Edge AI technology advancement, industry-academia collaboration, and the development of innovative prototypes. The AI Challenger is also designed to address real-world technological challenges and stimulate the growth of the AI sector. The AI Challenger 1 includes a chipset called the Exynos 2200, which is integral to the AI development tools offered. The AI Challenger 2 includes a chipset called the Exynos 2400, which is integral to the AI development tools offered. The"
    ],
    "reference": "The Exynos 2200 chipset features a CPU configuration with Cortex®-X2, Cortex®-A710, and Cortex®-A510, and includes the Samsung Xclipse 920 GPU. It is equipped with an AI Engine that has a Dual-core NPU and DSP. The camera capabilities support up to 200MP in single camera mode and dual-camera setups of 64MP + 32MP at 30fps. Video performance includes up to 8K decoding at 60fps and 8K encoding at 30fps, supporting 10-bit HEVC(H.265) and VP9. The display capabilities include supporting 4K/WQUXGA resolutions at 120Hz and QHD+ at 144Hz. It aims to enhance mobile gaming with features like hardware-accelerated ray tracing and variable rate shading. The chipset is built using a 4-nanometer EUV process and includes a fast 5G modem compliant with 3GPP Release 16.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\testset_2503\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:53:10.804823+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What is Cortex®-A710 in the context of Exynos 2200 chipset?",
    "reference_contexts": [
      "The Exynos 2200 chipset features a CPU configuration with Cortex®-X2, Cortex®-A710, and Cortex®-A510, and includes the Samsung Xclipse 920 GPU. The Exynos 2200 chipset is equipped with an AI Engine that has a Dual-core NPU and DSP. Camera capabilities of the Exynos 2200 support up to 200MP in single camera mode and dual-camera setups of 64MP + 32MP at 30fps. Video performance of the Exynos 2200 chipset includes up to 8K decoding at 60fps and 8K encoding at 30fps, supporting 10-bit HEVC(H.265) and VP9. The display capabilities of the Exynos 2200 chipset include supporting 4K/WQUXGA resolutions at 120Hz and QHD+ at 144Hz. The Exynos 2200 chipset aims to enhance mobile gaming with features like hardware-accelerated ray tracing and variable rate shading. Smartphones utilizing the Exynos 2200 chipset include the Samsung Galaxy S22 Ultra, S22+, and S22. The Exynos 2200 chipset is built using a 4-nanometer EUV process, incorporates Arm's latest Armv9 CPU cores, and includes a fast 5G modem compliant with 3GPP Release 16. The Exynos 2200 chipset offers advanced security features with an integrated Secure Element and robust encryption. The Exynos 2200 chipset's ISP supports resolutions up to 200MP and features AI-integrated camera functionalities for professional-quality images. There is no native library support for the Fourier Transform in the Exynos 2200 chipset; third-party libraries or manual implementation are recommended. The Exynos 2400 chipset includes a CPU configuration of Cortex®-X4, Cortex®-A720, and Cortex®-A520, paired with the Samsung Xclipse 940 GPU. The Exynos 2400 chipset features an AI Engine equipped with a 17K MAC NPU (2-GNPU+2-SNPU) and DSP. The Exynos 2400 chipset's camera capabilities support up to 320MP in single camera mode and a dual-camera setup of 64MP + 32MP at 30fps. The Exynos 2400 chipset's video performance includes up to 8K decoding at 60fps and encoding at 30fps for 10-bit HEVC(H.265) and VP9. The Exynos 2400 chipset's display support extends to 4K/WQUXGA at 120Hz and QHD+ at 144Hz. The Exynos 2400 aims to transform mobile gaming with console-quality graphics and advanced GPU technology. Smartphones utilizing the Exynos 2400 chipset include the Samsung Galaxy S24+ and Samsung Galaxy S24. The Exynos 2400 chipset offers a 14.7x improvement in AI performance over its predecessor, enhancing efficiency and reducing latency while boosting device security. The Exynos 2400 chipset includes a 5G NR modem supporting up to 9.6Gbps downlink speeds in FR1 and 12.1Gbps in FR2, compliant with 3GPP’s Rel.17 standard. The Exynos 2400 chipset also supports non-terrestrial network (NTN) satellite connections and has an integrated GNSS block for accurate positioning, enhancing connectivity even in cellular dead zones. The AI Challenger is a developer program by Samsung S.LSI that focuses on using and developing AI technology through the Exynos AI Studio. The AI Challenger aims to drive technological innovation in AI and provides a versatile toolkit for creating and optimizing AI models. The AI Challenger program is structured into three stages: familiarization with tools, active research and development, and finalizing and presenting projects. The AI Challenger targets Korean students and postgraduates, aiming to enhance their AI expertise and competitiveness in the industry. The AI Challenger promotes Edge AI technology advancement, industry-academia collaboration, and the development of innovative prototypes. The AI Challenger is also designed to address real-world technological challenges and stimulate the growth of the AI sector. The AI Challenger 1 includes a chipset called the Exynos 2200, which is integral to the AI development tools offered. The AI Challenger 2 includes a chipset called the Exynos 2400, which is integral to the AI development tools offered. The"
    ],
    "reference": "Cortex®-A710 is part of the CPU configuration in the Exynos 2200 chipset, which also includes Cortex®-X2 and Cortex®-A510.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\testset_2503\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:53:10.804823+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What Device Farm do?",
    "reference_contexts": [
      "Exynos Developer Society is managed by Samsung S. LSI and offers a range of tools and training resources for development on the Exynos platform. Best Lab is a platform within the Exynos Developer Society where administrators select projects that demonstrate excellence and potential for further development. Individuals cannot directly submit their projects to Best Lab; instead, administrators curate and showcase projects. An account on the Exynos Developer Society platform gets locked after five consecutive incorrect password attempts. Locked accounts can be unlocked through a verification process that involves steps sent to the linked email address. For support with development tools in the Exynos Developer Society, users are encouraged to participate in public forums. Personalized assistance can be obtained by submitting queries through the \"Contact Us\" page. The ADB Client Proxy tool enables ADB connections between devices in Device Farm and a local PC. The EDS Chatbot currently supports users within the Exynos Developer Society. The future goal of the EDS Chatbot is to provide information about technical issues, product details, events, and announcements pertinent to the Samsung S.LSI Exynos platform."
    ],
    "reference": "Device Farm allows ADB connections between devices and a local PC using the ADB Client Proxy tool.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\testset_2503\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:53:10.804823+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What kind of resources and support does the Exynos Developer Society provide for developers working with Exynos chipsets?",
    "reference_contexts": [
      "Exynos Developer Society is managed by Samsung S. LSI and offers a range of tools and training resources for development on the Exynos platform. Best Lab is a platform within the Exynos Developer Society where administrators select projects that demonstrate excellence and potential for further development. Individuals cannot directly submit their projects to Best Lab; instead, administrators curate and showcase projects. An account on the Exynos Developer Society platform gets locked after five consecutive incorrect password attempts. Locked accounts can be unlocked through a verification process that involves steps sent to the linked email address. For support with development tools in the Exynos Developer Society, users are encouraged to participate in public forums. Personalized assistance can be obtained by submitting queries through the \"Contact Us\" page. The ADB Client Proxy tool enables ADB connections between devices in Device Farm and a local PC. The EDS Chatbot currently supports users within the Exynos Developer Society. The future goal of the EDS Chatbot is to provide information about technical issues, product details, events, and announcements pertinent to the Samsung S.LSI Exynos platform."
    ],
    "reference": "Exynos Developer Society is managed by Samsung S. LSI and offers a range of tools and training resources for development on the Exynos platform. Users are encouraged to participate in public forums for support with development tools. Personalized assistance can be obtained by submitting queries through the 'Contact Us' page. Additionally, the EDS Chatbot supports users within the Exynos Developer Society and aims to provide information about technical issues, product details, events, and announcements related to the Samsung S.LSI Exynos platform.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\testset_2503\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:53:10.804823+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "Can you explain the role of Samsung S. LSI in the Exynos Developer Society and its impact on mobile technology development?",
    "reference_contexts": [
      "Exynos Developer Society is managed by Samsung S. LSI and offers a range of tools and training resources for development on the Exynos platform. Best Lab is a platform within the Exynos Developer Society where administrators select projects that demonstrate excellence and potential for further development. Individuals cannot directly submit their projects to Best Lab; instead, administrators curate and showcase projects. An account on the Exynos Developer Society platform gets locked after five consecutive incorrect password attempts. Locked accounts can be unlocked through a verification process that involves steps sent to the linked email address. For support with development tools in the Exynos Developer Society, users are encouraged to participate in public forums. Personalized assistance can be obtained by submitting queries through the \"Contact Us\" page. The ADB Client Proxy tool enables ADB connections between devices in Device Farm and a local PC. The EDS Chatbot currently supports users within the Exynos Developer Society. The future goal of the EDS Chatbot is to provide information about technical issues, product details, events, and announcements pertinent to the Samsung S.LSI Exynos platform."
    ],
    "reference": "Samsung S. LSI manages the Exynos Developer Society, which offers a range of tools and training resources for development on the Exynos platform. This includes the Best Lab platform, where administrators select projects that demonstrate excellence and potential for further development. The society also provides support through public forums and personalized assistance via the \"Contact Us\" page, enhancing the overall development experience for mobile technology developers.",
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\testset_2503\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:53:10.804823+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the key advancements in the Exynos 2400 chipset compared to the Exynos 2200, particularly in terms of AI performance and camera capabilities?",
    "reference_contexts": [
      "<1-hop>\n\nExynos Developer Society is managed by Samsung S. LSI and offers a range of tools and training resources for development on the Exynos platform. Best Lab is a platform within the Exynos Developer Society where administrators select projects that demonstrate excellence and potential for further development. Individuals cannot directly submit their projects to Best Lab; instead, administrators curate and showcase projects. An account on the Exynos Developer Society platform gets locked after five consecutive incorrect password attempts. Locked accounts can be unlocked through a verification process that involves steps sent to the linked email address. For support with development tools in the Exynos Developer Society, users are encouraged to participate in public forums. Personalized assistance can be obtained by submitting queries through the \"Contact Us\" page. The ADB Client Proxy tool enables ADB connections between devices in Device Farm and a local PC. The EDS Chatbot currently supports users within the Exynos Developer Society. The future goal of the EDS Chatbot is to provide information about technical issues, product details, events, and announcements pertinent to the Samsung S.LSI Exynos platform.",
      "<2-hop>\n\nThe Exynos 2200 chipset features a CPU configuration with Cortex®-X2, Cortex®-A710, and Cortex®-A510, and includes the Samsung Xclipse 920 GPU. The Exynos 2200 chipset is equipped with an AI Engine that has a Dual-core NPU and DSP. Camera capabilities of the Exynos 2200 support up to 200MP in single camera mode and dual-camera setups of 64MP + 32MP at 30fps. Video performance of the Exynos 2200 chipset includes up to 8K decoding at 60fps and 8K encoding at 30fps, supporting 10-bit HEVC(H.265) and VP9. The display capabilities of the Exynos 2200 chipset include supporting 4K/WQUXGA resolutions at 120Hz and QHD+ at 144Hz. The Exynos 2200 chipset aims to enhance mobile gaming with features like hardware-accelerated ray tracing and variable rate shading. Smartphones utilizing the Exynos 2200 chipset include the Samsung Galaxy S22 Ultra, S22+, and S22. The Exynos 2200 chipset is built using a 4-nanometer EUV process, incorporates Arm's latest Armv9 CPU cores, and includes a fast 5G modem compliant with 3GPP Release 16. The Exynos 2200 chipset offers advanced security features with an integrated Secure Element and robust encryption. The Exynos 2200 chipset's ISP supports resolutions up to 200MP and features AI-integrated camera functionalities for professional-quality images. There is no native library support for the Fourier Transform in the Exynos 2200 chipset; third-party libraries or manual implementation are recommended. The Exynos 2400 chipset includes a CPU configuration of Cortex®-X4, Cortex®-A720, and Cortex®-A520, paired with the Samsung Xclipse 940 GPU. The Exynos 2400 chipset features an AI Engine equipped with a 17K MAC NPU (2-GNPU+2-SNPU) and DSP. The Exynos 2400 chipset's camera capabilities support up to 320MP in single camera mode and a dual-camera setup of 64MP + 32MP at 30fps. The Exynos 2400 chipset's video performance includes up to 8K decoding at 60fps and encoding at 30fps for 10-bit HEVC(H.265) and VP9. The Exynos 2400 chipset's display support extends to 4K/WQUXGA at 120Hz and QHD+ at 144Hz. The Exynos 2400 aims to transform mobile gaming with console-quality graphics and advanced GPU technology. Smartphones utilizing the Exynos 2400 chipset include the Samsung Galaxy S24+ and Samsung Galaxy S24. The Exynos 2400 chipset offers a 14.7x improvement in AI performance over its predecessor, enhancing efficiency and reducing latency while boosting device security. The Exynos 2400 chipset includes a 5G NR modem supporting up to 9.6Gbps downlink speeds in FR1 and 12.1Gbps in FR2, compliant with 3GPP’s Rel.17 standard. The Exynos 2400 chipset also supports non-terrestrial network (NTN) satellite connections and has an integrated GNSS block for accurate positioning, enhancing connectivity even in cellular dead zones. The AI Challenger is a developer program by Samsung S.LSI that focuses on using and developing AI technology through the Exynos AI Studio. The AI Challenger aims to drive technological innovation in AI and provides a versatile toolkit for creating and optimizing AI models. The AI Challenger program is structured into three stages: familiarization with tools, active research and development, and finalizing and presenting projects. The AI Challenger targets Korean students and postgraduates, aiming to enhance their AI expertise and competitiveness in the industry. The AI Challenger promotes Edge AI technology advancement, industry-academia collaboration, and the development of innovative prototypes. The AI Challenger is also designed to address real-world technological challenges and stimulate the growth of the AI sector. The AI Challenger 1 includes a chipset called the Exynos 2200, which is integral to the AI development tools offered. The AI Challenger 2 includes a chipset called the Exynos 2400, which is integral to the AI development tools offered. The"
    ],
    "reference": "The Exynos 2400 chipset features significant advancements over the Exynos 2200, particularly in AI performance and camera capabilities. The Exynos 2400 includes a CPU configuration of Cortex®-X4, Cortex®-A720, and Cortex®-A520, paired with the Samsung Xclipse 940 GPU, and boasts an AI Engine equipped with a 17K MAC NPU (2-GNPU+2-SNPU) and DSP, offering a 14.7x improvement in AI performance over its predecessor. In terms of camera capabilities, the Exynos 2400 supports up to 320MP in single camera mode, compared to the Exynos 2200's support for up to 200MP. Both chipsets support dual-camera setups of 64MP + 32MP at 30fps and have similar video performance, including up to 8K decoding at 60fps and encoding at 30fps for 10-bit HEVC(H.265) and VP9. However, the Exynos 2400 aims to transform mobile gaming with console-quality graphics and advanced GPU technology, enhancing the overall user experience.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\testset_2503\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:53:10.804823+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the key differences between the Exynos 2400 and Exynos 2200 chipsets in terms of AI capabilities and camera support?",
    "reference_contexts": [
      "<1-hop>\n\nExynos Developer Society is managed by Samsung S. LSI and offers a range of tools and training resources for development on the Exynos platform. Best Lab is a platform within the Exynos Developer Society where administrators select projects that demonstrate excellence and potential for further development. Individuals cannot directly submit their projects to Best Lab; instead, administrators curate and showcase projects. An account on the Exynos Developer Society platform gets locked after five consecutive incorrect password attempts. Locked accounts can be unlocked through a verification process that involves steps sent to the linked email address. For support with development tools in the Exynos Developer Society, users are encouraged to participate in public forums. Personalized assistance can be obtained by submitting queries through the \"Contact Us\" page. The ADB Client Proxy tool enables ADB connections between devices in Device Farm and a local PC. The EDS Chatbot currently supports users within the Exynos Developer Society. The future goal of the EDS Chatbot is to provide information about technical issues, product details, events, and announcements pertinent to the Samsung S.LSI Exynos platform.",
      "<2-hop>\n\nThe Exynos 2200 chipset features a CPU configuration with Cortex®-X2, Cortex®-A710, and Cortex®-A510, and includes the Samsung Xclipse 920 GPU. The Exynos 2200 chipset is equipped with an AI Engine that has a Dual-core NPU and DSP. Camera capabilities of the Exynos 2200 support up to 200MP in single camera mode and dual-camera setups of 64MP + 32MP at 30fps. Video performance of the Exynos 2200 chipset includes up to 8K decoding at 60fps and 8K encoding at 30fps, supporting 10-bit HEVC(H.265) and VP9. The display capabilities of the Exynos 2200 chipset include supporting 4K/WQUXGA resolutions at 120Hz and QHD+ at 144Hz. The Exynos 2200 chipset aims to enhance mobile gaming with features like hardware-accelerated ray tracing and variable rate shading. Smartphones utilizing the Exynos 2200 chipset include the Samsung Galaxy S22 Ultra, S22+, and S22. The Exynos 2200 chipset is built using a 4-nanometer EUV process, incorporates Arm's latest Armv9 CPU cores, and includes a fast 5G modem compliant with 3GPP Release 16. The Exynos 2200 chipset offers advanced security features with an integrated Secure Element and robust encryption. The Exynos 2200 chipset's ISP supports resolutions up to 200MP and features AI-integrated camera functionalities for professional-quality images. There is no native library support for the Fourier Transform in the Exynos 2200 chipset; third-party libraries or manual implementation are recommended. The Exynos 2400 chipset includes a CPU configuration of Cortex®-X4, Cortex®-A720, and Cortex®-A520, paired with the Samsung Xclipse 940 GPU. The Exynos 2400 chipset features an AI Engine equipped with a 17K MAC NPU (2-GNPU+2-SNPU) and DSP. The Exynos 2400 chipset's camera capabilities support up to 320MP in single camera mode and a dual-camera setup of 64MP + 32MP at 30fps. The Exynos 2400 chipset's video performance includes up to 8K decoding at 60fps and encoding at 30fps for 10-bit HEVC(H.265) and VP9. The Exynos 2400 chipset's display support extends to 4K/WQUXGA at 120Hz and QHD+ at 144Hz. The Exynos 2400 aims to transform mobile gaming with console-quality graphics and advanced GPU technology. Smartphones utilizing the Exynos 2400 chipset include the Samsung Galaxy S24+ and Samsung Galaxy S24. The Exynos 2400 chipset offers a 14.7x improvement in AI performance over its predecessor, enhancing efficiency and reducing latency while boosting device security. The Exynos 2400 chipset includes a 5G NR modem supporting up to 9.6Gbps downlink speeds in FR1 and 12.1Gbps in FR2, compliant with 3GPP’s Rel.17 standard. The Exynos 2400 chipset also supports non-terrestrial network (NTN) satellite connections and has an integrated GNSS block for accurate positioning, enhancing connectivity even in cellular dead zones. The AI Challenger is a developer program by Samsung S.LSI that focuses on using and developing AI technology through the Exynos AI Studio. The AI Challenger aims to drive technological innovation in AI and provides a versatile toolkit for creating and optimizing AI models. The AI Challenger program is structured into three stages: familiarization with tools, active research and development, and finalizing and presenting projects. The AI Challenger targets Korean students and postgraduates, aiming to enhance their AI expertise and competitiveness in the industry. The AI Challenger promotes Edge AI technology advancement, industry-academia collaboration, and the development of innovative prototypes. The AI Challenger is also designed to address real-world technological challenges and stimulate the growth of the AI sector. The AI Challenger 1 includes a chipset called the Exynos 2200, which is integral to the AI development tools offered. The AI Challenger 2 includes a chipset called the Exynos 2400, which is integral to the AI development tools offered. The"
    ],
    "reference": "The Exynos 2400 chipset features an AI Engine equipped with a 17K MAC NPU (2-GNPU+2-SNPU) and DSP, offering a 14.7x improvement in AI performance over its predecessor, the Exynos 2200, which has a Dual-core NPU and DSP. In terms of camera support, the Exynos 2400 can handle up to 320MP in single camera mode, while the Exynos 2200 supports up to 200MP. Both chipsets support dual-camera setups, but the Exynos 2400 provides enhanced capabilities for mobile gaming and advanced GPU technology.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\testset_2503\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:53:10.804823+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What are the key differences in AI performance and camera capabilities between the Exynos 2400 and Exynos 2200 chipsets, and how does the Exynos Developer Society support developers working with these chipsets?",
    "reference_contexts": [
      "<1-hop>\n\nExynos Developer Society is managed by Samsung S. LSI and offers a range of tools and training resources for development on the Exynos platform. Best Lab is a platform within the Exynos Developer Society where administrators select projects that demonstrate excellence and potential for further development. Individuals cannot directly submit their projects to Best Lab; instead, administrators curate and showcase projects. An account on the Exynos Developer Society platform gets locked after five consecutive incorrect password attempts. Locked accounts can be unlocked through a verification process that involves steps sent to the linked email address. For support with development tools in the Exynos Developer Society, users are encouraged to participate in public forums. Personalized assistance can be obtained by submitting queries through the \"Contact Us\" page. The ADB Client Proxy tool enables ADB connections between devices in Device Farm and a local PC. The EDS Chatbot currently supports users within the Exynos Developer Society. The future goal of the EDS Chatbot is to provide information about technical issues, product details, events, and announcements pertinent to the Samsung S.LSI Exynos platform.",
      "<2-hop>\n\nThe Exynos 2200 chipset features a CPU configuration with Cortex®-X2, Cortex®-A710, and Cortex®-A510, and includes the Samsung Xclipse 920 GPU. The Exynos 2200 chipset is equipped with an AI Engine that has a Dual-core NPU and DSP. Camera capabilities of the Exynos 2200 support up to 200MP in single camera mode and dual-camera setups of 64MP + 32MP at 30fps. Video performance of the Exynos 2200 chipset includes up to 8K decoding at 60fps and 8K encoding at 30fps, supporting 10-bit HEVC(H.265) and VP9. The display capabilities of the Exynos 2200 chipset include supporting 4K/WQUXGA resolutions at 120Hz and QHD+ at 144Hz. The Exynos 2200 chipset aims to enhance mobile gaming with features like hardware-accelerated ray tracing and variable rate shading. Smartphones utilizing the Exynos 2200 chipset include the Samsung Galaxy S22 Ultra, S22+, and S22. The Exynos 2200 chipset is built using a 4-nanometer EUV process, incorporates Arm's latest Armv9 CPU cores, and includes a fast 5G modem compliant with 3GPP Release 16. The Exynos 2200 chipset offers advanced security features with an integrated Secure Element and robust encryption. The Exynos 2200 chipset's ISP supports resolutions up to 200MP and features AI-integrated camera functionalities for professional-quality images. There is no native library support for the Fourier Transform in the Exynos 2200 chipset; third-party libraries or manual implementation are recommended. The Exynos 2400 chipset includes a CPU configuration of Cortex®-X4, Cortex®-A720, and Cortex®-A520, paired with the Samsung Xclipse 940 GPU. The Exynos 2400 chipset features an AI Engine equipped with a 17K MAC NPU (2-GNPU+2-SNPU) and DSP. The Exynos 2400 chipset's camera capabilities support up to 320MP in single camera mode and a dual-camera setup of 64MP + 32MP at 30fps. The Exynos 2400 chipset's video performance includes up to 8K decoding at 60fps and encoding at 30fps for 10-bit HEVC(H.265) and VP9. The Exynos 2400 chipset's display support extends to 4K/WQUXGA at 120Hz and QHD+ at 144Hz. The Exynos 2400 aims to transform mobile gaming with console-quality graphics and advanced GPU technology. Smartphones utilizing the Exynos 2400 chipset include the Samsung Galaxy S24+ and Samsung Galaxy S24. The Exynos 2400 chipset offers a 14.7x improvement in AI performance over its predecessor, enhancing efficiency and reducing latency while boosting device security. The Exynos 2400 chipset includes a 5G NR modem supporting up to 9.6Gbps downlink speeds in FR1 and 12.1Gbps in FR2, compliant with 3GPP’s Rel.17 standard. The Exynos 2400 chipset also supports non-terrestrial network (NTN) satellite connections and has an integrated GNSS block for accurate positioning, enhancing connectivity even in cellular dead zones. The AI Challenger is a developer program by Samsung S.LSI that focuses on using and developing AI technology through the Exynos AI Studio. The AI Challenger aims to drive technological innovation in AI and provides a versatile toolkit for creating and optimizing AI models. The AI Challenger program is structured into three stages: familiarization with tools, active research and development, and finalizing and presenting projects. The AI Challenger targets Korean students and postgraduates, aiming to enhance their AI expertise and competitiveness in the industry. The AI Challenger promotes Edge AI technology advancement, industry-academia collaboration, and the development of innovative prototypes. The AI Challenger is also designed to address real-world technological challenges and stimulate the growth of the AI sector. The AI Challenger 1 includes a chipset called the Exynos 2200, which is integral to the AI development tools offered. The AI Challenger 2 includes a chipset called the Exynos 2400, which is integral to the AI development tools offered. The"
    ],
    "reference": "The Exynos 2400 chipset offers a significant 14.7x improvement in AI performance over its predecessor, the Exynos 2200, enhancing efficiency and reducing latency while boosting device security. In terms of camera capabilities, the Exynos 2400 supports up to 320MP in single camera mode, compared to the Exynos 2200, which supports up to 200MP. The Exynos Developer Society supports developers by providing a range of tools and training resources for development on the Exynos platform, including the Best Lab for showcasing projects, public forums for support, and personalized assistance through the 'Contact Us' page.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\testset_2503\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:53:10.804823+09:00",
    "chatbot_server": ""
  },
  {
    "user_input": "What advancements does the Exynos 2400 chipset bring compared to the Exynos 2200, particularly in terms of AI performance and camera capabilities?",
    "reference_contexts": [
      "<1-hop>\n\nExynos Developer Society is managed by Samsung S. LSI and offers a range of tools and training resources for development on the Exynos platform. Best Lab is a platform within the Exynos Developer Society where administrators select projects that demonstrate excellence and potential for further development. Individuals cannot directly submit their projects to Best Lab; instead, administrators curate and showcase projects. An account on the Exynos Developer Society platform gets locked after five consecutive incorrect password attempts. Locked accounts can be unlocked through a verification process that involves steps sent to the linked email address. For support with development tools in the Exynos Developer Society, users are encouraged to participate in public forums. Personalized assistance can be obtained by submitting queries through the \"Contact Us\" page. The ADB Client Proxy tool enables ADB connections between devices in Device Farm and a local PC. The EDS Chatbot currently supports users within the Exynos Developer Society. The future goal of the EDS Chatbot is to provide information about technical issues, product details, events, and announcements pertinent to the Samsung S.LSI Exynos platform.",
      "<2-hop>\n\nThe Exynos 2200 chipset features a CPU configuration with Cortex®-X2, Cortex®-A710, and Cortex®-A510, and includes the Samsung Xclipse 920 GPU. The Exynos 2200 chipset is equipped with an AI Engine that has a Dual-core NPU and DSP. Camera capabilities of the Exynos 2200 support up to 200MP in single camera mode and dual-camera setups of 64MP + 32MP at 30fps. Video performance of the Exynos 2200 chipset includes up to 8K decoding at 60fps and 8K encoding at 30fps, supporting 10-bit HEVC(H.265) and VP9. The display capabilities of the Exynos 2200 chipset include supporting 4K/WQUXGA resolutions at 120Hz and QHD+ at 144Hz. The Exynos 2200 chipset aims to enhance mobile gaming with features like hardware-accelerated ray tracing and variable rate shading. Smartphones utilizing the Exynos 2200 chipset include the Samsung Galaxy S22 Ultra, S22+, and S22. The Exynos 2200 chipset is built using a 4-nanometer EUV process, incorporates Arm's latest Armv9 CPU cores, and includes a fast 5G modem compliant with 3GPP Release 16. The Exynos 2200 chipset offers advanced security features with an integrated Secure Element and robust encryption. The Exynos 2200 chipset's ISP supports resolutions up to 200MP and features AI-integrated camera functionalities for professional-quality images. There is no native library support for the Fourier Transform in the Exynos 2200 chipset; third-party libraries or manual implementation are recommended. The Exynos 2400 chipset includes a CPU configuration of Cortex®-X4, Cortex®-A720, and Cortex®-A520, paired with the Samsung Xclipse 940 GPU. The Exynos 2400 chipset features an AI Engine equipped with a 17K MAC NPU (2-GNPU+2-SNPU) and DSP. The Exynos 2400 chipset's camera capabilities support up to 320MP in single camera mode and a dual-camera setup of 64MP + 32MP at 30fps. The Exynos 2400 chipset's video performance includes up to 8K decoding at 60fps and encoding at 30fps for 10-bit HEVC(H.265) and VP9. The Exynos 2400 chipset's display support extends to 4K/WQUXGA at 120Hz and QHD+ at 144Hz. The Exynos 2400 aims to transform mobile gaming with console-quality graphics and advanced GPU technology. Smartphones utilizing the Exynos 2400 chipset include the Samsung Galaxy S24+ and Samsung Galaxy S24. The Exynos 2400 chipset offers a 14.7x improvement in AI performance over its predecessor, enhancing efficiency and reducing latency while boosting device security. The Exynos 2400 chipset includes a 5G NR modem supporting up to 9.6Gbps downlink speeds in FR1 and 12.1Gbps in FR2, compliant with 3GPP’s Rel.17 standard. The Exynos 2400 chipset also supports non-terrestrial network (NTN) satellite connections and has an integrated GNSS block for accurate positioning, enhancing connectivity even in cellular dead zones. The AI Challenger is a developer program by Samsung S.LSI that focuses on using and developing AI technology through the Exynos AI Studio. The AI Challenger aims to drive technological innovation in AI and provides a versatile toolkit for creating and optimizing AI models. The AI Challenger program is structured into three stages: familiarization with tools, active research and development, and finalizing and presenting projects. The AI Challenger targets Korean students and postgraduates, aiming to enhance their AI expertise and competitiveness in the industry. The AI Challenger promotes Edge AI technology advancement, industry-academia collaboration, and the development of innovative prototypes. The AI Challenger is also designed to address real-world technological challenges and stimulate the growth of the AI sector. The AI Challenger 1 includes a chipset called the Exynos 2200, which is integral to the AI development tools offered. The AI Challenger 2 includes a chipset called the Exynos 2400, which is integral to the AI development tools offered. The"
    ],
    "reference": "The Exynos 2400 chipset brings significant advancements over the Exynos 2200, particularly in AI performance and camera capabilities. The Exynos 2400 features an AI Engine equipped with a 17K MAC NPU (2-GNPU+2-SNPU), offering a 14.7x improvement in AI performance, enhancing efficiency and reducing latency. In terms of camera capabilities, the Exynos 2400 supports up to 320MP in single camera mode, compared to the Exynos 2200's support for up to 200MP. Additionally, both chipsets support dual-camera setups, but the Exynos 2400's enhancements in AI technology aim to transform mobile gaming and improve overall device security.",
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "file": "C:\\Users\\User\\Downloads\\20250317_input\\Input\\testset_2503\\fail",
    "retrieved_contexts": "",
    "response": "",
    "chatbot_response": "",
    "date_time": "2025-03-17 18:53:10.804823+09:00",
    "chatbot_server": ""
  }
]