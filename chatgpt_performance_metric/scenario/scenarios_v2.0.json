[
    {
        "question": "Please tell me about EDS mean?",
        "contexts": [
            [
            "Question: Please tell me about EDS mean?\nAnswer: EDS stands for Exynos Developer Society, an operating system created for enriching the developer ecosystem in Samsung S.LSI.",
            "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
            "Question: Please tell me who operates EDS Assistant?\nAnswer: EDS Assistant is designed, developed, and operated by the Samsung S.LSI development team.",
            "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
            "Question: Please tell us about the company that actually developed EDS Assistant.\nAnswer: Samsung S.LSI plans and supervises EDS Assistant and ThunderSoft Korea supervises the actual development.",
            "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
            "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
            "Section: ENN Framework Data Type References\n# _ennBufferInfo\nContent:\n## Public Attributes\n| | Name |\n| -------------- | -------------- |\n| bool | **is_able_to_update** |\n| uint32_t | **n** |\n| uint32_t | **width** |\n| uint32_t | **height** |\n| uint32_t | **channel** |\n| uint32_t | **size** |\n| uint32_t | **buffer_type** |\n| const char * | **label** |\n## Public Attributes Documentation\n### variable is_able_to_update\n```cpp\nbool is_able_to_update;\n```\n### variable n\n```cpp\nuint32_t n;\n```\n### variable width\n```cpp\nuint32_t width;\n```\n### variable height\n```cpp\nuint32_t height;\n```\n### variable channel\n```cpp\nuint32_t channel;\n```\n### variable size\n```cpp\nuint32_t size;\n```\n### variable buffer_type\n```cpp\nuint32_t buffer_type;\n```\n### variable label\n```cpp\nconst char * label;\n```"
            ]
        ],
        "answer": "EDS stands for Exynos Developer Society, an operating system created for enriching the developer ecosystem in Samsung S.LSI.",
        "ground_truth": "EDS stands for Exynos Developer Society, an operating system created for enriching the developer ecosystem in Samsung S.LSI."
    },
    {
        "question": "Please tell me who operates EDS Assistant?",
        "contexts": [
            [
            "Question: Please tell me who operates EDS Assistant?\nAnswer: EDS Assistant is designed, developed, and operated by the Samsung S.LSI development team.",
            "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
            "Question: Please tell us about the company that actually developed EDS Assistant.\nAnswer: Samsung S.LSI plans and supervises EDS Assistant and ThunderSoft Korea supervises the actual development.",
            "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
            "Question: Please tell me about EDS mean?\nAnswer: EDS stands for Exynos Developer Society, an operating system created for enriching the developer ecosystem in Samsung S.LSI.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
            "Section: Exynos AI Studio User Guide (TBD)\nContent:\n# Introduction to Exynos AI High-Level Toolchain (EHT)\nEHT** is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models.\n## System overview diagram",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
            "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU."
            ]
        ],
        "answer": "EDS Assistant is designed, developed, and operated by the Samsung S.LSI development team.",
        "ground_truth": "EDS Assistant is designed, developed, and operated by the Samsung S.LSI development team."
    },
    {
        "question": "Please tell us about the purpose of EDS Assistant.",
        "contexts": [
            [
            "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
            "Question: Please tell me who operates EDS Assistant?\nAnswer: EDS Assistant is designed, developed, and operated by the Samsung S.LSI development team.",
            "Question: Please tell us about the company that actually developed EDS Assistant.\nAnswer: Samsung S.LSI plans and supervises EDS Assistant and ThunderSoft Korea supervises the actual development.",
            "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
            "Question: Please tell me about EDS mean?\nAnswer: EDS stands for Exynos Developer Society, an operating system created for enriching the developer ecosystem in Samsung S.LSI.",
            "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
            "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
            "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
            "Section: Exynos AI Studio User Guide (TBD)\nContent:\n# Introduction to Exynos AI High-Level Toolchain (EHT)\nEHT** is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models.\n## System overview diagram",
            "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model."
            ]
        ],
        "answer": "The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
        "ground_truth": "The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK."
    },
    {
        "question": "Please tell us about your future goals for EDS Assistant?",
        "contexts": [
            [
            "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
            "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
            "Question: Please tell me who operates EDS Assistant?\nAnswer: EDS Assistant is designed, developed, and operated by the Samsung S.LSI development team.",
            "Question: Please tell us about the company that actually developed EDS Assistant.\nAnswer: Samsung S.LSI plans and supervises EDS Assistant and ThunderSoft Korea supervises the actual development.",
            "Question: Please tell me about EDS mean?\nAnswer: EDS stands for Exynos Developer Society, an operating system created for enriching the developer ecosystem in Samsung S.LSI.",
            "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
            "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
            "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
            "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
            "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day."
            ]
        ],
        "answer": "The future goals for EDS Assistant include evolving to provide information on technical issues, product information, events, and announcements related to Samsung S.LSI Exynos products, in addition to AI and the Samsung S.LSI ENN SDK.",
        "ground_truth": "EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK."
    },
    {
        "question": "Please tell us about the company that actually developed EDS Assistant.",
        "contexts": [
            [
            "Question: Please tell us about the company that actually developed EDS Assistant.\nAnswer: Samsung S.LSI plans and supervises EDS Assistant and ThunderSoft Korea supervises the actual development.",
            "Question: Please tell me who operates EDS Assistant?\nAnswer: EDS Assistant is designed, developed, and operated by the Samsung S.LSI development team.",
            "Question: Please tell us about the purpose of EDS Assistant.\nAnswer: The current version of EDS Assistant targets support for users using the Samsung S.LSI ENN SDK.",
            "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
            "Question: Please tell me about EDS mean?\nAnswer: EDS stands for Exynos Developer Society, an operating system created for enriching the developer ecosystem in Samsung S.LSI.",
            "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
            "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
            "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
            "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
            "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers."
            ]
        ],
        "answer": "The EDS Assistant was developed by ThunderSoft Korea, while Samsung S.LSI plans and supervises the project.",
        "ground_truth": "Samsung S.LSI plans and supervises EDS Assistant and ThunderSoft Korea supervises the actual development."
    },
    {
        "question": "Please tell us about the ENN SDK.",
        "contexts": [
            [
            "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
            "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\n## 2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.",
            "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
            "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
            "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
            "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\n## 3. Tools\n### ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.",
            "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios."
            ]
        ],
        "answer": "The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for optimizing and implementing AI and machine learning models on Exynos-based devices. It provides tools for model conversion and optimization, facilitating the use of AI models on devices powered by Exynos chipsets. Key features include support for various AI algorithms, model optimization specifically for Exynos chipsets, and improvements in user interface for better developer usability. The SDK is primarily aimed at mobile device applications, helping developers efficiently utilize AI technologies.",
        "ground_truth": "ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement."
    },
    {
        "question": "I want to know about ENN SDK.",
        "contexts": [
            [
            "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
            "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\n## 2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
            "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
            "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
            "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\n## 3. Tools\n### ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.",
            "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
            "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers."
            ]
        ],
        "answer": "The ENN SDK (Exynos Neural Network Software Development Kit) is developed by Samsung S.LSI for the Exynos chipset. It is designed to optimize and implement AI and machine learning models specifically for Exynos-based devices. The SDK facilitates the conversion and optimization of AI models to ensure compatibility and efficiency on devices powered by Exynos processors. Key features include model optimization for Exynos chipsets and support for various AI algorithms, aimed particularly at mobile devices, providing developers with efficient tools through continuous updates and improvements.",
        "ground_truth": "The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor."
    },
    {
        "question": "I want to download the ENN SDK and install it on a local PC or server to use it.",
        "contexts": [
            [
            "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
            "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability.",
            "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
            "Section: ENN SDK Service\n# ENN SDK\n## ENN SDK Service\nContent:\n### Conversion\n1. Select **Convert** to initiate the conversion process.\n2. After the completion of conversion process, the **NNC Download** button is enabled.\n### Download Model\n1. Click **NNC Download** to obtain the converted NNC model file.\n2. To view the logs for the conversion that has failed, click **Log Download**.\n\tYou can download and examine the log files.\n3. Copy the downloaded model to `${APP_ROOT}/app/src/main/assets`.",
            "Question: Please tell us the minimum specifications required to compile via ENN SDK?\nAnswer: For general use, use a recently widely used general-purpose PC or laptop (Intel Available for Core i5 or AMD Ryzen 5 serie. However, for algorithms with high complexity, server-level use is required depending on the model.",
            "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
            "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\n## 3. Tools\n### ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\n## 3. Tools\n### ENN SDK Service\nContent:\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required."
            ]
        ],
        "answer": "Currently, the ENN SDK is not available for external download. Future plans include preparing an external distribution version along with GUI improvements. This version will be distributed first to those who participated in the AI Challenger.",
        "ground_truth": "Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger."
    },
    {
        "question": "Please provide operating system information to use the ENN SDK.",
        "contexts": [
            [
            "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability.",
            "Question: Please tell us the minimum specifications required to compile via ENN SDK?\nAnswer: For general use, use a recently widely used general-purpose PC or laptop (Intel Available for Core i5 or AMD Ryzen 5 serie. However, for algorithms with high complexity, server-level use is required depending on the model.",
            "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
            "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
            "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
            "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
            "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
            "Section: Documentation Home\n# ENN SDK Documentation\n## ENN SDK references\nContent:\n### ENN SDK developer guide\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### API references\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### Support matrix\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
            "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis."
            ]
        ],
        "answer": "The ENN SDK was developed based on Linux Server and can be used on various operating systems through a Docker-based distribution. However, the current version available is an internally developed one, and an externally distributed version with enhanced usability as a separate GUI version is planned for the future.",
        "ground_truth": "ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability."
    },
    {
        "question": "Please tell us the minimum specifications required to compile via ENN SDK?",
        "contexts": [
            [
            "Question: Please tell us the minimum specifications required to compile via ENN SDK?\nAnswer: For general use, use a recently widely used general-purpose PC or laptop (Intel Available for Core i5 or AMD Ryzen 5 serie. However, for algorithms with high complexity, server-level use is required depending on the model.",
            "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability.",
            "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
            "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
            "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
            "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\n## 3. Tools\n### ENN SDK Service\nContent:\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
            "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
            "Section: ENN Framework\n# ENN Framework\n## Using ENN Framework on Android\n### Loading Libraries\nContent:\nDownload the ENN framework library from resources.\nTo load the necessary libraries, perform the folowing steps:\n1. Modiy Android Manifest to:\n\t```xml\n\t<manifest>\n\t\t<application>\n\t\t\t...\n\t\t\t<!-- Declare the native library in the Android Manifest -->\n\t\t\t<uses-native-library android:name=\"libenn_user.samsung_slsi.so\" />\n\t\t\t...\n\t\t</application>\n\t</manifest>\n\t```\n2. Copy `libenn_public_api_ndk_v1.so` to `${APP_ROOT}/app/src/main/jniLibs/arm64-v8a`.\n3. Copy `enn-api-public_ndk_v1.hpp` and `enn_api-type_ndk_v1.h` to `${APP_ROOT}/app/src/main/cpp/include`."
            ]
        ],
        "answer": "For general use, it is recommended to use a recently widely used general-purpose PC or laptop, such as an Intel Core i5 or AMD Ryzen 5 series. However, for algorithms with high complexity, server-level hardware may be required depending on the model.",
        "ground_truth": "For general use, use a recently widely used general-purpose PC or laptop (Intel Available for Core i5 or AMD Ryzen 5 serie. However, for algorithms with high complexity, server-level use is required depending on the model."
    },
    {
        "question": "Please tell us about the advantages of ENN SDK compared to other SDKs?",
        "contexts": [
            [
            "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
            "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
            "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\n## 2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.",
            "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
            "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
            "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
            "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\n## 2. Features\nContent:\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix."
            ]
        ],
        "answer": "The ENN SDK has several advantages compared to other SDKs:\n\n1. **Optimization for Exynos Chipsets**: The ENN SDK is specifically designed and optimized for Exynos chipsets, ensuring that AI and machine learning models run efficiently on devices powered by these processors.\n\n2. **Integration with Samsung Mobile Devices**: It serves as an optimization tool for Samsung mobile devices, enhancing performance and capability for applications developed on these platforms.\n\n3. **Developer Engagement**: The SDK provides opportunities for domestic developers to voice their needs and participate in development, fostering a collaborative development environment.\n\n4. **Continuous Improvement**: The ENN SDK aims to provide developers with efficient tools through ongoing updates and enhancements, which may not be as pronounced in other SDKs.\n\nThese advantages position the ENN SDK as a robust option for developers working with AI models on Exynos-based devices.",
        "ground_truth": "The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program."
    },
    {
        "question": "Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.",
        "contexts": [
            [
            "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
            "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
            "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
            "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
            "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\n## 2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.",
            "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
            "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
            "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability."
            ]
        ],
        "answer": "The ENN SDK has some weaknesses compared to similar SDKs or tools from other companies, particularly in its expansion strategies for the IoT sector, as it is mainly used in mobile terminals. However, it is important to note that the ENN SDK team is actively planning to expand into various areas by reflecting the needs and feedback of developers.",
        "ground_truth": "Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers."
    },
    {
        "question": "Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.",
        "contexts": [
            [
            "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
            "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
            "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
            "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\n## 2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.",
            "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability.",
            "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\n## 3. Tools\n### ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.",
            "Section: Documentation Home\n# ENN SDK Documentation\n## ENN SDK references\nContent:\n### ENN SDK developer guide\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### API references\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### Support matrix\nThese support matrices provide information on the supported platforms and operators of ENN SDK."
            ]
        ],
        "answer": "Updates to the ENN SDK serviced through the portal are planned once a month. The team aims to respond to requests arising from this program and provide internally developed and updated content. Additionally, there are plans to distribute ENN Studio in the second half of next year, which will feature significantly improved UI and enhanced developer usability.",
        "ground_truth": "Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability."
    },
    {
        "question": "I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.",
        "contexts": [
            [
            "Question: I don't have enough knowledge about Edge AI, but I want to do a project using the ENN SDK.\nAnswer: To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model.",
            "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
            "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
            "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
            "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
            "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
            "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
            "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
            "Section: ENN SDK Service\n# ENN SDK\n## ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t"
            ]
        ],
        "answer": "To get started with your project using the ENN SDK, you don't need extensive knowledge about Edge AI. Familiarize yourself with common IoT device examples, as they illustrate how Edge AI is utilized. However, having development experience with AI models and algorithms is beneficial.\n\nThe ENN SDK is designed to help you optimize and implement AI models on Exynos devices. You can begin by exploring the available AI model categories and review the sample applications provided to understand how to develop your own. If you need any specific guidance or examples, feel free to ask!",
        "ground_truth": "To understand Edge Device, you only need to understand the usage examples of IOT devices that are generally understood. However, it is necessary to have development capabilities and experience with the AI model and algorithm itself. We will help you create a sample application that can be run on a reference device through an AI model."
    },
    {
        "question": "I want to know about the AI Challenger.",
        "contexts": [
            [
            "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
            "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
            "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
            "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
            "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
            "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
            "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
            "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
            "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
            "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK."
            ]
        ],
        "answer": "Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. \n\nContinually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
        "ground_truth": "The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK."
    },
    {
        "question": "I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.",
        "contexts": [
            [
            "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
            "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
            "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
            "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
            "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
            "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
            "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
            "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
            "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
            "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK."
            ]
        ],
        "answer": "In addition to the sample algorithm, you can use other algorithms to develop your model. However, please note that there may be restrictions on the areas supported. For details on the supported areas, you can check the portal bulletin board for the scope of support available.",
        "ground_truth": "In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board."
    },
    {
        "question": "I would like to see the source code related to the ENN SDK or portal in person.",
        "contexts": [
            [
            "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis.",
            "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
            "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
            "Question: Please let us know the weaknesses of ENN SDK compared to similar SDKs or tools from other companies.\nAnswer: Since it is mainly used in mobile terminals, there are some shortcomings in various expansion strategies for the IOT sector. However, like this program, we plan to quickly expand into various areas by reflecting the needs and voices of various developers.",
            "Question: Please provide operating system information to use the ENN SDK.\nAnswer: ENN SDK was developed based on Linux Server and can be used on various operating systems through Docker-based distribution. However, this is an internally developed version, and the externally distributed version will be distributed in the future as a separate GUI version with enhanced usability.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\n## 3. Tools\n### ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.",
            "Section: Documentation Home\n# ENN SDK Documentation\n## ENN SDK references\nContent:\n### ENN SDK developer guide\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n### API references\nThis API reference documentation provides a list of data types and functions of ENN SDK.\n### Support matrix\nThese support matrices provide information on the supported platforms and operators of ENN SDK.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\n## 2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.",
            "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor."
            ]
        ],
        "answer": "We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum, and we will check and provide support on a case-by-case basis.",
        "ground_truth": "We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis."
    },
    {
        "question": "I would like to know the AI model categories supported by ENN SDK.",
        "contexts": [
            [
            "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
            "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
            "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
            "Section: Support Matrix\n# Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\n## 2. Features\nContent:\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
            "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
            "Section: Support Matrix\n# Support Matrix\n## Supported Models\nContent:\nENN SDK supports the following models:\n- TensorFlow Lite\n\t- It is recommended to convert TFLite models using MLIR (Multi-Level Intermediate Representation) version 1.14 or higher.\n- Tensors up to four dimensions\n- Models with a maximum size of 1 GB\nAdditionally, for **NPU**, the models must be quantized.",
            "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL."
            ]
        ],
        "answer": "The ENN SDK accommodates a diverse range of AI model categories, including:\n\n- Image Classification\n- Object Detection\n- Segmentation\n- Pose Estimation\n- Image Enhancement\n- Depth Estimation\n- Recommendation Systems\n- Anomaly Detection\n- Computer Vision for Healthcare\n- Augmented and Virtual Reality",
        "ground_truth": "The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality."
    },
    {
        "question": "I would like to know which operators are supported by the ENN SDK when developing Image Classification.",
        "contexts": [
            [
            "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
            "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
            "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing segmentation\nAnswer: In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU."
            ]
        ],
        "answer": "For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
        "ground_truth": "For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION."
    },
    {
        "question": "I would like to know which operators are supported by the ENN SDK when developing Object Detection.",
        "contexts": [
            [
            "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
            "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
            "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing segmentation\nAnswer: In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION."
            ]
        ],
        "answer": "The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
        "ground_truth": "The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION."
    },
    {
        "question": "I would like to know which operators are supported by the ENN SDK when developing segmentation",
        "contexts": [
            [
            "Question: I would like to know which operators are supported by the ENN SDK when developing segmentation\nAnswer: In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
            "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU."
            ]
        ],
        "answer": "In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION.",
        "ground_truth": "In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION."
    },
    {
        "question": "I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.",
        "contexts": [
            [
            "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
            "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
            "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing segmentation\nAnswer: In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION."
            ]
        ],
        "answer": "For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
        "ground_truth": "For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION."
    },
    {
        "question": "Please tell us which operators are supported by ENN SDK when developing Image Enhancement.",
        "contexts": [
            [
            "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing segmentation\nAnswer: In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION.",
            "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality."
            ]
        ],
        "answer": "The Image Enhancement category supports the following operators: CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
        "ground_truth": "The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.\n"
    },
    {
        "question": "I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.",
        "contexts": [
            [
            "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
            "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing segmentation\nAnswer: In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU."
            ]
        ],
        "answer": "In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
        "ground_truth": "In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU."
    },
    {
        "question": "I would like to know which operators are supported by the ENN SDK when developing Recommendation System.",
        "contexts": [
            [
            "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing segmentation\nAnswer: In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION.",
            "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE."
            ]
        ],
        "answer": "For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
        "ground_truth": "For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU."
    },
    {
        "question": "I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection",
        "contexts": [
            [
            "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
            "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing segmentation\nAnswer: In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE."
            ]
        ],
        "answer": "In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
        "ground_truth": "In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL."
    },
    {
        "question": "I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.",
        "contexts": [
            [
            "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Image Classification.\nAnswer: For the Image Classification category, the supported operators include CONVOLUTION, DEPTHWISE_CONVOLUTION, FULLY_CONNECTED, SOFTMAX, RELU, AVGPOOL, and BATCH_NORMALIZATION.",
            "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
            "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing segmentation\nAnswer: In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION."
            ]
        ],
        "answer": "For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
        "ground_truth": "For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION."
    },
    {
        "question": "I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.",
        "contexts": [
            [
            "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
            "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing segmentation\nAnswer: In the Segmentation category, the supported operators are CONVOLUTION and CONCATENATION.",
            "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU."
            ]
        ],
        "answer": "In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
        "ground_truth": "In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE."
    },
    {
        "question": "I would like to know examples of applications that can be created using Image Classification.",
        "contexts": [
            [
            "Question: I would like to know examples of applications that can be created using Image Classification.\nAnswer: AI Applications that can be created using Image Classification include Face recognition, Plant disease diagnosis, Product identification.",
            "Question: I would like to know examples of applications that can be created using Image Enhancement.\nAnswer: AI Applications that can be created using Image Enhancement include Restoration of old photos, Clarity improvement in medical imaging, Enhancement of security camera image.",
            "Question: I would like to know examples of applications that can be created using Object Detection.\nAnswer: AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, Wildlife tracking.",
            "Question: I would like to know examples of applications that can be created using Segmentation.\nAnswer: AI Applications that can be created using Segmentation include Medical image analysis, Autonomous vehicle navigation, Agricultural crop monitoring.",
            "Question: I would like to know examples of applications that can be created using Computer Vision for Healthcare.\nAnswer: AI Applications that can be created using Computer Vision for Healthcare include Tumor detection in radiology images, Patient monitoring systems, Surgical assistance.",
            "Question: I would like to know examples of applications that can be created using Anomaly Detection\nAnswer: AI Applications that can be created using Anomaly Detection include Credit card fraud detection, Network security, Predictive maintenance in manufacturing.",
            "Question: I would like to know examples of applications that can be created using Recommendation System.\nAnswer: AI Applications that can be created using Recommendation System include Personalized online shopping, Content recommendation in streaming services, Customized news feed.",
            "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
            "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
            "Question: I would like to know examples of applications that can be created using Augmented and Virtual Reality.\nAnswer: AI Applications that can be created using augmented reality and virtual reality include Virtual training environments, Immersive gaming, Interactive educational contents."
            ]
        ],
        "answer": "AI Applications that can be created using Image Classification include face recognition, plant disease diagnosis, and product identification.",
        "ground_truth": "AI Applications that can be created using Image Classification include Face recognition, Plant disease diagnosis, Product identification."
    },
    {
        "question": "I would like to know examples of applications that can be created using Object Detection.",
        "contexts": [
            [
            "Question: I would like to know examples of applications that can be created using Object Detection.\nAnswer: AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, Wildlife tracking.",
            "Question: I would like to know examples of applications that can be created using Anomaly Detection\nAnswer: AI Applications that can be created using Anomaly Detection include Credit card fraud detection, Network security, Predictive maintenance in manufacturing.",
            "Question: I would like to know examples of applications that can be created using Image Classification.\nAnswer: AI Applications that can be created using Image Classification include Face recognition, Plant disease diagnosis, Product identification.",
            "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
            "Question: I would like to know examples of applications that can be created using Computer Vision for Healthcare.\nAnswer: AI Applications that can be created using Computer Vision for Healthcare include Tumor detection in radiology images, Patient monitoring systems, Surgical assistance.",
            "Question: I would like to know examples of applications that can be created using Segmentation.\nAnswer: AI Applications that can be created using Segmentation include Medical image analysis, Autonomous vehicle navigation, Agricultural crop monitoring.",
            "Question: I would like to know examples of applications that can be created using Image Enhancement.\nAnswer: AI Applications that can be created using Image Enhancement include Restoration of old photos, Clarity improvement in medical imaging, Enhancement of security camera image.",
            "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
            "Question: I would like to know examples of applications that can be created using Recommendation System.\nAnswer: AI Applications that can be created using Recommendation System include Personalized online shopping, Content recommendation in streaming services, Customized news feed.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Object Detection.\nAnswer: The Object Detection category supports operators such as CONVOLUTION, RESHAPE, CONCATENATION, MAXPOOL, and DEPTHWISE_CONVOLUTION."
        ]
    ],
        "answer": "AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, and Wildlife tracking.",
        "ground_truth": "AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, Wildlife tracking."
    },
    {
        "question": "I would like to know examples of applications that can be created using Segmentation.",
        "contexts": [
            [
            "Question: I would like to know examples of applications that can be created using Segmentation.\nAnswer: AI Applications that can be created using Segmentation include Medical image analysis, Autonomous vehicle navigation, Agricultural crop monitoring.",
            "Question: I would like to know examples of applications that can be created using Object Detection.\nAnswer: AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, Wildlife tracking.",
            "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
            "Question: I would like to know examples of applications that can be created using Image Classification.\nAnswer: AI Applications that can be created using Image Classification include Face recognition, Plant disease diagnosis, Product identification.",
            "Question: I would like to know examples of applications that can be created using Image Enhancement.\nAnswer: AI Applications that can be created using Image Enhancement include Restoration of old photos, Clarity improvement in medical imaging, Enhancement of security camera image.",
            "Question: I would like to know examples of applications that can be created using Computer Vision for Healthcare.\nAnswer: AI Applications that can be created using Computer Vision for Healthcare include Tumor detection in radiology images, Patient monitoring systems, Surgical assistance.",
            "Question: I would like to know examples of applications that can be created using Augmented and Virtual Reality.\nAnswer: AI Applications that can be created using augmented reality and virtual reality include Virtual training environments, Immersive gaming, Interactive educational contents.",
            "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
            "Question: I would like to know examples of applications that can be created using Recommendation System.\nAnswer: AI Applications that can be created using Recommendation System include Personalized online shopping, Content recommendation in streaming services, Customized news feed.",
            "Question: I would like to know examples of applications that can be created using Anomaly Detection\nAnswer: AI Applications that can be created using Anomaly Detection include Credit card fraud detection, Network security, Predictive maintenance in manufacturing."
            ]
        ],
        "answer": "AI Applications that can be created using Segmentation include medical image analysis, autonomous vehicle navigation, and agricultural crop monitoring.",
        "ground_truth": "AI Applications that can be created using Segmentation include Medical image analysis, Autonomous vehicle navigation, Agricultural crop monitoring."
    },
    {
        "question": "I would like to know examples of applications that can be created using Pose Estimation.",
        "contexts": [
            [
            "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
            "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
            "Section: ENN SDK Samples\n# ENN SDK Samples\n## Android (Kotlin) Samples\n### Pose Estimation In Android\nContent:\nThis sample application demonstrates the execution of a converted PoseNet model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nThen, it detects the points of a person and overlays the points and edges of a person.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/pose-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory within the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
            "Question: I would like to know examples of applications that can be created using Object Detection.\nAnswer: AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, Wildlife tracking.",
            "Question: I would like to know examples of applications that can be created using Segmentation.\nAnswer: AI Applications that can be created using Segmentation include Medical image analysis, Autonomous vehicle navigation, Agricultural crop monitoring.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Pose Estimation.\nAnswer: For the Pose Estimation category, the supported operators include CONVOLUTION, RELU, and CONCATENATION.",
            "Question: I would like to know examples of applications that can be created using Image Classification.\nAnswer: AI Applications that can be created using Image Classification include Face recognition, Plant disease diagnosis, Product identification.",
            "Question: I would like to know examples of applications that can be created using Augmented and Virtual Reality.\nAnswer: AI Applications that can be created using augmented reality and virtual reality include Virtual training environments, Immersive gaming, Interactive educational contents.",
            "Question: I would like to know examples of applications that can be created using Computer Vision for Healthcare.\nAnswer: AI Applications that can be created using Computer Vision for Healthcare include Tumor detection in radiology images, Patient monitoring systems, Surgical assistance.",
            "Question: I would like to know examples of applications that can be created using Image Enhancement.\nAnswer: AI Applications that can be created using Image Enhancement include Restoration of old photos, Clarity improvement in medical imaging, Enhancement of security camera image."
            ]
        ],
        "answer": "AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation, and Gaming.",
        "ground_truth": "AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming."
    },
    {
        "question": "I would like to know examples of applications that can be created using Image Enhancement.",
        "contexts": [
            [
            "Question: I would like to know examples of applications that can be created using Image Enhancement.\nAnswer: AI Applications that can be created using Image Enhancement include Restoration of old photos, Clarity improvement in medical imaging, Enhancement of security camera image.",
            "Question: I would like to know examples of applications that can be created using Image Classification.\nAnswer: AI Applications that can be created using Image Classification include Face recognition, Plant disease diagnosis, Product identification.",
            "Question: I would like to know examples of applications that can be created using Computer Vision for Healthcare.\nAnswer: AI Applications that can be created using Computer Vision for Healthcare include Tumor detection in radiology images, Patient monitoring systems, Surgical assistance.",
            "Question: Please tell us which operators are supported by ENN SDK when developing Image Enhancement.\nAnswer: The Image Enhancement category supports the operators CONVOLUTION, DEPTHWISE_CONVOLUTION, and RELU.",
            "Question: I would like to know examples of applications that can be created using Segmentation.\nAnswer: AI Applications that can be created using Segmentation include Medical image analysis, Autonomous vehicle navigation, Agricultural crop monitoring.",
            "Question: I would like to know examples of applications that can be created using Object Detection.\nAnswer: AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, Wildlife tracking.",
            "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
            "Section: ENN SDK Samples\n# ENN SDK Samples\n## Android (Kotlin) Samples\n### Image Enhance In Android\nContent:\nThis sample application demonstrates the execution of a converted Zero-DCE model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from an image file and enhances it.\nSpecifically, it takes low-light images and improves their quality.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/image-enhance` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Provide the image data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
            "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
            "Question: I would like to know examples of applications that can be created using Augmented and Virtual Reality.\nAnswer: AI Applications that can be created using augmented reality and virtual reality include Virtual training environments, Immersive gaming, Interactive educational contents."
            ]
        ],
        "answer": "AI Applications that can be created using Image Enhancement include restoration of old photos, clarity improvement in medical imaging, and enhancement of security camera images.",
        "ground_truth": "AI Applications that can be created using Image Enhancement include Restoration of old photos, Clarity improvement in medical imaging, Enhancement of security camera image."
    },
    {
        "question": "I would like to know examples of applications that can be created using Depth Estimation.",
        "contexts": [
            [
            "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
            "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
            "Section: ENN SDK Samples\n# ENN SDK Samples\n## Android (Kotlin) Samples\n### Depth Estimation In Andriod\nContent:\nThis sample application demonstrates the execution of a converted MiDaS V2 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file.\nA color that represents the estimated distance is overlayed on each pixel, thereby providing a visual representation of depth.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/depth-estimation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
            "Question: I would like to know examples of applications that can be created using Object Detection.\nAnswer: AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, Wildlife tracking.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Depth Estimation.\nAnswer: In the Depth Estimation category, the supported operators are CONVOLUTION, CONCATENATION, and RELU.",
            "Question: I would like to know examples of applications that can be created using Segmentation.\nAnswer: AI Applications that can be created using Segmentation include Medical image analysis, Autonomous vehicle navigation, Agricultural crop monitoring.",
            "Question: I would like to know examples of applications that can be created using Image Enhancement.\nAnswer: AI Applications that can be created using Image Enhancement include Restoration of old photos, Clarity improvement in medical imaging, Enhancement of security camera image.",
            "Question: I would like to know examples of applications that can be created using Augmented and Virtual Reality.\nAnswer: AI Applications that can be created using augmented reality and virtual reality include Virtual training environments, Immersive gaming, Interactive educational contents.",
            "Question: I would like to know examples of applications that can be created using Computer Vision for Healthcare.\nAnswer: AI Applications that can be created using Computer Vision for Healthcare include Tumor detection in radiology images, Patient monitoring systems, Surgical assistance.",
            "Section: ENN SDK Samples\n# ENN SDK Samples\n## Introduction\nContent:\n|Sample Name|Description|\n|-------------|-------|\n|Image Classification In Android| Sample Android application to demonstrate the execution of `Inception v4` model with ENN SDK|\n|Object Detection In Android| Sample Android application to demonstrate the execution of `YOLOv5` model with ENN SDK|\n|Segmentation In Android| Sample Android application to demonstrate the execution of `DeeplabV3` model with ENN SDK|\n|Pose Estimation In Android| Sample Android application to demonstrate the execution of `PoseNet` model with ENN SDK|\n|Image Enhance In Android| Sample Android application to demonstrate the execution of `Zero-DCE` model with ENN SDK|\n|Depth Estimation In Andriod| Sample Android application to demonstrate the execution of `MiDaS v2` model with ENN SDK|\n|Performance Comparison| Sample Android application to demonstrate the difference between ENN SDK and TFLite |\n|NNC Model Tester|Sample C++ program to demonstrate the execution of NNC model with ENN SDK|"
            ]
        ],
        "answer": "AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, and Robot navigation.",
        "ground_truth": "AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation."
    },
    {
        "question": "I would like to know examples of applications that can be created using Recommendation System.",
        "contexts": [
            [
            "Question: I would like to know examples of applications that can be created using Recommendation System.\nAnswer: AI Applications that can be created using Recommendation System include Personalized online shopping, Content recommendation in streaming services, Customized news feed.",
            "Question: I would like to know examples of applications that can be created using Anomaly Detection\nAnswer: AI Applications that can be created using Anomaly Detection include Credit card fraud detection, Network security, Predictive maintenance in manufacturing.",
            "Question: I would like to know examples of applications that can be created using Segmentation.\nAnswer: AI Applications that can be created using Segmentation include Medical image analysis, Autonomous vehicle navigation, Agricultural crop monitoring.",
            "Question: I would like to know examples of applications that can be created using Object Detection.\nAnswer: AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, Wildlife tracking.",
            "Question: I would like to know examples of applications that can be created using Image Classification.\nAnswer: AI Applications that can be created using Image Classification include Face recognition, Plant disease diagnosis, Product identification.",
            "Question: I would like to know examples of applications that can be created using Computer Vision for Healthcare.\nAnswer: AI Applications that can be created using Computer Vision for Healthcare include Tumor detection in radiology images, Patient monitoring systems, Surgical assistance.",
            "Question: I would like to know examples of applications that can be created using Augmented and Virtual Reality.\nAnswer: AI Applications that can be created using augmented reality and virtual reality include Virtual training environments, Immersive gaming, Interactive educational contents.",
            "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
            "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Recommendation System.\nAnswer: For the Recommendation Systems category, the supported operators include FULLY_CONNECTED, ADD, CONCATENATION, and RELU."
            ]
        ],
        "answer": "AI Applications that can be created using Recommendation System include personalized online shopping, content recommendation in streaming services, and customized news feeds.",
        "ground_truth": "AI Applications that can be created using Recommendation System include Personalized online shopping, Content recommendation in streaming services, Customized news feed."
    },
    {
        "question": "I would like to know examples of applications that can be created using Anomaly Detection",
        "contexts": [
            [
            "Question: I would like to know examples of applications that can be created using Anomaly Detection\nAnswer: AI Applications that can be created using Anomaly Detection include Credit card fraud detection, Network security, Predictive maintenance in manufacturing.",
            "Question: I would like to know examples of applications that can be created using Object Detection.\nAnswer: AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, Wildlife tracking.",
            "Question: I would like to know examples of applications that can be created using Segmentation.\nAnswer: AI Applications that can be created using Segmentation include Medical image analysis, Autonomous vehicle navigation, Agricultural crop monitoring.",
            "Question: I would like to know examples of applications that can be created using Image Classification.\nAnswer: AI Applications that can be created using Image Classification include Face recognition, Plant disease diagnosis, Product identification.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Anomaly Detection\nAnswer: In the Anomaly Detection category, the supported operators are DENSE, RELU, SOFTMAX, SUB, and MUL.",
            "Question: I would like to know examples of applications that can be created using Recommendation System.\nAnswer: AI Applications that can be created using Recommendation System include Personalized online shopping, Content recommendation in streaming services, Customized news feed.",
            "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
            "Question: I would like to know examples of applications that can be created using Computer Vision for Healthcare.\nAnswer: AI Applications that can be created using Computer Vision for Healthcare include Tumor detection in radiology images, Patient monitoring systems, Surgical assistance.",
            "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
            "Question: I would like to know examples of applications that can be created using Augmented and Virtual Reality.\nAnswer: AI Applications that can be created using augmented reality and virtual reality include Virtual training environments, Immersive gaming, Interactive educational contents."
            ]
        ],
        "answer": "AI Applications that can be created using Anomaly Detection include credit card fraud detection, network security, and predictive maintenance in manufacturing.",
        "ground_truth": "AI Applications that can be created using Anomaly Detection include Credit card fraud detection, Network security, Predictive maintenance in manufacturing."
    },
    {
        "question": "I would like to know examples of applications that can be created using Computer Vision for Healthcare.",
        "contexts": [
            [
            "Question: I would like to know examples of applications that can be created using Computer Vision for Healthcare.\nAnswer: AI Applications that can be created using Computer Vision for Healthcare include Tumor detection in radiology images, Patient monitoring systems, Surgical assistance.",
            "Question: I would like to know examples of applications that can be created using Segmentation.\nAnswer: AI Applications that can be created using Segmentation include Medical image analysis, Autonomous vehicle navigation, Agricultural crop monitoring.",
            "Question: I would like to know examples of applications that can be created using Image Classification.\nAnswer: AI Applications that can be created using Image Classification include Face recognition, Plant disease diagnosis, Product identification.",
            "Question: I would like to know examples of applications that can be created using Object Detection.\nAnswer: AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, Wildlife tracking.",
            "Question: I would like to know examples of applications that can be created using Image Enhancement.\nAnswer: AI Applications that can be created using Image Enhancement include Restoration of old photos, Clarity improvement in medical imaging, Enhancement of security camera image.",
            "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
            "Question: I would like to know examples of applications that can be created using Augmented and Virtual Reality.\nAnswer: AI Applications that can be created using augmented reality and virtual reality include Virtual training environments, Immersive gaming, Interactive educational contents.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Computer Vision for Healthcare.\nAnswer: For the Computer Vision for Healthcare category, the supported operators include CONVOLUTION, MAXPOOL, AVGPOOL, RESHAPE, and CONCATENATION.",
            "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
            "Question: I would like to know examples of applications that can be created using Anomaly Detection\nAnswer: AI Applications that can be created using Anomaly Detection include Credit card fraud detection, Network security, Predictive maintenance in manufacturing."
            ]
        ],
        "answer": "AI Applications that can be created using Computer Vision for Healthcare include Tumor detection in radiology images, Patient monitoring systems, and Surgical assistance.",
        "ground_truth": "AI Applications that can be created using Computer Vision for Healthcare include Tumor detection in radiology images, Patient monitoring systems, Surgical assistance."
    },
    {
        "question": "I would like to know examples of applications that can be created using Augmented and Virtual Reality.",
        "contexts": [
            [
            "Question: I would like to know examples of applications that can be created using Augmented and Virtual Reality.\nAnswer: AI Applications that can be created using augmented reality and virtual reality include Virtual training environments, Immersive gaming, Interactive educational contents.",
            "Question: I would like to know examples of applications that can be created using Depth Estimation.\nAnswer: AI Applications that can be created using Depth Estimation include 3D modeling, Augmented reality AI Applications, Robot navigation.",
            "Question: I would like to know examples of applications that can be created using Computer Vision for Healthcare.\nAnswer: AI Applications that can be created using Computer Vision for Healthcare include Tumor detection in radiology images, Patient monitoring systems, Surgical assistance.",
            "Question: I would like to know examples of applications that can be created using Pose Estimation.\nAnswer: AI Applications that can be created using Pose Estimation include Sports performance analysis, Physical therapy, Animation and Gaming.",
            "Question: I would like to know examples of applications that can be created using Segmentation.\nAnswer: AI Applications that can be created using Segmentation include Medical image analysis, Autonomous vehicle navigation, Agricultural crop monitoring.",
            "Question: I would like to know examples of applications that can be created using Object Detection.\nAnswer: AI Applications that can be created using Object Detection include Traffic monitoring, Security surveillance, Wildlife tracking.",
            "Question: I would like to know which operators are supported by the ENN SDK when developing Augmented and Virtual Reality.\nAnswer: In the Augmented and Virtual Reality category, the supported operators are CONVOLUTION, DEPTHWISE_CONVOLUTION, and RESHAPE.",
            "Question: I would like to know examples of applications that can be created using Recommendation System.\nAnswer: AI Applications that can be created using Recommendation System include Personalized online shopping, Content recommendation in streaming services, Customized news feed.",
            "Question: I would like to know examples of applications that can be created using Image Enhancement.\nAnswer: AI Applications that can be created using Image Enhancement include Restoration of old photos, Clarity improvement in medical imaging, Enhancement of security camera image.",
            "Question: I would like to know examples of applications that can be created using Anomaly Detection\nAnswer: AI Applications that can be created using Anomaly Detection include Credit card fraud detection, Network security, Predictive maintenance in manufacturing."
            ]
        ],
        "answer": "AI Applications that can be created using augmented reality and virtual reality include:\n\n- Virtual training environments\n- Immersive gaming\n- Interactive educational contents",
        "ground_truth": "AI Applications that can be created using augmented reality and virtual reality include Virtual training environments, Immersive gaming, Interactive educational contents."
    },
    {
        "question": "Please tell us about the AI Challenger.",
        "contexts": [
            [
            "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
            "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
            "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
            "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
            "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
            "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
            "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
            "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
            "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
            "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK."
            ]
        ],
        "answer": "The AI Challenger is an open innovation type of developer program that utilizes and develops AI SDK using Samsung S.LSI Exynos ENN SDK. It is a key part of the SEED AI initiative and is dedicated to driving technological innovation through artificial intelligence. The program focuses on facilitating AI model creation and optimization, supporting a wide range of AI applications, from image recognition to complex predictive analytics. By providing resources and tools, the AI Challenger nurtures a vibrant community of AI developers and enhances their capabilities in crafting impactful AI solutions across various sectors.",
        "ground_truth": "Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress."
    },
    {
        "question": "I would like to know the AI Challenger schedule.",
        "contexts": [
            [
            "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
            "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
            "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
            "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
            "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
            "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
            "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
            "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
            "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
            "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK."
            ]
        ],
        "answer": "The AI Challenger unfolds through three primary stages:\n\n1. **Stage I**: Participants are familiarized with the program's structure, resources, and tools such as the ENN SDK.\n2. **Stage II**: This phase focuses on the active phase of research and development, providing comprehensive technical support and coaching as participants progress through their projects.\n3. **Stage III**: The final stage is dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
        "ground_truth": "The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day."
    },
    {
        "question": "I would like to know the ENN SDK support models and environments.",
        "contexts": [
            [
            "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
            "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
            "Section: Support Matrix\n# Support Matrix\nContent:\nThe support matrices provide information about the models that are compatible with ENN SDK.",
            "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
            "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\n## 2. Features\nContent:\n### Workflow of ENN SDK\nUsing ENN SDK involves the following two steps:\n1. The user converts NN models to NNC models.\n\tNNC is an NN model format that can run efficiently in Samsung Exynos hardware.\n2. The user executes the converted model for inference.\n#### Model Conversion\nUse one of the tools that is provided to convert NN models.\nTo convert a model:\n1. Prepare a pre-trained NN model.\n2. Set parameters for tools.\n3. Execute tools for conversion.\n#### Model Execution\nExecuting converted models is performed by the ENN framework.\nWhen using the ENN framework:\n1. Initialize ENN framework.\n2. Load the converted model to ENN framework.\n3. Allocate and commit all the necessary buffers for the model.\nThen:\n1. Copy input data to input buffers.\n2. Execute model on ENN framework.\n3. Use data on output buffers.\n\t> To execute the model multiple times, repeat this process.\nFinally, perform the following steps:\n1. Uncommit and release buffers allocated to the model.\n2. Unload the model.\n3. De initialize ENN framework.\nENN framework APIs support language binding for C++.\n### Supported Neural Network Models\nENN SDK supports the following NN models:\n- TensorFlow Lite\n- Tensors with up to four dimensions\n- Models with a maximum size of 1 GB\nFor more information on supported TensorFlow Lite operations, refer to the Support Matrix.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\n## 2. Features\nContent:\nThis chapter provides a general overview of the features that are provided by ENN SDK.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
            "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\n## 3. Tools\n### ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model."
            ]
        ],
        "answer": "The ENN SDK primarily supports TensorFlow Lite models, requiring conversion through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher. The compatible models must have up to four dimensions and a maximum size of 1 GB. For Neural Processing Unit (NPU) utilization, quantization is mandatory, and model distillation is recommended for optimal performance. The SDK is designed to offer flexibility and comprehensive support across various AI applications, making it adaptable for different usage scenarios.",
        "ground_truth": "The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios."
    },
    {
        "question": "I want to know the planning intention of AI Challenger.",
        "contexts": [
            [
            "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
            "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
            "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
            "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
            "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
            "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
            "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
            "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
            "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
            "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK."
            ]
        ],
        "answer": "The planning intention of AI Challenger is to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program's objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
        "ground_truth": "Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions."
    },
    {
        "question": "I would like to know the device specifications provided by 1st AI Challenger.",
        "contexts": [
            [
            "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
            "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
            "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
            "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
            "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
            "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
            "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
            "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
            "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
            "Question: I would like to know the advantages of Exynos 2200, the base chipset of the reference device provided by First AI Challenger managers.\nAnswer: Console quality graphics now on mobile with the Exynos 2200 mobile processor. The Samsung Xclipse GPU sets to usher in a new era and completely change the way we experience mobile gaming. Playtime is well and truly over."
            ]
        ],
        "answer": "The device features the Exynos 2200 chipset, which includes a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, along with the Samsung Xclipse 920 GPU. It has an AI Engine that incorporates a Dual-core NPU and DSP. In terms of camera capabilities, it supports up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. For video performance, it allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, as well as 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
        "ground_truth": "The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz."
    },
    {
        "question": "I would like to know the advantages of Exynos 2200, the base chipset of the reference device provided by First AI Challenger managers.",
        "contexts": [
            [
            "Question: I would like to know the advantages of Exynos 2200, the base chipset of the reference device provided by First AI Challenger managers.\nAnswer: Console quality graphics now on mobile with the Exynos 2200 mobile processor. The Samsung Xclipse GPU sets to usher in a new era and completely change the way we experience mobile gaming. Playtime is well and truly over.",
            "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
            "Question: I want to know the main summary of Samsung Exynos 2200.\nAnswer: Samsung Electronics introduces the Exynos 2200, a premium mobile processor featuring the Samsung Xclipse GPU based on AMD RDNA 2 architecture, crafted using a 4-nanometer EUV process. This processor enhances mobile gaming, social media, and photography, offering the first mobile hardware-accelerated ray tracing for realistic lighting and variable rate shading for smoother gameplay. It boasts an upgraded neural processing unit (NPU), Arm's latest Armv9 CPU cores for superior performance and security, and a fast 5G modem compliant with 3GPP Release 16. With integrated Secure Element (iSE) and robust encryption, it ensures data security. The processor's ISP supports up to 200MP resolution, AI-integrated camera functionalities for professional-quality images, an advanced codec for up to 8K video, and a display solution with HDR10+ and up to 144Hz refresh rates, ensuring a comprehensive and immersive visual experience.",
            "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
            "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
            "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program.",
            "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
            "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
            "Question: I want to know what types of smartphones were mass-produced based on the Exynos 2200.\nAnswer: Smartphones mass-produced based on the Exynos 2200 chipset include Samsung Galaxy S22 Ultra, Samsung Galaxy S22+, and Samsung Galaxy S22.",
            "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor."
            ]
        ],
        "answer": "The Exynos 2200 mobile processor offers console-quality graphics on mobile devices, driven by the Samsung Xclipse GPU, which is designed to revolutionize the mobile gaming experience. This chipset enhances gaming performance significantly, making it a key advantage for applications requiring high graphics fidelity.",
        "ground_truth": "Console quality graphics now on mobile with the Exynos 2200 mobile processor. The Samsung Xclipse GPU sets to usher in a new era and completely change the way we experience mobile gaming. Playtime is well and truly over."
    },
    {
        "question": "I want to know what types of smartphones were mass-produced based on the Exynos 2200.",
        "contexts": [
            [
            "Question: I want to know what types of smartphones were mass-produced based on the Exynos 2200.\nAnswer: Smartphones mass-produced based on the Exynos 2200 chipset include Samsung Galaxy S22 Ultra, Samsung Galaxy S22+, and Samsung Galaxy S22.",
            "Question: I want to know the main summary of Samsung Exynos 2200.\nAnswer: Samsung Electronics introduces the Exynos 2200, a premium mobile processor featuring the Samsung Xclipse GPU based on AMD RDNA 2 architecture, crafted using a 4-nanometer EUV process. This processor enhances mobile gaming, social media, and photography, offering the first mobile hardware-accelerated ray tracing for realistic lighting and variable rate shading for smoother gameplay. It boasts an upgraded neural processing unit (NPU), Arm's latest Armv9 CPU cores for superior performance and security, and a fast 5G modem compliant with 3GPP Release 16. With integrated Secure Element (iSE) and robust encryption, it ensures data security. The processor's ISP supports up to 200MP resolution, AI-integrated camera functionalities for professional-quality images, an advanced codec for up to 8K video, and a display solution with HDR10+ and up to 144Hz refresh rates, ensuring a comprehensive and immersive visual experience.",
            "Question: I would like to know the advantages of Exynos 2200, the base chipset of the reference device provided by First AI Challenger managers.\nAnswer: Console quality graphics now on mobile with the Exynos 2200 mobile processor. The Samsung Xclipse GPU sets to usher in a new era and completely change the way we experience mobile gaming. Playtime is well and truly over.",
            "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
            "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
            "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
            "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
            "Section: Exynos AI Studio User Guide (TBD)\nContent:\n# Introduction to Exynos AI High-Level Toolchain (EHT)\nEHT** is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models.\n## System overview diagram",
            "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
            "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress."
            ]
        ],
        "answer": "Smartphones mass-produced based on the Exynos 2200 chipset include the Samsung Galaxy S22 Ultra, Samsung Galaxy S22+, and Samsung Galaxy S22.",
        "ground_truth": "Smartphones mass-produced based on the Exynos 2200 chipset include Samsung Galaxy S22 Ultra, Samsung Galaxy S22+, and Samsung Galaxy S22."
    },
    {
        "question": "I want to know the main summary of Samsung Exynos 2200.",
        "contexts": [
            [
            "Question: I want to know the main summary of Samsung Exynos 2200.\nAnswer: Samsung Electronics introduces the Exynos 2200, a premium mobile processor featuring the Samsung Xclipse GPU based on AMD RDNA 2 architecture, crafted using a 4-nanometer EUV process. This processor enhances mobile gaming, social media, and photography, offering the first mobile hardware-accelerated ray tracing for realistic lighting and variable rate shading for smoother gameplay. It boasts an upgraded neural processing unit (NPU), Arm's latest Armv9 CPU cores for superior performance and security, and a fast 5G modem compliant with 3GPP Release 16. With integrated Secure Element (iSE) and robust encryption, it ensures data security. The processor's ISP supports up to 200MP resolution, AI-integrated camera functionalities for professional-quality images, an advanced codec for up to 8K video, and a display solution with HDR10+ and up to 144Hz refresh rates, ensuring a comprehensive and immersive visual experience.",
            "Question: I would like to know the advantages of Exynos 2200, the base chipset of the reference device provided by First AI Challenger managers.\nAnswer: Console quality graphics now on mobile with the Exynos 2200 mobile processor. The Samsung Xclipse GPU sets to usher in a new era and completely change the way we experience mobile gaming. Playtime is well and truly over.",
            "Question: I want to know what types of smartphones were mass-produced based on the Exynos 2200.\nAnswer: Smartphones mass-produced based on the Exynos 2200 chipset include Samsung Galaxy S22 Ultra, Samsung Galaxy S22+, and Samsung Galaxy S22.",
            "Question: Please tell us about the ENN SDK.\nAnswer: ENN SDK is a software development kit for the Exynos chipset developed by Samsung S.LSI. It is designed to optimize and implement AI and machine learning models on Exynos-based device. Key features include model optimization for Exynos chipsets, support for various AI algorithms, and user-friendly interface improvement. It is primarily aimed at use on mobile devices and aims to provide developers with efficient tools through continuous updates and improvement.",
            "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
            "Question: I want to know about ENN SDK.\nAnswer: The ENN SDK (Exynos Neural Network Software Development Kit) is a software development kit developed by Samsung S.LSI for its Exynos chipset. It is specifically designed to optimize and implement AI and machine learning models on Exynos-based device. The SDK facilitates the conversion and optimization of AI models to be compatible and efficient on devices powered by Exynos processor.",
            "Section: Exynos AI Studio User Guide (TBD)\nContent:\n# Introduction to Exynos AI High-Level Toolchain (EHT)\nEHT** is a software that provides optimization techniques for neural networks, including computer vision (CV) models, large language models (LLM), and large vision models (LVM). This software takes models as input and offers functionalities such as quantization and model optimization to generate SNC models.\n## System overview diagram",
            "Question: Please tell us about your future goals for EDS Assistant?\nAnswer: EDS Assistant will evolve to provide information on technical issues, product information, events and announcements related to Samsug S.LSI Exynos products in addition to AI including Samsung S.LSI ENN SDK.",
            "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
            "Question: Please tell us about the advantages of ENN SDK compared to other SDKs?\nAnswer: The AI Toolset provided by each chipset is optimized for each chipset. ENN SDK is also optimized for Exynos chipset. Additionally, it is also used as an optimization tool for Samsung mobile device. Additionally, I think it will be a great advantage for developers to be able to reflect the voices of domestic developers and have the opportunity to participate in development like this program."
            ]
        ],
        "answer": "Samsung Electronics introduces the Exynos 2200, a premium mobile processor featuring the Samsung Xclipse GPU based on AMD RDNA 2 architecture, crafted using a 4-nanometer EUV process. This processor enhances mobile gaming, social media, and photography, offering the first mobile hardware-accelerated ray tracing for realistic lighting and variable rate shading for smoother gameplay. It boasts an upgraded neural processing unit (NPU), Arm's latest Armv9 CPU cores for superior performance and security, and a fast 5G modem compliant with 3GPP Release 16. With integrated Secure Element (iSE) and robust encryption, it ensures data security. The processor's ISP supports up to 200MP resolution, AI-integrated camera functionalities for professional-quality images, an advanced codec for up to 8K video, and a display solution with HDR10+ and up to 144Hz refresh rates, ensuring a comprehensive and immersive visual experience.",
        "ground_truth": "Samsung Electronics introduces the Exynos 2200, a premium mobile processor featuring the Samsung Xclipse GPU based on AMD RDNA 2 architecture, crafted using a 4-nanometer EUV process. This processor enhances mobile gaming, social media, and photography, offering the first mobile hardware-accelerated ray tracing for realistic lighting and variable rate shading for smoother gameplay. It boasts an upgraded neural processing unit (NPU), Arm's latest Armv9 CPU cores for superior performance and security, and a fast 5G modem compliant with 3GPP Release 16. With integrated Secure Element (iSE) and robust encryption, it ensures data security. The processor's ISP supports up to 200MP resolution, AI-integrated camera functionalities for professional-quality images, an advanced codec for up to 8K video, and a display solution with HDR10+ and up to 144Hz refresh rates, ensuring a comprehensive and immersive visual experience."
    },
    {
        "question": "NPU, particularly the hardware accelerators related to neural networks, are commonly structured as Systolic arrays. I'm not sure if this is possible, but is there a way to control the NPU resources we use through programming before compiling in 1st AI Challenger?",
        "contexts": [
            [
            "Question: NPU, particularly the hardware accelerators related to neural networks, are commonly structured as Systolic arrays. I'm not sure if this is possible, but is there a way to control the NPU resources we use through programming before compiling in 1st AI Challenger?\nAnswer: Unfortunately, controlling NPU resources directly through pre-compilation programming is not supported in the current framework. This limitation is primarily due to the intricate nature of NPU architectures and the complexity of directly managing hardware-level functions. NPUs are designed to operate with a high level of efficiency for specific neural network tasks, and allowing direct control over their resources could potentially compromise their performance and the overall system stability. We understand that this might be an area of interest for in-depth exploration and appreciate your understanding of the constraints within the current technological setup.",
            "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
            "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
            "Section: Optimizer\n# Optimizer\nContent:\nModels are trained and generated on floating-point hardware like CPUs and GPUs. These devices generally perform well, but not for chips due to hardware constraints. Optimizer provides optimization methods for these models to perform best on Exynos chips. Optimizer supports three powerful features.\n- shape_inference<br>\n\tIf the input shape is not specified or need to be changed, shape_inference allows the user to define/modify it.\n- 4dim conversion<br>\n\tIn order for the .nnc extension model to operate on a device, the input and output shapes of all operators in the model must be in four dimensions.",
            "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
            "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
            "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
            "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
            "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\n## 3. Tools\n### ENN SDK Service\nContent:\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required."
            ]
        ],
        "answer": "Unfortunately, controlling NPU resources directly through pre-compilation programming is not supported in the current framework. This limitation is primarily due to the intricate nature of NPU architectures and the complexity of directly managing hardware-level functions. NPUs are designed to operate with a high level of efficiency for specific neural network tasks, and allowing direct control over their resources could potentially compromise their performance and the overall system stability. We appreciate your understanding of the constraints within the current technological setup.",
        "ground_truth": "Unfortunately, controlling NPU resources directly through pre-compilation programming is not supported in the current framework. This limitation is primarily due to the intricate nature of NPU architectures and the complexity of directly managing hardware-level functions. NPUs are designed to operate with a high level of efficiency for specific neural network tasks, and allowing direct control over their resources could potentially compromise their performance and the overall system stability. We understand that this might be an area of interest for in-depth exploration and appreciate your understanding of the constraints within the current technological setup."
    },
    {
        "question": "I need to perform a Fourier Transform. Can you tell me if this is supported as an IP (Intellectual Property) function in the SoC (System on Chip) in 1st AI Challenger, and if so, which function should I call? If it's not supported, would we need to manually implement it ourselves?",
        "contexts": [
            [
            "Question: I need to perform a Fourier Transform. Can you tell me if this is supported as an IP (Intellectual Property) function in the SoC (System on Chip) in 1st AI Challenger, and if so, which function should I call? If it's not supported, would we need to manually implement it ourselves?\nAnswer: Unfortunately, there is no native library support for the Fourier Transform within our System on Chip (SoC). You would need to either use a 3rd party library or manually implement the Fourier Transform functionality. Additionally, please note that the Exynos Reference Design (ERD) provided for this iteration does not support the Transformer architecture. We recommend exploring external libraries or considering a custom implementation to meet your project's specific needs.",
            "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
            "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
            "Question: NPU, particularly the hardware accelerators related to neural networks, are commonly structured as Systolic arrays. I'm not sure if this is possible, but is there a way to control the NPU resources we use through programming before compiling in 1st AI Challenger?\nAnswer: Unfortunately, controlling NPU resources directly through pre-compilation programming is not supported in the current framework. This limitation is primarily due to the intricate nature of NPU architectures and the complexity of directly managing hardware-level functions. NPUs are designed to operate with a high level of efficiency for specific neural network tasks, and allowing direct control over their resources could potentially compromise their performance and the overall system stability. We understand that this might be an area of interest for in-depth exploration and appreciate your understanding of the constraints within the current technological setup.",
            "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
            "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
            "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
            "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
            "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
            "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions."
            ]
        ],
        "answer": "Unfortunately, there is no native library support for the Fourier Transform within the System on Chip (SoC) for the 1st AI Challenger. You would need to either use a 3rd party library or manually implement the Fourier Transform functionality.",
        "ground_truth": "Unfortunately, there is no native library support for the Fourier Transform within our System on Chip (SoC). You would need to either use a 3rd party library or manually implement the Fourier Transform functionality. Additionally, please note that the Exynos Reference Design (ERD) provided for this iteration does not support the Transformer architecture. We recommend exploring external libraries or considering a custom implementation to meet your project's specific needs."
    },
    {
        "question": "We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?",
        "contexts": [
            [
            "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
            "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
            "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
            "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
            "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
            "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
            "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
            "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
            "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
            "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger."
            ]
        ],
        "answer": "The OpenPOSE model you want to use is supported by the ENN SDK in the 1st AI Challenger. However, please note that there are many versions and types of OpenPOSE. If you encounter any problems during your application, feel free to contact us for assistance. Unfortunately, KoBERT is not supported.",
        "ground_truth": "The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported."
    },
    {
        "question": "I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?",
        "contexts": [
            [
            "Question: I am currently researching EO/IR Fusion Object Detection and using YOLO-based models (yolov5, yolov8). I am also conducting research using the MMdetection open-source library for object detection. I intend to undertake an OCR (Optical Character Recognition) project as part of the Exynos AI Challenger. Is support available for this kind of project in the 1st AI Challenger?\nAnswer: In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
            "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
            "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
            "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
            "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
            "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
            "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
            "Section: ENN SDK Samples\n# ENN SDK Samples\n## Android (Kotlin) Samples\n### Object Detection In Android\nContent:\nThis sample application demonstrates the execution of a converted YOLOv5 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and identifies the object within the input.\nA bounding box is drawn around the detected item, and the label and score that are associated with the object are displayed.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/object-detection` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Copy the corresponding label text file to the `assets` directory.\n3. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n4. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
            "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
            "Question: I need to perform a Fourier Transform. Can you tell me if this is supported as an IP (Intellectual Property) function in the SoC (System on Chip) in 1st AI Challenger, and if so, which function should I call? If it's not supported, would we need to manually implement it ourselves?\nAnswer: Unfortunately, there is no native library support for the Fourier Transform within our System on Chip (SoC). You would need to either use a 3rd party library or manually implement the Fourier Transform functionality. Additionally, please note that the Exynos Reference Design (ERD) provided for this iteration does not support the Transformer architecture. We recommend exploring external libraries or considering a custom implementation to meet your project's specific needs."
            ]
        ],
        "answer": "In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities.",
        "ground_truth": "In response to your inquiry, I regret to inform you that the YOLO-based models you intend to use (yolov5, yolov8), known to be 5-dimensional, are not supported. Please refer to the list of supported Operators list on our website for details about these limitations. Additionally, the MMdetection library you mentioned is not supported, and a manual implementation would be required. We understand this might not be the answer you were hoping for and appreciate your understanding of the constraints within our current system's capabilities."
    },
    {
        "question": "I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?",
        "contexts": [
            [
            "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
            "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
            "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
            "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
            "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
            "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
            "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
            "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
            "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
            "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions."
            ]
        ],
        "answer": "Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF (Neural Radiance Fields) for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
        "ground_truth": "Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have."
    },
    {
        "question": "How do I post an article in the Best Lab?",
        "contexts": [
            [
            "Question: How do I post an article in the Best Lab?\nAnswer: Best Lab is a dedicated platform where administrators meticulously select and upload projects that demonstrate excellence and have high potential for development. Currently, up to six projects are featured, receiving special attention. There is no separate process for individuals to submit their projects directly to Best Lab. Instead, administrators personally curate and showcase projects on the platform.",
            "Section: Quick Start\n# Quick Start\nContent:\n## Run docker container\n```bash\ndocker run -it --gpus all --name eht_container \\\n-v {LOCAL_DATASET_DIR}:{CONTAINER_DATASET_DIR} \\\neht:0.1.0\n```\n## Optimize sample model\n```bash\npython3 api.py -f /home/quick_start.yaml\n```\n```yaml\ninput_model_path: /home/sample/models/sample_add_constant.onnx\noutput_folder_path: /home/results_sample\ninput_model_format: onnx\nmodel_type: CV\nquantizer:\n\t\tprecision_weight: 8\n\t\tprecision_activation: 8\n\t\tcalibration_data_path: /home/sample/datasets/add_constant\n\t\tuse_cuda: True\nsimulator:\n\t\tmetric: snr\n\t\tthreshold: 100\n\t\tinput_data_path:\n\t\t\t/home/sample/datasets/add_constant\noptimizer:\n\t\tskip_4_dim_conversion: False\n\t\tcustom_template_path:\n\t\t\t\t/usr/local/lib/python3.10/dist-packages/optimizer/core/templates/fuse_math_or_replace_DWConv.py:\n\t\t\t\t- TemplateFuseMath\n```",
            "Section: ENN SDK Samples\n# ENN SDK Samples\n## Android (Kotlin) Samples\n### Segmentation In Android\nContent:\nThis sample application demonstrates the execution of a converted DeeplabV3 model using the ENN framework.\nThe model is converted using ENN SDK service with the **Default** hardware type option.\n#### Functionality\nThe application accepts input from a camera feed or an image file and segmentation objects within the input.\nEach pixel of the segmented object is overlayed with a color corresponding to its label, thereby providing a visual representation of the classification.\nAdditionally, the inference time is displayed at the bottom of the application interface.\n#### Location\nThe sample is available in the `enn-sdk-samples-9925/segmentation` directory within the Github repository.\n#### Getting Started\nTo utilize the sample application:\n1. Download or clone the sample application from the Github repository.\n2. Open the sample application project in Android Studio.\n3. Connect the ERD board to the computer.\n4. Run the application (using Shift + F10).\n5. Select Camera or Image mode and provide the data for inference.\nTo modify the model used in the sample application:\n1. Copy the desired model file to the `assets` directory of the project.\n2. Modify the parameters in the ModelConstants.kt file to reflect the specifications of the new model.\n3. If the inputs and outputs of the model differ from the pre-designed sample application, modify the `preProcess()` and `postProcess()` functions.",
            "Section: Getting Started with Android Samples\n# Getting Started With Android Samples\n## Implementing Sample Application\nContent:\n```mermaid\ngraph TB\nA[Start]\nsubgraph B[Starting Android Project]\nB1[Install Android Studio] --> B2[Create a New Android Project]\nB2 --> B3[Connect the ERD Board to Android Studio]\nend\nsubgraph C[Setting Necessary UI]\nC1[Set up Android UI View Components]\nend\nsubgraph D[Function Implementation]\nD1[Implement UI Functions]\nD1 --> D2[Implement Listener]\nD2 --> D3[Implement Data Processing Functions]\nend\nsubgraph E[ENNTools]\nE1[Use ENNTools to Convert the TFLite Model to the NNC Model]\nend\nsubgraph F[ENN Framework]\nF1[Implement the ENN Framework in the Sample Application]\nend\nG[End]\nA --> B\nB --> C\nC --> D\nD --> E\nE --> F\nF --> G\n```",
            "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
            "Section: Getting Started with Native Samples\n# Getting Started with Native Samples\nContent:\n## Implementing Native Program\nThis guide comprises the following sections:\n1. **Writing Native Program**: This section provides the process of writing a C++ program to implement the ENN framework.\n2. **Compiling Using NDK**: This section provides the step-by-step process to compile the native program using NDK.\n3. **Using ADB to Execute Native Program**: This section explains the method to execute the native program using ADB.\nThe general workflow of writing and executing a native program using the ENN SDK is described in the following flowchart.\n```mermaid\ngraph TB\nA[Start]\nB[Write Native Program]\nsubgraph C[Android Native Development Kit]\n\tC1[Create Makefile]-->C2[Compile Native Program]\nend\nsubgraph D[Android Debug Bridge]\n\tD1[Push Native Program and Data]--> D2[Execute Native Program]\nend\nE[End]\nA-->B-->C-->D-->E\n```\n## Glossary\n|Terms|Expanded Form|\n|-|-|\n|ENN SDK|Exynos Neural Network Software Development Kit|\n|ERD Board|Exynos Reference Design Board|\n|NN|Neural Network|\n|NNC|Neural Network Container|\n|TFLite|TensorFlow Lite|",
            "Section: Frontend API\n# **Frontend API**\n## **How to use optimization API**\nContent:\n\t\t- TemplateReplaceEltwise\n\t\t/usr/local/lib/python3.10/dist-packages/optimizer/core/templates/insert_dwconv_for_act_fold.py:\n\t\t- TemplateInsertDWConvForActFold\n\t\t/usr/local/lib/python3.10/dist-packages/optimizer/core/templates/fuse_deconv_bias.py:\n\t\t- TemplateFuseDeconvBias\n```\n**LVM sample yaml**\n```yaml\ninput_model_path: {INPUT_MODEL_PATH}\noutput_folder_path: {OUTPUT_FOLDER_PATH}\nmodel_type: LVM\ninput_model_format : onnx\nquantizer:\n\tprecision_weight: 8\n\tprecision_activation: 8\n\talpha: 1.0\n\tmpq_operator_dict:\n\t\tSoftmax: int16\n\tcalibration_data_path: {CALIBRATION_DATA_PATH}\n\tcalibration_args:\n\t\treduction: absmax\n\t\tsamples: 2\n\t\tseed: 42\nsimulator:",
            "Section: Function Implementation\n# Implementing Function\n## Processing Data\n### Processing Output Data\nContent:\nThe `postProcess` function processes the output from the neural network model.\nIt converts the quantized output values to percentile scores, matches the index with the label, filters items that exceed the score threshold, and sorts them in descending order.\nThe filtered items provide the final classification results.\n(example):\n```kotlin\nprivate fun postProcess(modelOutput: ByteArray): Map<String, Float> {\n\tval output = when (OUTPUT_DATA_TYPE) {\n\t\tDataType.UINT8 -> {\n\t\t\tmodelOutput.asUByteArray().mapIndexed { index, value ->\n\t\t\t\tlabelList[index] to dequantizedValues[((value.toInt()\n\t\t\t\t\t\t- OUTPUT_CONVERSION_OFFSET)\n\t\t\t\t\t\t/ OUTPUT_CONVERSION_SCALE).toInt()]\n\t\t\t}.filter { it.second >= threshold }.sortedByDescending { it.second }.toMap()",
            "Section: Writing Native Program\n# Writing Native Program\n## Processing Input and Outputs\n### Loading Golden Data and Comparing it with Output (Optional)\nContent:\nAfter executing the model, load the golden binary data into memory and compare it with the data in the output memory location of model.\nThis optional step validates whether the output of model matches the expected results.\nUse the EnnAllocateAllBuffer function to obtain the output memory location.\n(example):\n```cpp\ntemplate <typename T>\nint compare_data(T *data1, T *data2, int size, T threshold) {\n\tint diff = 0;\n\tfor (int idx = 0; idx < size; idx++) {\n\t\tif (std::abs(data1[idx] - data2[idx]) > threshold) {\n\t\t\tdiff++;\n\t\t}\n\t}\n\treturn diff;\n}\n```",
            "Section: Frontend API\n# **Frontend API**\n## **How to use optimization API**\nContent:\n\t\t\t- TemplateFoldingGeGLU\n\t\t/usr/local/lib/python3.10/dist-packages/optimizer/core/templates/replace_matmul2dynamicConv.py:\n\t\t\t- TemplateReplaceMatmulToDynamicConv\n\t\t/usr/local/lib/python3.10/dist-packages/optimizer/core/templates/replace_slice2split.py:\n\t\t\t- TemplateReplaceSlice2Split\n```\n**LLM Sample yaml**\n```yaml\ninput_model_path: {INPUT_MODEL_PATH}\ninput_encodings_path: {INPUT_ENCODINGS_PATH}\noutput_folder_path: {OUTPUT_FOLDER_PATH}\ninput_model_format: onnx\nmodel_type: LLM\nsimulator:\n\t\tinput_data_path: {INPUT_DATA_PATH}\n\t\tmetric: snr\n\t\tthreshold: 100\noptimizer:\n\t\tskip_4_dim_conversion: False\n\t\tcustom_template_path:\n\t\t\t\t/usr/local/lib/python3.10/dist-packages/optimizer/core/templates/folding_rmsnorm.py:"
            ]
        ],
        "answer": "Best Lab is curated by administrators who select and upload projects that demonstrate excellence and high potential for development. Currently, there is no separate process for individuals to submit their projects directly. Instead, the administrators personally showcase projects on the platform.",
        "ground_truth": "Best Lab is a dedicated platform where administrators meticulously select and upload projects that demonstrate excellence and have high potential for development. Currently, up to six projects are featured, receiving special attention. There is no separate process for individuals to submit their projects directly to Best Lab. Instead, administrators personally curate and showcase projects on the platform."
    },
    {
        "question": "How many projects can I create maximum in the SDK Service?",
        "contexts": [
            [
            "Question: How many projects can I create maximum in the SDK Service?\nAnswer: In the SDK Service, you are allowed to create a maximum of 5 tabs. If you attempt to create more than 5 tabs, a warning popup will appear to notify you of the limit. Please adjust your projects accordingly within this constraint.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
            "Section: ENN SDK Service\n# ENN SDK\n## ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\n## 3. Tools\n### ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.",
            "Question: Please let us know the schedule and details for updates or patches to the ENN SDK or other tools.\nAnswer: Updates to the ENN SDK serviced through the portal are planned once a month. We plan to respond to requests arising from this program and provide internally developed and updated content. In addition, as previously explained, we are planning to distribute ENN Studio in the second half of next year, which has significantly improved the UI and strengthened developer usability.",
            "Question: In the SDK Service, I'm unable to download the converted log and NNC file.\nAnswer: You can download the converted log and NNC file for up to 7 days after their creation. After this period, they are deleted, and the download button is disabled. If you need to download them again, you'll have to re-upload the same model file and repeat the conversion under the same conditions.",
            "Section: Starting Android Project\n# Starting the Android Project\n## Creating Android Project\nContent:\n1. To start a new project, click **New Project** or **+**.\n2. To select a template for project, select **Empty Views Activity** and click **Next**.\n3. To configure the project, input your preferred project name and choose a location on your computer to save it.\n4. Click **Finish** to create the project.\n### Version\nFollowing is the SDK version information for the sample application.\n| SDK Component | Version Number |\n| ---- | ---- |\n| Android minSdk | 31 |\n| Android targetSdk | 34 |\n| Android compileSdk | 34 |\n| CMake Minimum Version | 3.4.1 |\n| `com.android.application` Plugin | 8.0.2 |\n| `com.android.library` Plugin | 8.0.2 |\n| `org.jetbrains.kotlin.android` Plugin | 1.8.0 |\nFor more information, refer the following:\n- Module build gradle file\n- Project build gradle file",
            "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
            "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
            "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios."
            ]
        ],
        "answer": "In the SDK Service, you are allowed to create a maximum of 5 projects. If you attempt to create more than 5 projects, a warning popup will appear to notify you of the limit.",
        "ground_truth": "In the SDK Service, you are allowed to create a maximum of 5 tabs. If you attempt to create more than 5 tabs, a warning popup will appear to notify you of the limit. Please adjust your projects accordingly within this constraint."
    },
    {
        "question": "In the SDK Service, I'm unable to download the converted log and NNC file.",
        "contexts": [
            [
            "Question: In the SDK Service, I'm unable to download the converted log and NNC file.\nAnswer: You can download the converted log and NNC file for up to 7 days after their creation. After this period, they are deleted, and the download button is disabled. If you need to download them again, you'll have to re-upload the same model file and repeat the conversion under the same conditions.",
            "Section: ENN SDK Service\n# ENN SDK\n## ENN SDK Service\nContent:\n### Conversion\n1. Select **Convert** to initiate the conversion process.\n2. After the completion of conversion process, the **NNC Download** button is enabled.\n### Download Model\n1. Click **NNC Download** to obtain the converted NNC model file.\n2. To view the logs for the conversion that has failed, click **Log Download**.\n\tYou can download and examine the log files.\n3. Copy the downloaded model to `${APP_ROOT}/app/src/main/assets`.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\n## 3. Tools\n### ENN SDK Service\nContent:\n4. **Choose Hardware Preferences**:\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> **_Warning_**: The NPU does not support all layers.\n\t\tUsing unsupported layers may lead to complications.\n\t\tFor more information on the detailed list, refer to Support Matix.\n5. **Initiate the Conversion**:\n\t1. Click **Confirm** to verify your selections.\n\t2. Click **Convert** to start the model conversion process.\n6. **Download the Converted Model**:\n\t1. If the conversion is successful, the **NNC Download** button is enabled.\n\t2. Click NNC Download to download the NNC model.\n\t3. Integrate the downloaded NNC model into your application required.",
            "Section: ENN SDK Service\n# ENN SDK\n## ENN SDK Service\nContent:\nUse the ENN SDK service to convert trained TFLite models to NNC models by performing the following steps.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\n## 3. Tools\n### ENN SDK Service\nContent:\nThe ENN SDK service is an online platform designed to enable users to convert TFLite models into NNC models.\nTo utilize this service:\n1. **Access the Exynos Developer Society**:\n\t- If you are a new user, sign up to create an account.\n\t- If you are an existing user, log in to Exynos Developer Society.\n2. **Navigate to the Service**:\n\t- Visit the ENN SDK service page.\n3. **Provide Project Information**:\n\t1. Enter a descriptive title for your project.\n\t2. Use the provided interface to upload your TFLite model.",
            "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger.",
            "Section: Quick Start Guide\n# Quick Start Guide\n## Example Using ENN SDK\n### Converting NN Models with ENN SDK Service\nContent:\nIn this example, let us consider converting a trained TFLite MobileNet V1 model from TensorFlow Hub.\nTo get started, you must be a member of the Exynos Developer Society.\n- If you are not a member of the Exynos Developer Society, sign up from here.\n- If you already have an account, log in to the Exynos Developer Society.\nTo convert MobileNet V1 model:\n1. Download `lite-model_mobilenet_v1_100_224_uint8_1.tflite` from here.\n2. Navigate to the ENN SDK service page and provide a title for your project.\n3. Then, upload the downloaded TFLite model\n4. Next, select hardware preferences.\n\t- The **Default** option creates a model that utilizes only the CPU and GPU for conversion.\n\t- The **Accelerate** option creates a model that utilizes NPU as an accelerator with CPU and GPU.\n5. After confirming your selections, click **Convert** to convert the model.\n6. After the compilation process is successfully completed, the **NNC Download** button is enabled.\n7. Click **NNC Download** to download the converted NNC model.\n\tYou can now integrate the NNC model into the desired application.",
            "Section: ENN SDK Service\n# ENN SDK\n## ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
            "Section: Documentation Home\n# ENN SDK Documentation\n## ENN SDK usage guide\nContent:\n### Quick start guide\nThis Quick Start Guide provides basic instructions for using ENN SDK.\nThis document describes the method to convert NN models to NNC models and execute NNC models on Exynos devices.\n### ENN SDK samples\nThis guide provides a list of samples of ENN SDK and their explanation.\n#### Getting started with android samples\nThis guide provides a comprehensive overview of developing an image classification Android application using ENN SDK.\n#### Getting started with native samples\nThis guide provides a walkthrough for developing a native program using ENN SDK.\nLast Updated: 2024-05-13 0200 UTC",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\nContent:\n## Abstract\nThis guide describes the method to use Exynos Neural Network Software Development Kit (ENN SDK).\nIt provides instructions for converting Neural Network (NN) models to Neural Network Container (NNC) models.\nIt also provides information about the ENN framework, providing input to the model, executing the model, and obtaining the output.\n## 1. Introduction\nENN SDK allows users to convert trained TensorFlow Lite neural network models to a format that can run efficiently in Samsung Exynos hardware.\nThis guide is applicable for users who want to test or construct an application to run inference on ENN SDK.\n### Structure of Documentation\n- Chapter 1 introduces ENN SDK and its eco-system.\n- Chapter 2 provides information on the features of ENN SDK.\n- Chapter 3 provides information on tools provided with ENN SDK.\n- Chapter 4 provides information on ENN framework API.\n- The subsequent chapters provide additional information on ENN SDK.\n### Samples\nThe list of samples for ENN SDK is available in ENN SDK Samples.\n### Support\nSupport materials including forums, FAQs, and others are available at the Exynos Developer Society web page.\n### Reporting Bugs\nTo report a bug or issue, follow the instructions described in the Reporting ENN SDK Issues."
            ]
        ],
        "answer": "You can download the converted log and NNC file for up to 7 days after their creation. After this period, they are deleted, and the download button is disabled. If you need to download them again, you'll have to re-upload the same model file and repeat the conversion under the same conditions.",
        "ground_truth": "You can download the converted log and NNC file for up to 7 days after their creation. After this period, they are deleted, and the download button is disabled. If you need to download them again, you'll have to re-upload the same model file and repeat the conversion under the same conditions."
    },
    {
        "question": "I entered the wrong password multiple times, and now my account is locked. What should I do?",
        "contexts": [
            [
            "Question: I entered the wrong password multiple times, and now my account is locked. What should I do?\nAnswer: In the Eco System, an account gets locked after 5 consecutive incorrect password attempts. If your account is locked, you can unlock it by going through the verification process sent to your linked email address. Please check your email for the necessary steps to regain access to your account.",
            "Question: In the SDK Service, I'm unable to download the converted log and NNC file.\nAnswer: You can download the converted log and NNC file for up to 7 days after their creation. After this period, they are deleted, and the download button is disabled. If you need to download them again, you'll have to re-upload the same model file and repeat the conversion under the same conditions.",
            "Question: How many projects can I create maximum in the SDK Service?\nAnswer: In the SDK Service, you are allowed to create a maximum of 5 tabs. If you attempt to create more than 5 tabs, a warning popup will appear to notify you of the limit. Please adjust your projects accordingly within this constraint.",
            "Section: ENN SDK Developer Guide\n# ENN SDK Developer Guide\nContent:\n## 6. Troubleshooting\n### FAQs\nFollowing are the responses to some of the most frequently asked questions:\n#### 1. How do I use ENN SDK service?\nThe ENN SDK service section provides detailed information on using the ENN SDK service.\n#### 2. How many projects can I create in ENN SDK service?\nUsers can create a maximum of five projects with the ENN SDK service.\n#### 3. Is ENN SDK service a paid service?\nThe ENN SDK service is currently free.\n## Reporting ENN SDK Issues\nWe encourage you to share general questions, feedbacks, or suspected bugs related to the ENN SDK on our forums for public discussion.\nIf you prefer a more direct approach or need personalized assistance, submit your concerns to our Contact Us page.",
            "Section: Starting Android Project\n# Starting the Android Project\n## Using ERD Board\n### Linking Up the ERD Board\nContent:\n#### Configuring the ERD Board\nEnabling Developer Mode in ERD board\n1. Open the **Settings** app.\n2. Scroll down and tap **About phone**.\n3. Find the **Build number** and tap it a few times until the **You are now a developer** message appears.\nEnabling USB Debugging in ERD board\n1. Navigate to the main **Settings** screen.\n2. Scroll down and tap **System**.\n\tThe **Developer options** is now displayed.\n3. Tap **Developer options**, then scroll down and turn on **USB debugging**.\n#### Connecting the ERD Board\n1. Connect the ERD board to your computer using a USB cable.\n2. In the pop-up that appears, select **Allow** to enable the USB debugging.\n3. Android Studio automatically detects the device. If the device is not detected, enable **File transfer** on the device.",
            "Section: Changing the Model\n# Changing the Model in an Android Sample Application\nContent:\nThis guide provides a step-by-step process to replace the model in an Android sample application.",
            "Section: Using Adb to Execute\n# Using Adb to Execute\n## Executing Native Program on the ERD Board\nContent:\nAfter copying the necessary files to the ERD board, execute the native program using the following commands:\n```shell\nadb shell\ncd /data/local/tmp/\nexport LD_LIBRARY_PATH=/data/local/tmp\n./enn_nnc_model_tester --model model.nnc --input input.bin --golden golden.bin --threshold 0.0001\n```\n> When nnc_model_tester is built from Windows, execute permission must be provided.\n> ```shell\n> adb shell \"chmod +x /data/local/tmp/enn_nnc_model_tester\"\n> ```\nThese commands perform the following actions:\n- `adb shell` starts a shell session on the board.\n- `cd /data/local/tmp/` changes the current directory to `/data/local/tmp/`, where the necessary files are available.\n- The `LD_LIBRARY_PATH` environment variable sets the directory that contains `libenn_public_api_ndk_v1.so`.\n- The native program is executed with the `--model`, `--input`, `--golden`, and `--threshold` parameters, which specify the model file, input data file, golden data file, and threshold value, respectively.",
            "Section: ENN SDK Service\n# ENN SDK\n## ENN SDK Service\nContent:\n### Preparation\n1. Access the Exynos Developer Society\n\t- If you are a new user, sign up to create an account.\n\t- If you are a returning user, log in to Exynos Developer Society.\n2. Download Inception v4 `.tflite` model from here.\n### Project Creation\n1. Navigate to the ENN SDK service page.\n\t\n2. Enter a descriptive title for your model.\n3. For this guide, choose **Accelerate** hardware type.\n\t- **Default**: Utilizes only the CPU and GPU.\n\t- **Accelerate**: Engages the NPU as an additional accelerator.\n\t\t> Warnig: NPU does not support all the models.\n\t\t> Choosing the **_Accelerate_** option may lead to complications.\n4. After confirming the selections, the subsequent screen appears:\n\t",
            "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
            "Question: I would like to see the source code related to the ENN SDK or portal in person.\nAnswer: We do not provide separate source code for the ENN SDK or the portal itself. However, samples, guide materials, and source code necessary for this program will be provided as much as possible. For technical support or source code inquiries required during the program, please contact us through the portal forum and we will check and provide support on a case-by-case basis."
            ]
        ],
        "answer": "If your account is locked after entering the wrong password multiple times, you can unlock it by going through the verification process sent to your linked email address. Please check your email for the necessary steps to regain access to your account.",
        "ground_truth": "In the Eco System, an account gets locked after 5 consecutive incorrect password attempts. If your account is locked, you can unlock it by going through the verification process sent to your linked email address. Please check your email for the necessary steps to regain access to your account."
    },
    {
        "question": "Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?",
        "contexts": [
            [
            "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
            "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
            "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
            "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
            "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
            "Question: I would like to know the ENN SDK support models and environments.\nAnswer: The ENN SDK facilitates AI model development by supporting various models, primarily focusing on TensorFlow Lite compatibility. It advises the conversion of TensorFlow Lite models through MLIR (Multi-Level Intermediate Representation) version 1.14 or higher, accommodating models with up to four dimensions and a maximum size of 1 GB. For models targeted for the Neural Processing Unit (NPU), quantization is mandatory, and model distillation is advised for optimization. This SDK is crafted to provide flexibility and comprehensive support, ensuring wide compatibility and adaptability across diverse AI applications and usage scenarios.",
            "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
            "Question: I would like to know the AI model categories supported by ENN SDK.\nAnswer: The ENN SDK accommodates a diverse range of AI model categories, including Image Classification, Object Detection, Segmentation, Pose Estimation, Image Enhancement, Depth Estimation, Recommendation Systems, Anomaly Detection, Computer Vision for Healthcare, and Augmented and Virtual Reality.",
            "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
            "Question: I want to download the ENN SDK and install it on a local PC or server to use it.\nAnswer: Currently, the ENN SDK is not disclosed external. In the future, we plan to prepare an external distribution version along with GUI improvements, and if so, we will distribute it first to those who participated in the AI Challenger."
            ]
        ],
        "answer": "In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must have up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
        "ground_truth": "In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK."
    },
    {
        "question": "Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?",
        "contexts": [
            [
            "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
            "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
            "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
            "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
            "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
            "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
            "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
            "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz.",
            "Question: We are participating in the 1st AI Challenger. I would like to create an AI model by referring to OpenPOSE and KoBERT. Is this supported by the ENN SDK in the 1st AI Challenger?\nAnswer: The OpenPOSE you want to use is a supported model. However, there are so many versions and types. If you have any problems during actual application, please contact us at any time. Unfortunately, KoBERT is not supported.",
            "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions."
            ]
        ],
        "answer": "In the 1st AI Challenger, certain models encounter technical support constraints. These limitations include models that require device code alterations, voice models, Generative AI models that demand real-time training, and those that require operational analysis tool support on the device. Please consider these restrictions when designing your projects for the program. If you have any inquiries or need additional details regarding these limitations, feel free to contact us for assistance.",
        "ground_truth": "In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance."
    },
    {
        "question": "Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?",
        "contexts": [
            [
            "Question: Could you inform me about any models that have technical support possibilities when executing the 2nd AI Challenger?\nAnswer: The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
            "Question: Could you inform me about any models that have technical support limitations when executing the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, please note that certain models encounter technical support constraints. These limitations include models needing device code alterations, voice models, Generative AI models demanding real-time training, and those requiring operational analysis tool support on the device. Consider these restrictions when designing your projects for the program. For any inquiries or additional details regarding these limitations, don't hesitate to contact us for assistance.",
            "Question: Could you please inform me about the models supported by the ENN SDK in the 1st AI Challenger?\nAnswer: In the 1st AI Challenger, the ENN SDK supports TensorFlow Lite models, requiring conversion with Multi-Level Intermediate Representation (MLIR) version 1.14 or higher. Models must be up to four dimensions and not exceed 1GB in size. For Neural Processing Unit (NPU) utilization, quantization of models is mandatory. Additionally, model distillation is highly recommended for optimal performance. Ensure your models meet these criteria for compatibility with the ENN SDK.",
            "Question: I would like to know the AI Challenger schedule.\nAnswer: The AI Challenger unfolds through three primary stages, beginning with Stage I, where participants are familiarized with the program's structure, resources, and tools such as the ENN SDK. Stage II focuses on the active phase of research and development, providing participants with comprehensive technical support and coaching as they progress through their projects. The program culminates in Stage III, dedicated to finalizing projects, conducting thorough testing, and presenting outcomes, potentially concluding with a showcase event or demo day.",
            "Question: I want to develop a model using an algorithm other than the sample algorithm provided by the AI Challenger.\nAnswer: In addition to the sample algorithm, other algorithms can be used to develop model. However, there may be restrictions on the areas supported, so we will inform you about the scope of support available on the portal bulletin board.",
            "Question: I want to know about the AI Challenger.\nAnswer: The AI Challenger is an open innovation type of developer program that uses and develops AI SDK using Samsung S.LSI Exynos ENN SDK.",
            "Question: I am interested in developing an AI model using the NeRF (Neural Radiance Fields) for image-based rendering in the 1st AI Challenger. Do you support it in the 1st AI Challenger?\nAnswer: Regarding your interest in using the NeRF (Neural Radiance Fields) model, it's understood that NeRF requires real-time learning based on input imagery. Unfortunately, our current setup does not support on-device training, making it challenging to support NeRF for your project within the 1st AI Challenger. For further guidance and to understand the constraints better, you may refer to the list of supported operators available on our website. We appreciate your understanding of these limitations and are here to assist with any further queries you might have.",
            "Question: I want to know the planning intention of AI Challenger.\nAnswer: Samsung System LSI's AI Challenger aims to cultivate AI expertise among Korean students and postgraduates through practical engagement and innovation. The program\u2019s objectives include advancing Edge AI technology by equipping participants with essential skills, fostering industry-academia collaboration through networking and mentorship, and transforming innovative ideas into tangible prototypes or solutions. Additionally, it offers practical exposure to real-world tech challenges. Ultimately, the AI Challenger seeks to enhance participants' competitiveness in the AI field and fuel the growth of AI technology and industry by serving as a breeding ground for applicable tech solutions.",
            "Question: Please tell us about the AI Challenger.\nAnswer: Samsung S.LSI's AI Challenger, a key part of the SEED AI initiative, is dedicated to driving technological innovation through artificial intelligence. Central to the program is the Exynos ENN SDK, an advanced toolkit by Samsung S.LSI designed to facilitate AI model creation and optimization. This versatile SDK is instrumental for developing a wide range of AI applications, from image recognition to complex predictive analytics, and is a vital resource for developers seeking to craft impactful AI solutions in various sectors. Continually refined and supported by Samsung's dedicated ML team, the Exynos ENN SDK remains at the forefront of the ever-evolving digital landscape, meeting the dynamic needs of the AI industry. Through the AI Challenger, Samsung S.LSI showcases its commitment to leading AI advancements and nurturing a vibrant community of AI developers, propelling the industry towards a future where AI is integral to technological innovation and progress.",
            "Question: I would like to know the device specifications provided by 1st AI Challenger.\nAnswer: The device features the Exynos 2200 chipset, with a CPU configuration of Cortex\u00ae-X2, Cortex\u00ae-A710, and Cortex\u00ae-A510, alongside the Samsung Xclipse 920 GPU. It's equipped with an AI Engine incorporating a Dual-core NPU and DSP. The camera capabilities include support for up to 200MP in single camera mode, 108MP at 30fps for a single camera, and a dual-camera setup of 64MP + 32MP at 30fps. Video performance allows up to 8K decoding at 60fps for 10-bit HEVC(H.265) and 30fps for 10-bit VP9, AV1, along with 8K encoding at 30fps for 10-bit HEVC(H.265) and VP9. The display supports 4K/WQUXGA at 120Hz and QHD+ at 144Hz."
            ]
        ],
        "answer": "The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon.",
        "ground_truth": "The 2nd AI Challenger introduces enhancements to the SDK, featuring customizable model optimization algorithms allowing personal code integration and conversion, along with voice model support for speech and voice recognition projects. It also includes Generative AI model support for cutting-edge AI developments. Future updates will introduce SDK retraining for improved model adaptability and accuracy, and device-specific operational analysis tools for optimal model performance tuning. These ongoing improvements aim to refine the SDK's functionalities to align with evolving development requirements. Further updates and features are on the horizon."
    }
]